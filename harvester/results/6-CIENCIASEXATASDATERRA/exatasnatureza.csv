Título|Ano de Publicação|Tipo de Acesso|Tipo de Documento|Assuntos|Idioma|Descrição|Link de Acesso
Um algoritmo de criptografia de chave pública semanticamente seguro baseado em curvas elípticas;A semantically secure public key algorithm based on elliptic curves |2006|Open Access|Dissertação|Criptografia;Seguranca : Computadores;Elliptic curves;Public-key cryptography;Semantic security;Complex multiplication;Cryptographically strong pseudorandom bit generators|por|Esta dissertação apresenta o desenvolvimento de um novo algoritmo de criptografia de chave pública. Este algoritmo apresenta duas características que o tornam único, e que foram tomadas como guia para a sua concepção. A primeira característica é que ele é semanticamente seguro. Isto significa que nenhum adversário limitado polinomialmente consegue obter qualquer informação parcial sobre o conteúdo que foi cifrado, nem mesmo decidir se duas cifrações distintas correspondem ou não a um mesmo conteúdo. A segunda característica é que ele depende, para qualquer tamanho de texto claro, de uma única premissa de segurança: que o logaritmo no grupo formado pelos pontos de uma curva elíptica de ordem prima seja computacionalmente intratável. Isto é obtido garantindo-se que todas as diferentes partes do algoritmo sejam redutíveis a este problema. É apresentada também uma forma simples de estendê-lo a fim de que ele apresente segurança contra atacantes ativos, em especial, contra ataques de texto cifrado adaptativos. Para tanto, e a fim de manter a premissa de que a segurança do algoritmo seja unicamente dependente do logaritmo elíptico, é apresentada uma nova função de resumo criptográfico (hash) cuja segurança é baseada no mesmo problema.;This dissertation presents the development of a new public key algorithm. This algorithm has two key features, which were taken to be a goal from the start. The first feature is that it is semantically secure. That means that no polynomially bounded adversary can extract any partial information about the plaintext from the ciphertext, not even decide if two different ciphertexts correspond to the same plaintext. The second feature of the algorithm is that it depends on only one security assumption: that it is computationally unfeasible to calculate the logarithm on the group formed by the points of a prime order elliptic curve. That is achieved by ensuring that all parts of the algorithm are reducible to that problem. Also, it is presented a way to extend the algorithm so that it the resists attacks of an active adversary, in special, against an adaptive chosen-ciphertext attack. In order to do that, and attain to the assumption that only the assumption of the logarithm is necessary, it is introduced a new hash function with strength based of the same problem.|http://hdl.handle.net/10183/394
Proposta de uma arquitetura especial para simulação lógica|1988|Open Access|Dissertação|Arquitetura de computadores;Simulacao logica|por|O objetivo deste trabalho é a proposta de uma arquitetura especial para simulação lógica (AESL). As técnicas e modelos utilizados no processo de simulação lógica são brevemente revistos. É definida uma taxonomia para AESL sob a qual são analisadas diversas propostas de AESL relatadas na literatura. Uma taxonomia já existente é comparada com a proposta. A AESL definida é programável para diferentes algoritmos de simulação lógica. O detalhamento da AESL é, então, incrementado pela implementação de um algoritmo particular. Uma linguagem de simulação discreta é utilizada na construção de um modelo da arquitetura. Os resultados da simulação deste modelo permitem avaliar o desempenho da AESL e otimizar sua estrutura. Uma comparação com outras arquiteturas conclui a análise.|http://hdl.handle.net/10183/1319
Implementação de objetos replicados usando java|2000|Open Access|Dissertação|Sistemas operacionais;Tolerancia : Falhas;Java (Linguagem de programação);Replicacao : Objetos|por|Este trabalho busca a implementação da replicação de objetos através da linguagem Java e de seu sistema de invocação remota de métodos (Remote Method Invocation - RMI). A partir deste sistema, define-se uma classe de replicação - a máquina de replicação – onde a implementação de grupos de objetos é estruturada de acordo com a arquitetura cliente/servidor, sendo o cliente o representante (a interface) de um grupo de objetos e os servidores representam os demais componentes do grupo. A classe de replicação atende a uma necessidade importante dos sistemas distribuídos - o desenvolvimento de aplicações tolerantes a falhas. Fundamentalmente, a tolerância a falhas é obtida por redundância e, no caso de mecanismos de tolerância a falhas por software, esta redundância significa basicamente replicação de dados, processos ou objetos. A tolerância a falhas para tal tipo de sistema é importante para garantir a transparência do mesmo, visto que, assim como um sistema distribuído pode auxiliar muito o usuário pelas facilidades oferecidas, o não cumprimento de suas atividades de acordo com o esperado pode, em algumas situações, causar-lhe transtornos e erros irrecuperáveis nas aplicações. Finalmente, como principal contribuição, este trabalho descreve e implementa a solução completa para a construção de uma biblioteca de classes que oferece a replicação de forma totalmente transparente para o usuário.|http://hdl.handle.net/10183/1328
Em direção a agentes pedagógicos com dimensões afetivas|2001|Open Access|Tese|Informática : Educação;Sistemas multiagentes;Tutores inteligentes;Avaliacao pedagogica;Inteligência artificial|por|O contexto desta tese é a Inteligência Artificial aplicada à Educação, especificamente a área dos Sistemas Tutores Inteligentes (STI). Apesar das características multidisciplinares e interdisciplinares, a preocupação maior do trabalho se dá quanto aos aspectos computacionais. A multidisciplinaridade está na relação entre os aspectos educacionais, filosóficos e psicológicos inerentes a toda construção de um software educacional, e a interdisciplinaridade acontece no relacionamento da IA com a Informática na Educação. Esta tese propõe o uso de aspectos afetivos como apoio à decisão de ação por parte de um STI. As nossas hipóteses fundamentais são: um sistema de ensino e aprendizagem computacional deve levar em consideração fatores afetivos tornando mais flexível a interação; e a arquitetura de um sistema computacional de interação em tempo real com agentes humanos deve prever explicitamente, em sua arquitetura básica, as crenças e o raciocínio afetivos. Para demonstrar essas idéias, foi definida uma arquitetura para apoiar um STI de modo a reconhecer alguns fatores afetivos, representativos de estratégias de ação de agentes humanos em interação com sistemas. Esse reconhecimento é realizado através de construções retiradas dos comportamentos observáveis do agente humano em contextos determinados. A arquitetura prevê um Sistema Multiagente para executar a percepção de fatores afetivos e da conduta do aluno em interação e de um agente pedagógico, representando o tutor.  O agente tutor é modelado através de estados mentais e é responsável pelo raciocínio de alto nível. O modelo computacional de agentes de Móra [MÓR2000] foi utilizado para implementar o “kernel cognitivo” (termo cunhado por Móra e Giraffa [GIR99] que designa a parte responsável pela deliberação). O “kernel cognitivo” decide que ações tomar para um conjunto de características de uma avaliação pedagógica. A utilização de fatores afetivos e da avaliação cognitiva de situações emocionais permite a flexibilização das estratégias quanto à adaptabilidade a agentes humanos. Particularmente, foi adotado o enfoque cognitivo para análise de situações, baseado em teorias cognitivistas sobre emoções. O uso de tecnologia multiagente, no enfoque mentalístico, especificamente BDI (Belief, Desire, Intention) e da ferramenta X-BDI, permite a formalização e construção de um tutor atuante na avaliação pedagógica. A modelagem do aluno passa a ser constituída de aspectos qualitativos e quantitativos. Estudos de casos são apresentados, em situações que consideram os fatores afetivos e nas mesmas situações sem estas considerações. As decisões do tutor para agir são analisadas e confrontadas. Os resultados mostram um impacto positivo na adaptabilidade e ação pedagógica do tutor, sendo coerente com as teorias modernas [SAL97],[DAM2000] sobre as emoções que as consideram partes fundamentais para agir. A maior contribuição desta tese está na agregação de raciocínio sobre a afetividade envolvida em situações de ensino aprendizagem de agentes humanos e artificiais e avança dentro da perspectiva de pesquisa do grupo de IA da UFRGS, quanto ao desenvolvimento de Ambientes de Ensino e Aprendizagem modelados com tecnologia multiagente, com o uso da metáfora de estados mentais.|http://hdl.handle.net/10183/1329
Eletroforese capilar de zona|1993|Open Access|Dissertação|Áreas clássicas de fenomenologia e suas aplicações;Lasers;Eletroforese;Fluorescência;Fisica de laser|por|A aplicação de uma diferença de potencial entre os extremos de um capilar de material dielétrico, preenchido com solução aquosa, provoca um fluxo eletroosmótico através deste. As moléculas, de um pequeno volume de uma amostra injetada neste fluxo, se separam umas das outras devido a diferença entre suas mobilidades eletroforéticas. Este método de separação é conhecido como Eletroforese Capilar. Ele tem se mostrado muito eficiente na separação de moléculas orgânicas complexas e é um método complementar à Cromatografia Líquida. Os sistemas de detecção mais comumente utilizados são do tipo eletroquímico ou ópticos, estes últimos podem ser de absorção ou fluorimétricos. Alta sensibilidade tem sido obtida com sistemas de detecção com fluorescência induzida a lazer, como o de Argônio, Hélio-Cádmio e de semicondutor. Neste trabalho testamos o desempenho de um laser ultravioleta pulsado, usando o mini-laser de N2 (337 nm), para induzir a fluorescência em moléculas marcadas com marcadores moleculares fluorogênicos apropriados. Para aminoácidos obtivemos limites de detecção da ordem de alguns attomóis(10-18 mol).Em condições otimizadas a quantidade mínima detectável de melatonina e serotonina, usando o marcador molecular fluorogênico isotiocianato de fluoresceÍna, foi de 150 attomóis, que corresponde a um limite de detecção em concentração de 50 nM. Os peptídeos bradicinina, lisil-bradicinina e metionil-lisil-bradicinina foram separados entre si e detectados no limite de detecção em concentração de 1,2 fmóis quando marcados com fluorescamina e a 90 attomóis quando com orto""ftaldialdeído. Do nosso conhecimento, esta é a primeira vez que um laser pulsado (N2) é usado num sistema de detecção de eletroforese capilar.|http://hdl.handle.net/10183/1338
Uma interface visual para modelos de bancos de dados orientados a objetos com suporte para versões|1998|Open Access|Dissertação|Banco : Dados;Banco : Dados orientados : Objetos;Interface : Usuario;Versoes : Banco : Dados;Interface visual|por|Este trabalho apresenta o projeto de uma interface visual para modelos de bancos de dados orientados a objetos, com suporte para versões. Um requisito importante, não atendido pelas interfaces visuais específicas e genéricas para sistemas orientados a objetos, é a capacidade de definir e manipular versões de um objeto nos vários níveis da hierarquia de classes (herança por extensão, adotada pelo modelo de versões [GOL 95]). As interfaces, que manipulam versões, suportam essa característica no nível mais especializado da hierarquia (herança por refinamento, adotada pelos principais SGBDOOs). Procurando prover a possibilidade do versionamento de objetos nos vários níveis da hierarquia de classes, surgiu a motivação para projetar e desenvolver uma interface visual com funcionalidades de interfaces existentes (específicas e genéricas) e que obedeça às características principais dos Modelos de Dados Orientados a Objetos e do Modelo de Versões [GOL 95], seguindo as características recomendadas para interfaces visuais para MDOOs, propostas em [SIL 96]. Foi implementado um protótipo com algumas das características projetadas para o browser de objeto e seu suporte para versões.|http://hdl.handle.net/10183/1347
Paralelização de métodos de resolução de sistemas lineares esparsos com o DECK em um Cluster de PCs|2000|Open Access|Dissertação|Análise numérica;Sistemas lineares;Cluster|por|O objetivo desta dissertação é a paralelização e a avaliação do desempenho de alguns métodos de resolução de sistemas lineares esparsos. O DECK foi utilizado para implementação dos métodos em um cluster de PCs. A presente pesquisa é motivada pela vasta utilização de Sistemas de Equações Lineares em várias áreas científicas, especialmente, na modelagem de fenômenos físicos através de Equações Diferenciais Parciais (EDPs). Nessa área, têm sido desenvolvidas pesquisas pelo GMC-PAD – Grupo de Matemática da Computação e Processamento de Alto Desempenho da UFRGS, para as quais esse trabalho vem contribuindo. Outro fator de motivação para a realização dessa pesquisa é a disponibilidade de um cluster de PCs no Instituto de Informática e do ambiente de programação paralela DECK – Distributed Execution and Communication Kernel. O DECK possibilita a programação em ambientes paralelos com memória distribuída e/ou compartilhada. Ele está sendo desenvolvido pelo grupo de pesquisas GPPD – Grupo de Processamento Paralelo e Distribuído e com a paralelização dos métodos, nesse ambiente, objetiva-se também validar seu funcionamento e avaliar seu potencial e seu desempenho. Os sistemas lineares originados pela discretização de EDPs têm, em geral, como características a esparsidade e a numerosa quantidade de incógnitas. Devido ao porte dos sistemas, para a resolução é necessária grande quantidade de memória e velocidade de processamento, característicos de computações de alto desempenho.  Dois métodos de resolução foram estudados e paralelizados, um da classe dos métodos diretos, o Algoritmo de Thomas e outro da classe dos iterativos, o Gradiente Conjugado. A forma de paralelizar um método é completamente diferente do outro. Isso porque o método iterativo é formado por operações básicas de álgebra linear, e o método direto é formado por operações elementares entre linhas e colunas da matriz dos coeficientes do sistema linear. Isso permitiu a investigação e experimentação de formas distintas de paralelismo. Do método do Gradiente Conjugado, foram feitas a versão sem précondicionamento e versões pré-condicionadas com o pré-condicionador Diagonal e com o pré-condicionador Polinomial. Do Algoritmo de Thomas, devido a sua formulação, somente a versão básica foi feita. Após a paralelização dos métodos de resolução, avaliou-se o desempenho dos algoritmos paralelos no cluster, através da realização de medidas do tempo de execução e foram calculados o speedup e a eficiência. As medidas empíricas foram realizadas com variações na ordem dos sistemas resolvidos e no número de nodos utilizados do cluster. Essa avaliação também envolveu a comparação entre as complexidades dos algoritmos seqüenciais e a complexidade dos algoritmos paralelos dos métodos. Esta pesquisa demonstra o desempenho de métodos de resolução de sistemas lineares esparsos em um ambiente de alto desempenho, bem como as potencialidades do DECK. Aplicações que envolvam a resolução desses sistemas podem ser realizadas no cluster, a partir do que já foi desenvolvido, bem como, a investigação de précondicionadores, comparação do desempenho com outros métodos de resolução e paralelização dos métodos com outras ferramentas possibilitando uma melhor avaliação do DECK.|http://hdl.handle.net/10183/1367
Filmes finos de óxido de estanho: efeitos da implantação iônica e de ambientes oxidantes e redutores|1990|Open Access|Dissertação|Filmes finos;Sputtering;Implantação de íons|por|Filmes finos de óxido de estanho depositados por ""sputtering"" reativo foram caracterizados com bases em análises feitas por Espalhamento Nuclear Ressonante, Espectroscopia por Retroespalhamento Rutheford, Espectroscopia Mössbauer de Elétrons de Conversão e Difração de Raios-X e pelas medidas de Resistência de Folha. Numa primeira fase, as amostras foram submetidas a tratamentos térmicos em ar e expostas à gás de cozinha, afim de testar as propriedades sensoras do material. Os filmes finos como depositados foram modificados pelos tratamentos térmicos e exposições a gases, que aumentaram sua condutividade elétrica e alteraram a concentração de vacâncias de oxigênio. Numa segunda fase, os filmes foram submetidos a diferentes tratamentos térmicos, implantados com os íons Fe+, ZN+,Cu+,Ga+ e As+ e novamente tratados termicamente. Foram observados aumentos na condutividade elétrica induzidos pela dopagem dos filmes e algumas correlações entre o perfil de distribuição das espécies implantadas e as razões O/Sn em função da profundidade.|http://hdl.handle.net/10183/1370
O Estuário do Guaíba : características texturais, mineralógicas e morfológicas|1971|Open Access|Dissertação|Sedimentos : Guaíba, Lago (RS) : Guaíba, Lago (RS)|por|Baseado em análise texturais e mineralógicas de amostras do leito do Estuário do Guaíba o autor define as seguintes fácies sedimentares: Arenosa, sub-devidida em Sub-Fácies Areia Grossa, Média e Final; Areno-Síltica; Silto-Arenosa e Areno-Silto-Argilosa. Cada uma destas fácies é o resultado da mistura e deposição de detritos sólidos provenientes de quatro áreas fontes distintas: o Escudo Cristalino; as Formações Quaternárias; a suspensão na corrente dos Rios Jacuí, Gravataí, Sinos e Caí que cortam rochas de idade Pré-Cambriana, Paleozóica, Mesozóica e Quaternária, originando, assim, uma complexa mistura que flui para o estuário e o material recente que ocorre nas margens do mesmo. A mineralogia das Argilas mostra um significativo vínvulo com as rochas-fonte, salientando-se a Montmorilonita proveniente da Formação Graxaim, Caolinita e Clorita carregadas do Escudo Pré- Cambriano e Caolinita resultante da erosão da Laterita Serra de Tapes. Relações com os resultados obtidos por diversos autores em outros estuários são discutidos paralelamente, ficando bem caracterizada a grande influência das áreas adjacentes e emersas, na distribuição dos sedimentos de fundo.|http://hdl.handle.net/10183/1371
O problema do pixel mistura: um estudo comparativo|1993|Open Access|Dissertação|Pixel mistura;Processamento de imagens;Sensoriamento remoto;Classificacao multiespectral|por|Os satélites para sensoriamento remoto atualmente dispoívies à comunidade científica possuem diferenies resoluções espaciais, por exemplo: SPOT 20 e 10 metros, LANDSAT-TM 30 metros e NOA-AVHRR 1100 metros. Essa resolução frequentemente não é grande o suficiente para um grande número de aplicações que necessitam de uma percepção da cena mais detalhada. Muitas vezes, no interior de uma célula de resolução (pixel) mais de uma classe ocorre. Este caso é conhecido como pixel mistura. Na classificação de imagens obtidas por sensoriamento remoto é comum a utilização de metodologias que atribuem somente uma classe a um pixel, como o procedimento clássico da máxima verossimilhança. Esse procedimento resulta frequentemente em uma estimação errônea das áreas ocupadas pelas classes presentes na cena. Em alguns casos, especialmente quando não há uma classe dominante, isto pode ser a fonte de um erro significativo. Desde o início dos anos 70, diferentes metodologias têm sido propostas para o trabalho num nível de subpixel. A grande vantagem do trabalho nesse nível é que um pixel não é necessariamente atribuído a somente uma classe. O pixel tem um grau que o correlaciona a cada classe: de zero(se a classe não ocorre no pixel) até 1 (a classe ocorre no pixel inteiro). Assim, cada pixel tem um vetor associado que estima a proporção de cada classe nele. A metodologia mais comumente utilizada considera a refletância do pixel mistura como uma combinação linear da refletância média de cada classe componente. De acordo com essa visão as refletâncias associadas às classes componentes são consideradas constantes conhecidas i.e., não são variáveis aleatórias. Assim, a proporção de cada classe no pixel é obtida pela resolução de um sistema de equações lineares.  Uma outra metodologia é assumir as refletâncias que caracterizam as classes como sendo variáveis aleatórias. Nesta visão, as informações a respeito das distribuições das classes é utilizada. A estimativa das proporções de cada classe é obtida pelo vetor de proporções que maximiza a função de verossimilhança. Mais recentemente, uma visão diferente foi proposta: a utilização da lógica fuzzy. Esta metodologia utiliza o conceito de função de pertinência que é essencial à teoria dos conjuntos fuzzy. Esta função utiliza elementos com natureza estatística ou não para a estimação das proporções. No presente trabalho, duas funções de pertinência foram definidas: a primeira baseada na função densidade probabilidade gaussiana e a segunda baseada diretamente na distância de Mahalanobis. O objetivo deste estudo é avaliar cada uma das metodologias anteriores em termos de acurácia, performance e dados necessários. Para este objetivo, as metodologias foram implementadas computacionalmente e alimentadas com imagens LANDSAT-TM. Para a avaliação da acurácia dos modelos um estudo qualitativo foi executado.|http://hdl.handle.net/10183/1373
Inversão numérica da transformada de Laplace por polinômios trigonométricos e de Laguerre|1988|Open Access|Dissertação|Metodos numericos : Engenharia nuclear;Algoritmo computacional alg-619;Metodo operacional : Transformada de laplace : Inversao numerica|por|Neste trabalho são desenvolvidos métodos numéricos para inversão da transformada de Laplace, fazendo-se uso de polinômios trigonométricos e de Laguerre. Sua utilização é ilustrada num problema de fronteira móvel da área de engenharia nuclear, através do algoritmo computacional ALG-619. Uma revisão dos aspectos analíticos básicos da transformada de Laplace e sua utilização na resolução de equações diferenciais parciais é apresentada de maneira suscinta.|http://hdl.handle.net/10183/1382
Derivações em anéis primos e semiprimos|1994|Open Access|Dissertação|Aneis primos : Derivacao em aneis : Aneis semiprimos|por|Dissertacao essencialmente sobre derivacoes em aneis mostra que toda derivacao de jordan num anel primo e livre de 2-torcao e uma derivacao usual. prova que toda derivacao de hasse-schmidt-jordan definida num anel semiprimo e livre de 2-torcao e uma derivacao de hasse-schmidt. finalisa com derivacoes algebricas d definidas num anel primo r (c0m unidade) e com suas respectivas extensoes d* ao anel de quocientes (a direita) de martingale de r denotado por q. e demonstrado entao, uma equivalencia entre as r, q e c-algebricidades de d e d*, onde c denota o centroide estendido de r.|http://hdl.handle.net/10183/1384
Métodos numéricos para a integração das equações da cinética pontual de um reator nuclear|1990|Open Access|Dissertação|Física matemática;Metodos numericos : Equacoes diferenciais : Integracao : Cinetica pontual : Reator nuclear|por|As equações da cinétiica pontual de um reator nuclear térmico são integradas numericamente, utilizando um método matricial de continuação analitica. Essas equações são essencialmente não-negativas e possuem um autovalor dominante vinculado à reatividade do sistema. Também, descrevem-se os métodos de Hansen e Porsching.|http://hdl.handle.net/10183/1385
Interações magnéticas e magnetoresistencia em Co/sub 10/ Cu/sub 90/|1997|Open Access|Dissertação|Física da matéria condensada;Magnetorresistência;Magnetismo;Interacoes magneticas|por|Foram investigados os efeitos de tratamentos térmicos nas propriedades estruturais, magnéticas e de magnetotransporte eletrônico de fitas de Co10Cu90 rapidamente resfriadas (“melt-spun”) enfatizando o estudo de possíveis relações entre as interações magnéticas entre grãos, camadas e cristais presentes nestes materiais e a magnetoresistência. A análise combinada da evolução estrutural e da intensidade das interações entre os grãos magnéticos mostra claramente o papel de alguns parâmetros estruturais (tamanho e densidade de partículas, distância entre partículas) e da própria intensidade das interações entre as partículas nas propriedades de transporte eletrônico em presença de um campo magnético.|http://hdl.handle.net/10183/1392
Uma Proposta de especificação formal e fundamentação teórica para simulated annealing|2000|Open Access|Dissertação|Teoria : Ciência : Computação;Otimizacao combinatoria;Algoritmos|por|Os algoritmos baseados no paradigma Simulated Annealing e suas variações são atualmente usados de forma ampla na resolução de problemas de otimização de larga escala. Esta popularidade é resultado da estrutura extremamente simples e aparentemente universal dos algoritmos, da aplicabilidade geral e da habilidade de fornecer soluções bastante próximas da ótima. No início da década de 80, Kirkpatrick e outros apresentaram uma proposta de utilização dos conceitos de annealing (resfriamento lento e controlado de sólidos) em otimização combinatória. Esta proposta considera a forte analogia entre o processo físico de annealing e a resolução de problemas grandes de otimização combinatória. Simulated Annealing (SA) é um denominação genérica para os algoritmos desenvolvidos com base nesta proposta. Estes algoritmos combinam técnicas de busca local e de randomização. O objetivo do presente trabalho é proporcionar um entendimento das características do Simulated Annealing e facilitar o desenvolvimento de algoritmos com estas características. Assim, é apresentado como Simulated Annealing e suas variações estão sendo utilizados na resolução de problemas de otimização combinatória, proposta uma formalização através de um método de desenvolvimento de algoritmos e analisados aspectos de complexidade.  O método de desenvolvimento especifica um programa abstrato para um algoritmo Simulated Annealing seqüencial, identifica funções e predicados que constituem os procedimentos deste programa abstrato e estabelece axiomas que permitem a visualização das propriedades que estes procedimentos devem satisfazer. A complexidade do Simulated Annealing é analisada a partir do programa abstrato desenvolvido e de seus principais procedimentos, permitindo o estabelecimento de uma equação genérica para a complexidade. Esta equação genérica é aplicável aos algoritmos desenvolvidos com base no método proposto. Uma prova de correção é apresentada para o programa abstrato e um código exemplo é analisado com relação aos axiomas estabelecidos. O estabelecimento de axiomas tem como propósito definir uma semântica para o algoritmo, o que permite a um desenvolvedor analisar a correção do código especificado para um algoritmo levando em consideração estes axiomas. O trabalho foi realizado a partir de um estudo introdutório de otimização combinatória, de técnicas de resolução de problemas, de um levantamento histórico do uso do Simulated Annealing, das variações em torno do modelo e de embasamentos matemáticos documentados. Isto permitiu identificar as características essenciais dos algoritmos baseados no paradigma, analisar os aspectos relacionados com estas características, como as diferentes formas de realizar uma prescrição de resfriamento e percorrer um espaço de soluções, e construir a fundamentação teórica genérica proposta.|http://hdl.handle.net/10183/1394
Morfologia e bacias de drenagem da cobertura de gelo da Ilha Rei George, Antártica|1998|Open Access|Dissertação|Glaciologia;Geomorfologia;Sensoriamento remoto : Glaciologia;Rei George, Ilha (Antártica)|por|Esta dissertação investiga feições morfológicas do campo de gelo e geleiras da Ilha Rei George, nas Shetlands do Sul, Antártica, principalmente para determinar variações na posição das frentes de gelo no período entre 1956 e 1995, através de mapas elaborados a partir de levantamentos aerofotogramétricos e imagens de satélites. A posição das frentes de gelo da ilha foram determinadas a partir de três imagens multiespectraisdo satéliteSPOT, de 1988, 1992 e 1995, e comparadas com suas posições obtidas de fotografia aéreas datadas de 1956. Imagens de satélite Landmt MSS, de 1973 e 1979, e ERS-l SAR, de 1992, foram usadas como intrumentos auxiliares no reconhecimento de feições superficiais do campo de gelo e para delimitação de bacias de drenagem em setores onde as imagens SPOT eram de difícil interpretação. Constatou-se que dos 1250 km2 da área da Ilha Rei George, 92,7% são cobertos por massas de gelo, tendo sido identificadas setenta bacias de drenagem glacial. Retração generalizada das frentes de gelo foi observada ao longo de quatro décadas, resultando na perda de aproximadamente 7% da cobertura glacial.|http://hdl.handle.net/10183/1395
Depinning do fluxo de Josephson em compósitos YBa 2 Cu 3 O 7-delta/Ag|1997|Open Access|Dissertação|Supercondutividade;Supercondutores;Física da matéria condensada|por|Apresenta-se neste trabalho um estudo experimental da dinâmica do fluxo magnético intergranular no cuprato supercondutor Y Ba2Cu3O7 e nos compósitos Y Ba2CU3O7 fi/Ag para várias concentrações de prata. Verificou-se que sob certas condições é possível separar os efeitos da dinâmica do fluxo de Josephson daqueles resultantes da dinâmica de Abrikosov. Através da técnica de medidas de magnetização DC foram obtidas as linhas de depinning inicial e de irreversibilidade magnética em campo magnético aplicado até 120 Oe. O revestimento dos grãos supercondutores aparentemente não afeta a energia de acoplamento dos elos fracos entre os grãos e o potencial de pinning até 16.1 % em peso de prata. A vantagem da mistura da prata é que esta melhora as propriedades mecânicas dos óxidos supercondutores além de melhorar as correntes críticas do sistema. Observamos que o depinning do fluxo de Josephson começa numa temperatura bem definida, dependendo do campo magnético aplicado e gradualmente aumenta com a temperatura até ou muito próximo do limite de irreversibilidade, acima do qual todo o fluxo intergranular (e intragranular) está livre. As linhas de irreversibilidade das nossas amostras podem ser entendidas dentro do modelo do Vidro supercondutor.|http://hdl.handle.net/10183/1396
Tipos de representações mentais utilizadas por estudantes de física geral na área de mecânica clássica e possíveis modelos mentais nessa área|1997|Open Access|Dissertação|Ensino de física;Modelos mentais : Mecanica classica;Representacoes mentais : Mecanica classica;Física geral|por|Neste trabalho, além de investigarmos o tipo de representação mental (proposições, imagens ou modelos mentais) utilizado por estudantes de Física Geral na área de Mecânica Newtoniana, tentamos identificar possíveis modelos mentais que estes estudantes teriam sobre alguns conceitos físicos. Baseamos nosso estudo na Teoria dos Modelos Mentais de Johnson-Laird. Estudantes de nível universitário foram observados durante dois semestres com o objetivo de determinar o tipo de representação mental que eles teriam utilizado durante o curso, quando resolviam os problemas e as questões propostas nas tarefas instrucionais. Foi realizada uma entrevista no final do curso com a finalidade de encontrar elementos adicionais que nos permitissem inferir modelos mentais sobre conceitos físicos usados pelos estudantes na elaboração de suas respostas. Os resultados desta pesquisa sugerem a importância dos modelos mentais na compreensão e uso dos conceitos físicos. Parece que quanto mais “elaborados” os modelos mentais, mais facilmente os alunos poderiam compreender situações e contextos distintos daqueles trabalhados em aula.|http://hdl.handle.net/10183/1401
Petrologia do macico mafico-ultramafico passo do ivo, sao gabriel, Rio Grande do Sul|1981|Open Access|Dissertação|Macico mafico-ultramafico passo do ivo (RS);Petrologia;Mapa geologico : Passo do ivo (RS)|por|o Maciço Máfico-ultramáfico Passo do Ivo, localizado ao sul de São Gabriel (RS), é um corpo de forma alongada,encaixado nos metamorfitos do Grupo Vacacaí e o conjunto está encravado tectonicamente, em litologias graníticas. Foram distinguidas zonas de concentração preferencial de minerais dentro do maciço, constituindo as seguintes litologias: olivina acumulados, clinopiroxênio-olivina acumulados, clinopiroxênio, ottopiroxênio-olivina acumulados e gabros. Na parte central do corpo, a mineralogia e as texturas ígneas estão preservadas. Nas partes externas, predomina a mineralogia metamórfica: actinolita, clorita, tremolita e cumingtonita. Os estudos petrográficos revelaram texturas acumuladas, características dos complexos estratiformes. As análises químicas mostram que MgO, Fe203 apresentam correlação negativa com Si02; FeO, Ti02, Na20, CaO e A1203 apresentam correlação positiva com Si02; Cr, Ni e Co apresentam valores médios de, respectivamente, 4288 ppm, 656 ppm e 175 ppm nos ultramafitos. Infere-se que o corpo tenha-se formado a partir da cristalização fracionada de minerais máficos de um magma toleítico magnesiano que se diferenciou,segundo um ""trend"" komatiitico. Após o emplaçamento do corpo máfico-ultramáfico nos metamorfitos, os granitos circundantes, provavelmente intrusivos, provocaram um metamorfismo de contato nas rochas do maciço que atingiram o facies xistos-verdes e localmente anfibolito. A formação de xistos magnesianos em algumas partes do corpo, sugere metamorfismo dinâmico associado aos extensos falhamentos NE que cortam a região.|http://hdl.handle.net/10183/1423
Tolerância a falhas e reflexão computacional num ambiente distribuído|2000|Open Access|Dissertação|Tolerancia : Falhas;Orientacao : Objetos;Reflexao computacional|por|O modelo de objetos apresenta-se como um modelo promissor para o desenvolvimento de software tolerante a falhas em virtude de características inerentes ao próprio modelo de objetos, tais como abstração de dados, encapsulamento, herança e reutilização de objetos (componentes). O uso de técnicas orientadas a objetos facilita o controle da complexidade do sistema porque promove uma melhor estruturação de seus componentes e também permite que componentes já validados sejam reutilizados [LIS96]. Técnicas básicas para tolerância a falhas em software baseiam-se na diversidade de projeto e de implementação de componentes considerados críticos. Os componentes diversitários são gerenciados através de alguma técnica que tenha por objetivo assegurar o fornecimento do serviço solicitado, como, por exemplo, a conhecida técnica de blocos de recuperação. Reflexão Computacional é a capacidade que um sistema tem de fazer computações para se auto analisar. Ela é obtida quando o programa pára sua execução por um período de tempo para fazer computações sobre si próprio; analisa seu estado, se o processamento está correto, se pode prosseguir com a execução e atingir o objetivo satisfatoriamente; se não precisa mudar de estratégia ou algoritmo de execução, fazendo, ainda, processamentos necessários para o sucesso da execução.  Um sistema de programação distribuída consiste basicamente em vários aplicativos executados em diferentes computadores, os quais realizam troca de mensagens para solucionar um problema comum. A comunicação entre os computadores é realizada através da rede que os interliga. As Redes que controlam sistemas críticos são normalmente de pequena escala pois redes de grandes dimensões podem apresentar atrasos e baixa confiabilidade. Portanto, a abordagem aqui proposta consiste em utilizar, em um ambiente distribuído, uma arquitetura reflexiva aliada a técnicas do domínio da tolerância a falhas para promover a separação entre as atividades de controle, salvamento, recuperação, distribuição e validação de componentes e as funcionalidades executadas pelo próprio componente, a fim de que falhas não venham a prejudicar a disponibilidade, confiabilidade e clareza de determinadas computações. A proposta apóia-se num estudo de caso, implementado na linguagem de programação Java, com seus protocolos de reflexão computacional e de comunicação.|http://hdl.handle.net/10183/1433
Utilizacao do limoneno como doador de hidrogenio na reducao catalitica por transferencia de algumas cetonas cataliticas alfa, beta insaturadas|1990|Open Access|Dissertação|Limoneno : Hidrogenação catalítica heterogênea;Reducao : Cetonas : Transferencia : Hidrogenios|por|A hidrogenação catalítica por transferência de hidrogênio, utilizando como doador o limoneno e como catalisador 10% Pd/C foi experimentada em cinco cetonas alicíclicas, α,8-insaturadas de seis membros e em uma cetona alifática α, ß,γ ,δ-insaturada. Vários parâmetros de reação foram investigados e tendo como subtrato modeelo a ísoforone. Observou-se 100% de conversão em todas as enonas testadas e aproximadamente 100% de seletividade em cetona saturada nos casos em que o substrato enônico apresentava metilas geminadsa; estabilidae de anel de quatro membros nas condições reacionais e a não seletividade na redução das ligações olefínicas de cetona α, ß,γ ,δ-insaturada. Constatou-se, além de desproporcionação do doador, a ococrrência de um processo competitivo ded desproporcionação do aceptor, o qual foi insignificante no caso de cetonas possuidoas de metilas geminadas, mas perceptível no caso de cetonas que apresentam hidrogênio em cada um dos outros cinco carbonos do anel.|http://hdl.handle.net/10183/1449
Incorporando suporte a restrições espaciais de caráter topológico ao modelo abstrato do consórcio Open GIS|2001|Open Access|Dissertação|Geoinformática;Banco : Dados geograficos;Sistemas : Informacao geografica;Open GIS|por|Os Sistemas de Informação Geográfica (SIG) são construídos, especificamente, para armazenar, analisar e manipular dados geográficos, ou seja, dados que representam objetos e fenômenos do mundo real, cuja localização em relação à superfície da Terra seja considerada. A interoperabilidade desses sistemas, que constitui-se na capacidade de compartilhar e trocar informações e processos entre ambientes computacionais heterogêneos, se faz necessária, pois, devido ao elevado custo de aquisição dos dados geográficos, as comunidades de informação precisam compartilhar dados de fontes existentes, sem a necessidade de fazer conversões. Porém, pela complexidade e incompatibilidades de representação, de estrutura e de semântica das informações geográficas, a maioria dos softwares de SIG, hoje, não são interoperáveis. Existe também, além do problema da não interoperabilidade, uma crescente preocupação com relação à qualidade e à integridade espacial dos dados geográficos.  Contudo, alguns modelos conceituais de dados geográficos e os softwares de SIG não oferecem, ainda, os meios adequados para representar e garantir a integridade espacial das informações. As restrições de integridade definidas durante a fase de projeto conceitual, normalmente, são implementadas durante o projeto físico, seja de forma implícita ou explícita, podendo ser incorporadas diretamente no modelo de implementação do SIG, de forma que o usuário da aplicação apenas mencione a regra e o sistema a implemente e a garanta automaticamente.Este trabalho de pesquisa propõe uma extensão ao Modelo Abstrato OpenGIS, modelo este que deve ser um padrão de interoperabilidade de software para SIG. A extensão proposta incorpora ao mesmo um subconjunto de tipos de restrição espacial, buscando com isso oferecer melhor suporte às regras da realidade geográfica expressáveis na modelagem conceitual do sistema.|http://hdl.handle.net/10183/1461
Geração de sombras em objetos modelados por geometria sólida construtiva|2001|Open Access|Dissertação|Computação gráfica;Geometria sólida construtiva;Sombras : Computação gráfica|por|Atualmente os sistemas computacionais mais sofisticados são aqueles que apresentam imagens gráficas. Devido às características de alta velocidade de processamento e excelente resultado na geração de imagens o uso da Computação Gráfica se dá em diversas áreas como a indústria, pesquisa, publicidade, entretenimento, medicina, treinamento, dentre outras. Este trabalho aborda dois assuntos clássicos na Computação Gráfica, Geometria Sólida Construtiva (CSG) e Sombras Projetadas. Ambos são muito importantes para esta linha de pesquisa da Ciência da Computação. A Geometria Sólida Construtiva é utilizada na modelagem de objetos e as sombras projetadas são necessárias para aumentar o realismo das imagens. Geometria sólida construtiva (CSG) é uma técnica para a modelagem de sólidos, que define sólidos complexos pela composição de sólidos simples (primitivas). Isso inclui também a composição de objetos já combinados, até que se chegue a um objeto mais complexo. Um fator muito importante e necessário na obtenção de imagens realistas e que deve ser considerado é a utilização de sombras, pois estas são eficazes no realismo e impressão espacial de objetos tridimensionais. As sombras estabelecem diversos níveis de profundidade na imagem, fazem uma pontuação geométrica na cena de modo a evitar que os objetos não pareçam estar flutuando no ar.  Este trabalho consiste em apresentar uma proposta para a geração de sombras em objetos modelados pela Geometria Sólida Construtiva. Para tanto foram estudados os assuntos referentes à modelagem de objetos por CSG, algoritmos para a geração de sombras “bem delimitadas” e formas de gerar sombras na Geometria Sólida Construtiva. O processo de geração de sombras em cenas modeladas por CSG, através da aplicação das mesmas operações booleanas envolvidas na modelagem dos objetos, sobre as sombras nem sempre apresenta resultados corretos. Diante disso, foram investigadas outras formas de solucionar o problema. Dentre estas, uma alternativa é a realização de transformações na árvore binária CSG, através de outras operações, envolvendo o uso de complemento com operações de união e interseção, para a modelagem do objeto e geração da sombra correspondente. Com base nos estudos realizados foram implementados dois protótipos que exibem a sombra projetada de objetos modelados por CSG. Na implementação do protótipo A utilizaram-se as técnicas tradicionais de modelagem de sólidos e sombra projetada. Os resultados obtidos com este protótipo serviram de referência. No protótipo B os resultados foram obtidos através da aplicação da zona ativa das primitivas na modelagem dos objetos e a sombra é projetada durante o processo de avaliação de contornos do sólido. Os resultados obtidos com este protótipo são comparados com os resultados do protótipo A e são apresentados como forma de exibir a aplicação do método proposto.|http://hdl.handle.net/10183/1462
Aprendizado relacional por um modelo neural|2001|Open Access|Dissertação|Banco : Dados;Mineracao : Dados;Redes neurais;Programação lógica indutiva|por|As técnicas que formam o campo da Descoberta de Conhecimento em Bases de Dados (DCBD) surgiram devido à necessidade de se tratar grandes volumes de dados. O processo completo de DCBD envolve um elevado grau de subjetividade e de trabalho não totalmente automatizado. Podemos dizer que a fase mais automatizada é a de Mineração de Dados (MD). Uma importante técnica para extração de conhecimentosa partir de dados é a Programação Lógica Indutiva (PLI), que se aplica a tarefas de classificação, induzindo conhecimento na forma da lógica de primeira ordem. A PLI tem demonstrado as vantagens de seu aparato de aprendizado em relação a outras abordagens, como por exemplo, aquelas baseadas em aprendizado proposicional Os seus algorítmos de aprendizado apresentam alta expressividade, porém sofrem com a grande complexidade de seus processos, principalmente o teste de corbertura das variáveis. Por outro lado, as Redes Neurais Artificiais (RNs) introduzem um ótimo desempenho devido à sua natureza paralela. às RNs é que geralmente são ""caixas pretas"", o que torna difícil a obtenção de um interpretação razoável da estrutura geral da rede na forma de construções lógicas de fácil compreensão  Várias abordagens híbridas simbólico-conexionistas (por exemplo, o MNC MAC 890 , KBANN SHA 94 , TOW 94 e o sistema INSS OSO 98 têm sido apresentadas para lidar com este problema, permitindo o aprendizado de conhecimento simbólico através d euma RN. Entretanto, estas abordagens ainda lidam com representações atributo-valor. Neste trabalho é apresentado um modelo que combina a expressividade obtida pela PLI com o desempenho de uma rede neural: A FOLONET (First Order Neural Network).|http://hdl.handle.net/10183/1463
Imagens ecocardiográficas fetais integradas a banco de dados|1998|Open Access|Dissertação|Informática médica;Processamento de imagens;Banco : Dados|por|Este trabalho discorre no escopo de informática médica, no âmbito da Unidade de Cardiologia Fetal do Instituto de Cardiologia - Fundação Universitária de Cardiologia do RS. Sabe-se que a medicina gera um grande volume de dados, sejam eles, textuais, numéricos, gráficos ou mesmo imagens ou sons geradas por equipamentos de ultra-som, tomógrafos computadorizados, ressonância magnética, RX, entre outros. Este trabalho desenvolve a integração das imagens ecocardiográficas fetais ao banco de dados. Atualmente, a tendência observada no desenvolvimento de sistemas de informações é a utilização de banco de dados que sejam capazes de manipular informações completas sobre seus pacientes, tais como: consultas, medicamentos, internações, bem como os laudos de exames com suas respectivas imagens quando estes possuírem. É com base nestas tendências que foram definidos os tópicos relevantes a serem estudados e implementados neste trabalho, integrando os estudos ecocardiográficos fetais com as informações do banco de dados da unidade de cardiologia fetal (UCF). Neste trabalho está apresentado o modelo do banco de dados da UCF. Para esta modelagem foram realizados estudos para aquisição de conhecimento da área e também para compreender as necessidades da unidade  Da mesma forma, as imagens ecocardiográficas fetais foram estudadas para que fosse possível serem modeladas junto ao banco de dados. Para esta modelagem foi necessário fazer uma breve revisão dos conceitos utilizados pelo paradigma de orientação a objetos, uma vez que o modelo foi desenvolvido utilizando esta metodologia. As imagens ecocardiográficas fetais receberam grande atenção, uma vez que para elas foram criadas classes distintas. Também para aumentar a funcionalidade foram estudados conceitos de imagem digital, para posterior aplicação sobre as imagens do domínio. Foram realizados estudos sob manipulação de imagens, como modificação do brilho, medidas, filtros e formas de armazenamento. Considerando os formatos de gravação, dois padrões foram contemplados neste trabalho: o utilizado pela placa disponível no instituto denominado DT-IRIS e o DICOM que é um padrão internacional de armazenamento e comunicação de imagens médicas. Por fim, a implementação do protótipo procura demonstrar a viabilidade do modelo proposto, disponibilizando dados textuais, imagens e ainda realizando manipulações sobre estas imagens do domínio.|http://hdl.handle.net/10183/1466
Sistema de controle de consumo para redes de computadores|2000|Open Access|Dissertação|Redes : Computadores;Gerencia : Energia eletrica;Tolerancia : Falhas;Snmp|por|Este trabalho define e implementa um sistema de controle de consumo para redes de computadores, objetivando aumentar o tempo de operação da rede em caso de operação com recursos limitados e redução de consumo de energia em situações de fornecimento normal. Na definição do sistema, denominado NetPower, foi estabelecida uma estrutura através da qual um gerente (coordenador) monitora as atividades dos equipamentos vinculados à rede, e determina alterações nos estados de consumo respectivos, de acordo com as necessidades ou atendimento de padrões de otimização. Aos equipamentos podem ser atribuídos diferentes privilégios em uma hierarquia adaptável a diversos ambientes. Um reserva oferece opção às falhas do gerente. A implementação está baseada no protocolo SNMP (Simple Network Management Protocol) para a gerência e são considerados preponderantemente os padrões para controle de consumo dos equipamentos Advanced Power Management, APM, e Advanced Configuration and Power Interface Specification, ACPI. Além da arquitetura do gerente e dos agentes, foi definida também uma MIB (Management Information Base) para controle de consumo.  No projeto do sistema, foi privilegiado o objetivo de utilização em qualquer ambiente de rede, sem preferência por equipamentos de algum fabricante específico ou por arquitetura de hardware. Tecnologias de domínio público foram utilizadas, quando possível. No futuro este sistema pode fazer parte da distribuição de sistemas operacionais, incorporando controle de consumo às redes. No texto é feita uma comparação entre os softwares existentes para controle de consumo, são apresentados os recursos de controle de consumo disponíveis nos equipamentos de computação, seguido da descrição do protocolo de gerência utilizado. Em seguida, é apresentada a proposta detalhada do sistema de controle e descrita da implementação do protótipo.|http://hdl.handle.net/10183/1473
Proposta de extensão do modelo de expressões intervalares para sincronização multimídia na web|2000|Open Access|Dissertação|Automação;Escritórios;Multimídia;Sincronizacao multimidia|por|O desenvolvimento de apresentações multimídia sincronizadas para Web se baseia na definição e especificação de relações entre objetos advindos de diversas mídias, que possuem requisitos específicos de desempenho, estando ainda sujeitos à atrasos randômicos durante sua transmissão. Neste sentido, foram definidos modelos para especificação de sincronismo que têm por objetivo estruturar a ordenação destas aplicações de maneira a executar as relações definidas pelo autor quando da exibição coerente no destino. Este trabalho realiza um estudo dos principais modelos utilizados no desenvolvimento de apresentações multimídia sincronizadas, principalmente relacionando-se ao ambiente Web. Desta forma, tornando possível identificar características, vantagens e desvantagens de seus usos, bem como verificar que o Modelo de Expressões Intervalares é adequado a este trabalho, embora ainda apresente algumas deficiências no controle de mídias e interações do usuário. Finalmente, visando solucionar as deficiências encontradas no Modelo de Expressões Intervalares, foi desenvolvida uma proposta de extensão para o mesmo. Ela consiste na criação de um controlador para a interação do usuário e para a manipulação de mídias, chamado mídia invisível; e na definição do operador Sinc-Point, que proporciona ao autor maior flexibilidade na definição da sincronização. Esta proposta apresenta as seguintes vantagens: possibilita ao autor criar sincronizações do tipo “overlaps”, que baseia o disparo de uma mídia durante a apresentação de outra; possibilita ao autor realizar um controle sobre o intervalo para a interação do usuário e para a apresentação de mídias; mantém a consistência da apresentação; não acarreta muitas mudanças no modelo original.|http://hdl.handle.net/10183/1474
Aspectos da sedimentação na região nordeste da Lagoa dos Patos : lagoa do casamento e saco do cocuruto - RS - Brasil|1977|Open Access|Tese|Casamento, Lagoa do (RS);Patos, Lagoa dos (RS);Planície costeira : Rio Grande do Sul;Sedimento costeiro : Rio Grande do Sul;Cocuruto, Saco do (RS)|por|A formação da Lagoa dos Patos foi condicionada pelo desenvolvimento de uma barreira múltipla, arenosa, sob a influência das oscilações eustáticas ocorridas durante o Quaternário. Os aspectos geomorfológicos da margem lagunar desta barreira evidenciam a existência de pelo menos quatro ciclos de transgressão e regressão. Os processos de sedimentação que os acompanharam, proporcionaram a compartimentação da laguna mediante o crescimento de pontais arenosos, mecanismo que deu origem a Lagoa - do Casamento e ao Saco do Cocuruto. Os sedimentos do fundo destes corpos lagunares são arenosos e silticos. As fácies arenosas ocorrem nas partes marginais e rasas e tem suas caracteristicas texturais influenciadas pelo tipo de material das áreas fonte, da natureza, intensidade e tempo de atuação dos agentes de sedimentação. As fácies silticas ocupam as porções centrais, mais profundas. As zonas intermediárias são atapetadas por fácies transicionais areno-silticas e silto-arenosas.Os terrenos quaternários da margem lagunar, retrabalhados durante os ciclos transgressivos, constituem a principal fonte dos sedimentos lagunares. Parte do material siltico trazido em suspensão pelas águas da Lagoa dos Patos que ingressam na Lagoa do Casamento, provém das terras altas que margeiam a provincia Costeira.  Os principais agentes envolvidos nos processos de sedimentação são o vento, as ondas e as correntes lagunares. A circulação das águas é também influenciada pelos sistemas fluviais atuantes na região. A sedimentação se processa em um ambiente de águas rasas e doces, levemente ácidas, oxidantes nas margens e pouco redutoras nas partes centrais. A atividade biológica bentônica é reduzida e relacionada a uma pequena fauna de moluscos. A evolução geomorfológica da área controlou os aspectos maiores da sedimentação no corpo lagunar. Tentativas de caracterização ambiental baseadas em análise granulométrica mostraram que os métodos de FOLK & WARD (1957), PASSEGA & BYRANJEE (1969) e DOEGLAS (1968),são efetivamente úteis na descrição e interpretação de ambientes recentes de sedimentação onde os parâmetros físicos são conhecidos. Entretanto a sua utilização como critério único na determinação paleo-ambiental fica prejudicada, pois a sedimentação em uma província costeira e policíclica e promovida por vários ambientes definidos que se deslocam no tempo e no espaço. Ocorre muitas vêzes que o rápido retrabalhamento de materiais depositados em ambientes de alta energia é incapaz de apagar as características texturais herdadas do ciclo anterior  A maior parte dos sedimentos das fácies arenosas do fundo lagunar tem propriedades semelhantes aos depositados em ambientes praiais e eólicos.|http://hdl.handle.net/10183/1480
Aplicacoes da altimetria topex/poseidon no estudo de aspectos dinamicos do oceano atlantico sul ocidental|1996|Open Access|Dissertação|Altimetria espacial;Oceanografia fisica;Sensoriamento remoto : Oceanografia;Sensoriamento remoto|por|Dados altimétricos do satélite TOPEX/POSEIDON obtidos durante 1993 e 1994 foram utilizados para estudar a dinâmica oceanográfica na região oeste do oceano Atlântico Sul, entre as latitudes de 15°S e 50°S e longitudes de 30°W e 65°W. Simultaneamente, dados hidrográficos (históricos e sinópticos) e imagens termais do sensor AVHRR/NOAA foram utilizados com o objetivo de proporcionar uma comparação com os resultados da altimetria. Características da topografia dinâmica superficial, da variabilidade e anomalias da topografia dinâmica são apresentadas. Imprecisões dos modelos do geóide marinho restringiram o estudo dos padrões da topografia dinâmica supercial e das correntes geostrósficas superficiais, a comprimentos de onda espaciais superiores a 2400 Km. A variabilidade temporal da topografia dinâmica demonstrou um padrão altamente energético na região da Confluência Brasil-Malvinas durante o período. Diferenças sazonais foram observadas com os maiores valores de variabilidade nos meses de inverno e primavera. As anomalias (resíduos) da topografia dinâmica foram utilizadas no estudo de fenômenos transientes, onde um vórtice foi acompanhado acompanhado ao longo do tempo, e em conjunto com imagens AVHRR/NOAA. Feições térmicas superficiais foram confrontadas com anomalias altimétricas de trajetórias específicas, contemporâneas às imagens, apresentando ótima concordância e confirmando a excelente potencialidade do uso conjunto destes dois tipos de dados. Finalmente, a técnica da topografia dinâmica composta (combinação de dados altimétricos e climatológicos) foi utilizada para reconstruir a topografia dinâmica total a partir das anomalias altimétricas, sem a contaminação das imprecisões dos modelos do geóide.  O ciclo escolhido corresponde ao mesmo período de um cruzeiro hidrográfico, proporcionando uma comparação entre topografias dinâmicas (e velocidades geostróficas superficiais) estimadas pelos dois conjuntos de dados. A concordãncia entre os dois resultados foi bastante satisfatória, indicado a possibilidade de se gerar estimativas da circulação geostrófica superficial, com boa confiabilidade, apenas com dados do sátelite e dados hidrográficos climatológicos. Concluíndo, o TOPEX/POSEIDON apresenta-se como uma excelente ferramente para o estudo da dinâmica oceanográfica no Atlântico Sul, proporcionando uma análise quasi-sinóptica de qualidade e, principalmente, aumentando o conhecimento desta região.|http://hdl.handle.net/10183/1481
Contribuição a Geologia do Holoceno da Província Costeira do Rio Grande do Sul - Brasil|1972|Open Access|Dissertação|Holoceno;Geologia : Provincia costeira : Rio Grande do Sul;Formacao graxaim;Formacao chui;Formacao guaiba;Formacao itapoa;Formacao gravatai;Geologia : Planície costeira : Holoceno : Rio Grande do Sul|por|A região costeira do Brasil Meridional, definida como Provincia Costeira do Rio Grande do Sul é constituida por dois elementos geológicos maiores, o Embasamento e a Bacia de Pelotas. O primeiro, composto pelo complexo cristalino pré-cambriano e pelas sequências sedimentares e vulcânicas, paleozóicas e, mesozoicas, da Bacia do Paraná, comportando-se como uma plataforma instável durante os tempos cretácicos, deu origem ao segundo, através de movimentações tectônicas. Desde então a Bacia de Pelotas, uma bacia marginal subsidente, passou a receber a carga clástica derivada da dissecação das terras altas adjacentes as quais constituíam parte do seu embasamento que na parte ocidental atuou no inicio como uma área levemente positiva, estabilizando-se após. A sequência sedimentar ali acumulada, cerca de I 500 metros de espessura, é fruto de sucessivas transgressões e regressões. Controladas no princípio pelo balanço entre as taxas de subsidência e de sedimentação, a partir do Pleistoceno estas transgressões e regressões passaram a ser governadas pelas variações glacio-eustáticas ocorridas no decorrer da Era Cenozóica  A cobertura holocênica deste conjunto é considerada como outro elemento geológico importante da provincia costeira pelo fato de compor a maioria das grandes feiçoes morfograficas responsáveis pela configuraçao superficial da região. Ela é constituida por um pacote trans-regressivo cuja porção superior expõe-se na planície litorânea, encerrando uma série de unidades lito-estratigraficas descontinuas e de idade variável resultantes do deslocamento de vários ambientes de sedimentação por sobre a mesma região. O estabelecimento de sua história geológica somente tornou-se possivel após detida análise geomorfológica. A planicie arenosa litorânea que separa a Lagoa dos Patos do Oceano Atlântico, revelou-se composta pela sucessão de quatro sistemas de barreiras, constituindo a denominada Barreira Múltipla da Lagoa dos Patos, cuja origem está diretamente relacionada às oscilações eustáticas que se sucederam na região,durante os últimos 6 000 anos, após o final da Transgressão Flandriana. A primeira barreira formou-se durante o nivel máximo atingido pelo mar no final da grande transgressão holocênica, construida a partir de longos esporões arenosos ancorados aos promontórios existentes na entrada das várias baias que ornamentavam a costa de então. Sobre tais esporões acumularam-se, durante pequenas oscilações do nivel do mar, extensos depósitos arenosos de natureza eólica  Outra barreira foi desenvolvida a partir da emersão de barras marinhas durante a fase regressiva que se sucedeu, aprisionando um corpo lagunar sobre um terraço marinho recém exposto. Ainda no decorrer desta transgressão, grandes quantidades de areia trazidas da antepraia foram mobilizadas pelo vento construindo grandes campos de dunas sobre a barreira emersa.Na área lagunar, igualmente afetada pela regressão, depósitos lagunares e paludais foram acumulados sobre o terraço marinho. Assim estruturada, a barreira resistiu a fase transgressiva subsequente quando o aumento do nivel do mar foi insuficiente para encobri-Ia. Na margem oceânica a ação das ondas promoveu a formação de falésias, retrabalhando o material arenoso e redistribuindo-o pela antepraia. Na margem da Lagoa dos Patos de então, o avanço das aguas causou a abrasão do terraço marinho parcialmente recoberto por depósitos paludais e lagunares. Nova fase regressiva ocasionou o desenvolvimento de outra barreira no lado oceânico isolando uma nova laguna enquanto que na margem da Lagoa dos Patos emergia um terraço lagunar. Obedecendo a este mesmo mecanismo a quarta barreira foi acrescentada a costa oceânica e um segundo terraço lagunar construido na borda da Lagoa dos Patos. O constante acúmulo de sedimentos arenosos que se processou na área após a transgressão holocênica, através de processos praiais e eólicos desenvolvidos num ambiente de completa estabilidade tectônica, aumentou consideravelmente a área emersa da província costeira  A parte superior da cobertura holocênica constitui assim uma grande sequência regressiva deposicional, que teve como principal fonte os extensos depósitos que recobriam a plataforma continental adjacente. De posse de tais elementos, a costa atual do Rio Grande do Sul é uma costa secundária, de barreiras, modelada por agentes marinhos, conforme a classificação de Shepard. A tentativa de correlaçao entre as várias oscilaçoes eustáticas deduzidas a partir da análise geomorfológica da região ( com aquelas que constam na curva de variaçao do nlvel do mar o corrida nos últimos 6 000 anos, apresentada por Fairbridge, revela coincidências encorajadoras no sentido de estender o esquema evolutivo aqui proposto, como uma hipótese de trabalho no estudo dos terrenos holocênicos restantes da Província Costeira do Rio Grande do Sul. A sua utilização tornará muito mais fácil a organização crono-estratigráfica das diversas unidades que nela se encontram.|http://hdl.handle.net/10183/1482
Uso de sistemas de informação geografica como subsidio ao planejamento em areas agricolas : um caso no planalto do Rio Grande do Sul|1995|Open Access|Dissertação|Sensoriamento remoto;Planejamento regional : Agricultura;Sistemas de Informação Geográfica (SIG)|por|Os Sistemas de Informação Geográfica vêm sendo cada vez mais utilizados em estudos envolvendo o planejamento e gerenciamento de recursos e meio-ambiente. A agricultura é uma das atividades humanas mais intimamente relacionadas com o meio ambiente. Este trabalho investiga o emprego desses sistemas para integrar diferentes informações relacionadas à produção agrícola e obter respostas que subsidiem o planejamento em regiões agrícolas.A área estudada é uma localidade do município de Não-me-Toque, situado no planalto médio do Rio Grande do Sul, entre as coordenadas 28°21' e 28°34'sul e 53°40' e 53°57'oeste. Os resultados evidenciaram as vantagens dos Sistemas de Informação Geográfica sobre os métodos convencionais de análise, especialmente no que se refere à velocidade, precisão e à associação dos dados de interesse à sua localização geográfica.|http://hdl.handle.net/10183/1501
Integração de imagens TM e aerogeofísicas para análise litoestrutural de uma porção da zona de cisalhamento transcorrente dorsal de Canguçu, regiao de Quitéria-Várzea do Capivarita, RS|1998|Open Access|Dissertação|Sensoriamento remoto;Processamento de imagens;Sistemas de Informação Geográfica (SIG);Geofísica|por|A utilização de programas de processamento de imagens digitais e de sistemas de informações geográficas que admitem a importação e exportação de inúmeros formatos de apresentação de dados, aliado a modernos equipamentos de computação, tem tornado a integração de dados, de diferentes sensores, um caminho padrão em Geociências, pela otimização da relação custo/tempo na execução de serviços de mapeamento. Neste contexto, esse trabalho resulta da análise da integração de dados de sensoriamento remoto e geofísica, com o objetivo de verificar sua aplicabilidade na identificação e caracterização litológica e estrutural de uma área-teste, localizada na Região de Quitéria -Várzea do Capivarita, no Estado do Rio Grande do Sul. A metodologia usada, em um primeiro momento, priorizou o processamento e análise individual de dados cartográficos, de imagens TM/LANDSAT-5 e dados de aeromagnetometria e aerogamaespectrometria nos canais Contagem Total (CT), Potássio (K), Tório (Th) e Urânio (U).  Os dados foram, a seguir, convertidos para o formato digital na forma de imagens (“raster”) com resolução espacial de 30 x 30 m, a fim de permitir o cruzamento de informações através de técnicas de Processamento Digital de Imagens e de Sistemas de Informações Geográficas (SIG’s). A integração das imagens TM e geofísicas foi realizada com o uso da Transformação IHS, através da conversão das bandas TM para as componentes individuais I, H e S; substituindo-se a componente H, pela imagem geofísica no retorno ao espaço RGB. A análise dos produtos de sensoriamento remoto e geofísica obtidos nessa pesquisa, permitiram identificar os Domínios Morfoestruturais; identificar e delimitar as diferentes Unidades Fotolitológicas; reconhecer os principais sistemas estruturais a partir da extração e análise de lineamentos; obter informações do padrão de relevo magnético; e, principalmente, a geração de imagens temáticas de teores de radioelementos com a identificação de áreas promissoras de mineralizações. Os resultados comprovam a eficiência do emprego de técnicas de integração de dados digitais, via computador, tanto para fins de mapeamento litoestrutural, como em caráter prospectivo, em serviços geológicos de grandes áreas.|http://hdl.handle.net/10183/1507
Tecnicas de sensoriamento remoto e geoprocessamento aplicadas ao mapeamento geologico e geotecnico no municipio de Tres Cachoeiras, litoral norte do Rio Grande do Sul|1996|Open Access|Dissertação|Sensoriamento remoto;Geoprocessamento;Geotectônica;Mapeamento geotectonico : Tres cachoeiras|por|As técnicas de sensoriarnento remoto e geoprocessamento são fundamentais para processamento e integração de dados de mapeamento geológico/geotécnico, principalmente estudos de gerenciamento e planejamento. A área estudada compreende o município de Três Cachoeiras. Litoral Norte do Rio Grande do Sul o qual inclui-se na ""Reserva da Biosfera da Mata Atlântica"". O município tem st: deparado com problemas de localização de sitios adequados à disposição final dos resíduos sólidos. bem como o assentamento de loteamentos residenciais e industriais, localização de jazidas de extração de material para construção, fontes de abastecimento de água e necessidade de criação de áreas de preservação ambiental. O objetivo deste trabalho foi produzir mapeamentos da área em questão, através da pesquisa geológico-geotécnica desenvolvida com emprego de imagens de satélite e fotografias aéreas, em que as informações foram cruzadas no SIG. Baseado nisto, investigaram-se os aspectos acima mencionados. a partir de uma contribuição geológico/geotécnica ao município, incluindo-se levantamento de campo, fotointerpretação, processamento e classificação de imagens do município de Três Cachoeiras, sendo os dados integrados num sistema de geoprocessamento. Utilizando-se cartas planialtimétricas, fotografias aéreas e imagem de satélite LANDSAT TM5. foram criados planos de informação como o limite da área estudada, a estrutura viária municipal, a delimitação de reservas ecológicas baseadas na legislação ambiental vigente e, por meio do modelo numérico do terreno, a carta de declividade.  A fotointerpretação gerou planos de rede de drenagem, litológica. morfoestruturas e formações superficiais. Os dados de campo. sobrepostos às litológicas obtidas por fotointerpretação, produziram a carta litológica. No tratamento das imagem, foram gerados produtos com contraste, operações entre bandas, filtragens e análise de componentes principais, os quais contribuíram parira classificação da imagem e resultando nos planos de rochas/solos e cobertura/uso do solo (carta de uso atual do solo). O cruzamento destas informações permitiu a obtenção da carta de formações superficiais, lidrogeológica que, juntamente com as cartas litológica, declividades e uso atual do solo distribuíram os atributos do meio físico em planos elaborados por novos cruzamentos, que satisfazem o objetivo do estudo, sendo estes planos o produto final, ou seja, cartas de recomendação: a extração de materiais para construção civil; a implantação de obras de infraestrutura; a disposição de resíduos sólidos e loteamentos; geotécnica à agricultura; à implantação de áreas destinadas à preservação ambienta1 e recuperação.;Remote sensing and GIS techniques are basic for proccesing and integration in geological/geotechnical mapping mainly in management and plannig studies. The surveyerd areas was Três Cachoeiras County, north coast of Rio Grande do Sul, Brazil, whch is included in ""Mata AtLântica Biosphere Reserve"". This investgation results in thermatic maps, such as declivity, lithology, hidrogeology, surface formations and land use. Such maps were crossed and superposed. the mentioned county has problems in terms of waste sites as well as suply and environmental preservation sites. Such aspects were sunrveved from a geological/geotechnical point of view, that includes integrationby geological/geotechnical mapping image processing and classification and data integration by geoprocessing techniques. Bv means topogaphic maps, aeial photographies anrl TM LANDSAT 5 images layers were developed such as terrain numerical model, declivity map, county boundaries, ecological reserves limits based on current environmental laws and county road system. The photointerpretation generated drainage network, lithologies, morphostructures and surface formation layers. Fieldwork data was superposed on photolithology to generate a lithological map. Image processing used techniques such as enhancement, stretching, filtering and principal components analysis wich contribute to classify the image. As results, the lithology/soil and cover/land use were obtained, generated a current land use map. Surface formations map was obtained by crossing aerial surface formations data and image lithology/sois data. Environmental physics atributes were çlustered by crossing and distribuited in recomendation maps containing building material extraction, waste sites and human/industry settlement, building construction works, agriculture and environmental presevation and reclamation areas.|http://hdl.handle.net/10183/1556
Transição súbita e quebra de aproximação modulacional em sitemas espaço-temporais|2002|Open Access|Tese|Caos;Interacoes;Ondas;Momentos de transicao;Dinâmica;Equacoes nao-lineares|por|O caos e a incoerência nas interações conservativas de três ondas e a transição súbita para o caos na equação não linear de Klein Gordon são estudados. É analisada a influência da presença de caos sobre a incoerência no problema da interação de um tripleto de ondas quando um modelo de aproximação adiabática deixa de ser válido. É encontrado um limiar para o valor do descasamento do tripleto de ondas, abaixo do qual a coerência e o acoplamento entre as ondas é o comportamento dominante. Na equação não linear de Klein Gordon estudou-se a transição entre um regime de dinâmica modulacional para um de caos espaço temporal e foi encontrada uma curva crítica no plano amplitude-frequência que o divide em regiões onde só existe transição para o caos caso o valor de amplitude exceder um certo limiar.|http://hdl.handle.net/10183/1557
Ambiente baseado em componentes para o desenvolvimento de sistemas computacionais microcontrolados distribuídos|2001|Open Access|Dissertação|Automação industrial;Sistemas embarcados;Sistemas distribuídos;Microcontroladores;Componentes : Software|por|A modelagem e desenvolvimento de sistemas embarcados (""embedded systems"") de forma distribuída, tende a ser uma tarefa extremamente complexa, especialmente quando envolve sistemas heterogêneos e sincronização de tarefas. Com a utilização do modelo de componentes de software é possível descrever, de uma forma simplificada, todos os elementos de distribuição e de comunicação para este tipo de sistemas. Neste sentido, a especificação de uma ferramenta capaz de auxiliar na modelagem e no desenvolvimento deste tipo de aplicação, certamente irá tornar o trabalho mais simples. Esta dissertação inicia por uma análise comparativa entre as tecnologias passíveis de serem utilizadas na definição de sistemas distribuídos heterogêneos, focando-se principalmente nas metodologias de modelagem, e nos mecanismos e middlewares de comunicação. Dos conceitos formados a partir desta análise é descrita uma ferramenta, baseada em componentes de software. A ferramenta é uma extensão do projeto SIMOO-RT, onde foram adicionados os conceitos de componente de software, biblioteca de componentes e diagrama de implantação. Além disso, foram realizadas modificações no sistema de geração de código, para dar suporte aos novos conceitos da ferramenta. A dissertação termina com a descrição de alguns estudos de caso utilizados para validar a ferramenta.|http://hdl.handle.net/10183/1586
Métodos recursivos para o cálculo da integral de convolução|2002|Open Access|Dissertação|Integral de convolução;Metodos recursivos|por|O objetivo principal deste trabalho é apresentar um método recursivo para a determinação da resposta forçada de sistema de segunda ordem na forma de uma íntegra de concolução, proveniente da utilização de propriedades de transição da resposta impulso de tais sistemas. Descrevem-se também diversos métodos analíticos e numéricos desenvolvidos para o cálculo da resposta forçada, bem como as limitações de cada método. As vantagens do método recursivo proposto são notáveis já que não é requerido o cálculo de autovalores das matrizes nem a redução à primeira ordem, e nem o uso de hipóteses adicionais sobre natureza dos coeficientes matriciais do sistema. Como aplicação do método proposto, considera-se o cálculo da resposta dinâmica de estruturas flexíveis sujeitas a excitações arbitrárias tais como terremotos.|http://hdl.handle.net/10183/1587
Solução de um problema não linear do tipo reação-difusão na modelagem dispersão de insetos|2002|Open Access|Dissertação|Ecologia matematica;Modelagem|por|Neste trabalho tratamos da solução de um problema não linear do tipo tração-difusão, na modelagem de dispersão de insetos. Começamos estabelecendo uma lei de conservação e a partir desta, deduzimos algumas equações importantes para o desenvolvimento do nosso estudo, tais como a equação de convecção, de difusão e simultaneamente convecção e difusão. Se considerarmos uma escala de tempo que possibilite a adição ou retirada de indivíduos no meio, conforme seja considerada reprodução, migração ou morte, podemos acrescentar ao processo difusivo um termo de reação, obtendo então, a equação do tipo reação-difusão. Se o temp de reação for deendendee da densidade populacional e do tipo logístico, obtém-se a equação de Fischer. Dessa equação abordamos alguns aspectos, tais como, determinação dos estados estacionários, análise da estabillidade dos mesmos, representação gráfica no plano de fase e por último investigamos a existência de solução do tipo onda viajante. Abordamos, também, alguns exemplos apresentados na literatura, envolvendo equação da difusão com coeficiente constante e com coeficiente dependente da densidade populacional. Além disso, apresentamos o resultados obtidos com a modelagem em tempo discreto, a partir de um trabalho experimental com besouros marcados para o experimento e depois liberados Banks et al (1985) , em que os autores admitiram uma variação temporal e a partir dos dados obtidos fizeram uma estimativa para o coeficiente de difusão D (t), bem como para o coeficiente de decaimento α(t) do termo de reação linear em u.  Construimos curvas que se ajustam a essas alternativas e apresentamos esses coeficientes em versão continua D (t) e α(t), dependentes da variável tempo t. Através de uma abordagem numérica, os modelos foram comparadas da variável tempo t. Através de uma abordagem numérica, os modelos foram comparados para diversos casos, usando diferentes combinação de D constante e D variando no tempo, a constante e a variando no tempo. Além disso, analisamos tambén, o efeito da substituição do coeficiente de difusão D constante por D(t) na equação de Fisher.|http://hdl.handle.net/10183/1588
Bioclastos de organismos terrestres e marinhos na praia e plataforma interna do Rio Grande do Sul : natureza, distribuição, origem e significado geológico|2002|Open Access|Tese|Bioclastos;Plataforma interna : Rio Grande do Sul;Mamíferos fósseis;Paleontologia : Quaternário costeiro : Rio Grande do Sul|por|Fácies marinhas e costeiras associadas a eventos transgressivos-regressivos quaternários ocorrem na Planície Costeira do Rio Grande do Sul e na plataforma continental adjacente. Enquanto as fácies expostas na planície costeira apresentam uma composição essencialmente siliciclástica, as fácies submersas, hoje aflorantes na antepraia e plataforma interna, apresentam, muitas vezes, uma composição carbonática. Formada por coquinas e arenitos de praia fortemente cimentados, estas fácies destacam-se do fundo oceânico como altos topográficos submersos. Os altos topográficos da antepraia têm atuado como fonte de boa parte dos sedimentos e bioclastos de origem marinha encontrados nas praias da área de estudo. Os bioclastos carbonáticos que ocorrem nestes locais caracterizam uma Associação Heterozoa, ou seja, são formados por carbonatos de águas frias, característicos de médias latitudes, e são representados principalmente por moluscos, equinodermos irregulares, anelídeos, crustáceos decápodos, restos esqueletais de peixes ósseos e cartilaginosos, cetáceos, tartarugas e aves semelhantes à fauna atual. Além destes bioclastos de origem marinha, as praias estudadas apresentam a ocorrência de fragmentos orgânicos provenientes de afloramentos continentais fossilíferos, contendo abundantes restos esqueletais de mamíferos terrestres gigantes extintos, das ordens Edentada, Notoungulada, Litopterna, Proboscidea, Artiodactila, Perissodactila, Carnívora e Rodentia.  A concentração dos bioclastos na praia resultada da ação direta dos processos hidrodinâmicos que atuam na região de estudo (ondas de tempestade, deriva litorânea, correntes, etc). A variação no tamanho médio dos bioclastos encontrados ao longo da linha de costa está relacionada ao limite da ação das ondas de tempestades sobre o fundo oceânico, o qual é controlado principalmente pela profundidade. Os afloramentos-fonte submersos podem ser divididos em holocênicos e pleistocênicos. A tafonomia dos bioclastos pleistocênicos permite argumentar que após o penúltimo máximo transgressivo que resultou na formação do sistema deposicional Laguna-Barreira III (aproximadamente 120 ka) parte dos depósitos lagunares permaneceram emersos e não estiveram sob a ação marinha (barrancas do arroio Chuí, com a megafauna preservada in situ), enquanto que parte dos depósitos lagunares esteve sob ação direta do ambiente praial. Em diversas feições submersas observam-se coquinas contendo fósseis de mamíferos terrestres, indicando o retrabalhamento dos sedimentos lagunares em ambiente praial. As coquinas que apresentam moluscos pouco arredondados e de maior granulometria são aqui definidas, informalmente, como Coquinas do Tipo 1. Como conseqüência da última regressão pleistocênica (iniciada após o máximo transgressivo de 120 ka) estas coquinas ficaram submetidas a uma exposição subaérea. Este fato possibilitou a dissolução diferenciada dos componentes carbonáticos existentes nos depósitos (coquinas e arenitos) e sua recristalização (calcita espática) em ambientes saturados em água doce.  A Transgressão Pós-Glacial (iniciada em torno de 18 ka) foi responsável pelo retrabalhamento dos arenitos e coquinas, recristalizando mais uma vez os elementos carbonáticos. Devido ao seu grau de consolidação estes depósitos resistiram à erosão associada à elaboração da superfície de ravinamento e encontram-se atualmente expostos na antepraia e, mesmo, na linha de praia atual. Pelo menos há 8 ka houve novamente um período favorável à precipitação de carbonato de cálcio, ocorrendo a litificação de rochas sedimentares em uma linha de praia numa cota batimétrica inferior a atual. Neste intervalo de tempo formaram-se as coquinas e arenitos não recristalizados, apresentando fragmentos de moluscos muito fragmentados e arredondados e de menor granulometria, aqui definidas, informalmente, como Coquinas do Tipo 2. A interpretação da tafonomia dos bioclastos de idade holocênica sugere pelo menos duas fácies deposicionais: (a) Fósseis articulados numa matriz areno-síltica, preenchidos por silte e argila, interpretados como originalmente depositados em regime transgressivo no ambiente Mesolitoral (foreshore) para Infralitoral superior (upper shoreface), com baixa ação de ondas. (b) Fragmentos de carapaças e quelas isoladas encontradas numa coquina fortemente cimentada por calcita espática, por vezes recristalizada, interpretados como concentrados na Zona de Arrebentação por ondas de tempestades. A dinâmica costeira atual retrabalha novamente os sedimentos inconsolidados enquanto as rochas sedimentares consolidadas (formadas pelas Coquinas Tipo 1 e 2) resistem parcialmente à erosão e constituem os altos topográficos submersos (parcéis) descritos neste trabalho.|http://hdl.handle.net/10183/1607
Estudo do calor específico de compostos Heusler paramagnéticos, da série do níquel Ni 2 Tal, onde T=Ti, Zr, Hf, V, Nb e Ta|1997|Open Access|Dissertação|Calor especifico;Calorimetria;Medidas de calor especifico;Ligas de heusler;Estruturas eletronicas;Dinamica de rede;Densidade de estados eletronicos;Temperatura de debye;Calorímetro;Instrumentação|por|Neste trabalho apresentamos os resultados experimentais da medida do calor específico dos compostos Heusler da série Ni2TA1, onde T =Ti, Zr,Hf,V,Nb, Ta. Estas medidas foram feitas utilizando-se um calorímetro adiabático que atua na faixa de 1,84 K a 10,3 K e visaram o estudo da estrutura elêtrônica dos compostos em termos da densidade de estados eletrônicos à nível de Fermi, bem como da dinâmica da rede cristalina, com o uso dos modelos de Debye e de Einstein para o calor específico. Apresentamos, também, uma descrição do calorímetro, seu funcionamento e as alterações ocorridas em função deste trabalho.|http://hdl.handle.net/10183/1610
OZJ : uma ferramenta para geração de óraculos para teste de software a partir de especificação formal|2002|Open Access|Dissertação|Engenharia : Software;Testes : Software;Especificacao formal|por|A literatura sobre Teste de Software apresenta diversas estratégias e metodologias que definem critérios eficazes e automatizáveis para selecionar casos de teste capazes de detectar erros em softwares. Embora eficientes na descoberta de erros, as técnicas de seleção de casos de teste exigem que uma quantidade relativamente grande de testes seja realizada para satisfazer os seus critérios. Essa característica acarreta, em parte, um alto custo na atividade de teste, uma vez que, ao fim de cada teste deve-se verificar se o comportamento do software está ou não de acordo com os seus requisitos. Oráculo para teste de software é um mecanismo capaz de determinar se o resultado de um teste está ou não de acordo com os valores esperados. Freqüentemente, assume-se que o próprio projetista de teste é o responsável por esta tarefa. A automatização da atividade dos oráculos deu origem a oráculos automáticos, os quais são capazes de determinar o bom ou mau funcionamento do software a partir de uma fonte de informação confiável.  Ao longo dos anos, a especificação formal vêm sendo largamente utilizada como fonte de informação para oráculos automáticos. Diversas estratégias vêm propondo geradores de oráculos baseados em especificações formais. Dentre as características marcantes dessas estratégias, cita-se aquelas que são aplicáveis a implementações derivadas a partir da estrutura da especificação e aquelas que geram oráculos a partir de técnicas específicas de seleção de casos. Essas características, entretanto, limitam a aplicação abrangente dos oráculos por restringi-los tanto a implementações derivadas diretamente de especificações como ao uso de técnicas específicas de seleção de casos de teste. Este trabalho apresenta um estudo sobre os geradores de oráculos para teste de software, identifica aspectos fundamentais que regem seu processo de construção e propõe uma estratégia que permite a geração de oráculos semi-automaticamente, mesmo para implementações não derivadas diretamente da estrutura da especificação. A estratégia proposta é, também, aplicável aos casos de teste derivados de qualquer técnica de seleção de casos de teste.|http://hdl.handle.net/10183/1618
APSEE-Reuse : um meta-modelo para apoiar a reutilização de processos de software|2002|Open Access|Tese|Engenharia : Software;Reutilizacao : Software;Processo : Software|por|Dentre as principais áreas que constituem a Ciência da Computação, uma das que mais influenciam o mundo atual é a Engenharia de Software, envolvida nos aspectos tecnológicos e gerenciais do processo de desenvolvimento de software. Software tornou-se a base de sustentação de inúmeras organizações dos mais diversos ramos de atuação espalhados pelo planeta, consistindo de um elemento estratégico na diferenciação de produtos e serviços atuais. Atualmente, o software está embutido em sistemas relacionados a infindável lista de diferentes ciências e tecnologias. A Tecnologia de Processo de Software surgiu em meados da década de 1980 e representou um importante passo em direção à melhoria da qualidade de software através de mecanismos que proporcionam o gerenciamento automatizado do desenvolvimento de software. Diversas teorias, conceitos, formalismos, metodologias e ferramentas surgiram nesse contexto, enfatizando a descrição formal do modelo de processo de software, para que possa ser automatizado por um ambiente integrado de desenvolvimento de software. Os modelos de processos de software descrevem o conhecimento de uma organização e, portanto, modelos que descrevem experiências bem sucedidas devem ser continuamente disseminados para reutilização em diferentes projetos. Apesar da importância desse tópico, atualmente apenas uma pequena porção do conhecimento produzido durante o desenvolvimento de software é mantido para ser reutilizado em novos projetos. Embora, à primeira vista, o desafio de descrever modelos reutilizáveis para processos de software pareça ser equivalente ao problema tratado pela tradicional área de reutilização de produtos software, isso é apenas parcialmente verdade, visto que os processos envolvem elementos relacionados com aspectos sociais, organizacionais, tecnológicos e ambientais.  A crescente complexidade da atual modelagem de processos vem influenciando a investigação de tecnologias de reutilização que sejam viáveis nesse campo específico. A investigação conduzida nesse trabalho culminou na especificação de um meta-modelo que tem como objetivo principal aumentar o nível de automação fornecido na reutilização de processos, apoiando a modelagem de processos abstratos que possam ser reutilizados em diferentes contextos. O meta-modelo proposto por esse trabalho - denominado APSEE-Reuse - fornece uma série de construtores sintáticos que permitem que os diferentes aspectos desse contexto sejam descritos segundo múltiplas perspectivas, complementares entre si, contribuindo para diminuir a complexidade do modelo geral. A solução proposta destaca-se por fornecer um formalismo para modelagem de processos, o qual é integrado à uma infraestrutura de automação de processos de software, permitindo que a reutilização esteja intimamente relacionada com as outras etapas do ciclo de vida de processos. Os diferentes componentes envolvidos na definição do modelo APSEE-Reuse proposto foram especificados algebricamente, constituindo uma base semântica de alto 15 nível de abstração que deu origem a um conjunto de protótipos implementados no ambiente PROSOFT-Java. O texto ainda discute os experimentos realizados com o meta-modelo proposto na especificação de diferentes estudos de casos desenvolvidos a partir de exemplos retirados na literatura especializada, e de processos que fornecem soluções em contextos e necessidades específicas de projetos desenvolvidos no PPGC-UFRGS. Finalmente, são apresentadas considerações acerca dos trabalhos relacionados, os elementos críticos que influenciam a aplicabilidade do modelo e as atividades adicionais vislumbradas a partir do trabalho proposto.|http://hdl.handle.net/10183/1622
Efeito do transporte radial de partículas na eficiência da geração de corrente por ondas do tipo híbrida inferior em tokamaks|2002|Open Access|Dissertação|Geração de corrente;Ondas;Processos de transporte;Tokamaks;Eletrons;Difusão;Análise numérica;Analise no domínio tempo;Processos de colisao|por|Usamos a teoria quase-linear para estudar os efeitos do transporte radial de partículas na eficiência da geração de corrente por ondas do tipo híbrida inferior (lower hybrid ou LH), em um tokamak modelado como uma lâmina. Nossos resultados numéricos foram obtidos com cinco diferentes modelos do termo de transporte e indicaram que embora a potência absorvida e a corrente gerada possam ser modificadas por efeito do transporte, a proporção de variação dessas quantidades não é muito sensível a uma forma particular do termo de transporte. Na formulação quase-linear utilizada, a evolução no tempo da função distribuição de elétrons, em um dado ponto da geometria de lâmina proposta, ocorre sob a ação de ondas do tipo híbrida inferior, colisões e transporte, e é descrita pela seguinte equação: 8rfe = (8rfehH + (8rfe)COL + (8rfeh . Oterceiro termo pretende demonstrar a natureza e a magnitude dos efeitos de transporte, e é dado pela seguinte forma: (8rfeh = 8s [DT(S) 8sie] , com um coeficiente para difusão espacial dependente de posição. Utilizamos cinco formas totalmente arbitrárias para a dependência de posição, com as quais pretendemos verificar a sensibilidade do processo de geração de corrente a aspectos do termo de difusão.|http://hdl.handle.net/10183/1624
Influência de bolhas de hélio e da microestrutura sobre a evolução térmica de filmes de alumínio implantados com cobre|2002|Open Access|Dissertação|Tamanho de partícula;Filmes finos;Átomos;Alumínio;Solucoes solidas;Implantação de íons;Hélio;Retroespalhamento rutherford;Microscopia eletrônica de transmissão;Íons|por|Este trabalho apresenta os resultados de um estudo sistemático sobre as influências do tamanho de grão de filmes finos de Al e da implantação de íons de He sobre a evolução térmica de distribuições de átomos de Cu e formação de precipitados de Al-Cu. Filmes finos de Al depositados sobre substrato de SiO2/Si através de dois processos diferentes foram implantados com íons de Cu+ e He+ produzindo uma solução sólida supersaturada de Al-Cu (≈ 2,5 a 3,5 at. %) e nano-bolhas de He. Os valores de energia dos íons foram escolhidos de tal forma a produzirem uma camada rica em Cu e He na região a aproximadamente 100nm da superfície. Tais filmes foram tratados termicamente em alto-vácuo nas temperaturas de 200ºC e 280ºC por tempos de 0,5h e 2h. Os filmes foram analisados por Retroespalhamento Rutherford, para determinação do perfil de concentração dos átomos de Cu, e por Microscopia Eletrônica de Transmissão, para determinação da microestrutura do Al e dos sistemas de nano-partículas Al-Cu e Al-He. Os resultados experimentais mostraram que a evolução térmica da distribuição dos átomos de Cu e a formação de precipitados de Al-Cu são significativamente afetadas pela configuração e tamanho de grão do filme de Al e pelas implantações de He.  O presente estudo mostrou que existe uma forte correlação entre o fluxo de vacâncias e a estabilidade da microestrutura de filmes finos de Al (Al/SiO2/Si) implantados com íons de Cu+ e He+ e tratados termicamente. A possibilidade de controlar os fluxos de vacâncias através de configurações da microestrutura dos filmes de Al é, portanto, um tema de grande interesse tecnológico relacionado a durabilidade das interconexões metálicas de dispositivos microeletrônicos.|http://hdl.handle.net/10183/1625
Métodos de fatoração de números inteiros|2002|Open Access|Dissertação|Fatoracao;Números inteiros|por|A fatoração de números inteiros é um assunto que, embora muito antigo, desperta cada vez mais interesse. Existem vários métodos de criptografia de chave pública, baseados não só em fatoração de inteiros, mas também em resolução de logarítmos discretos, por exemplo, cuja segurança depende da ineficiência dos métodos de fatoração conhecidos. Este trabalho tem como objetivo descrever os principais métodos de fatoração utillizados hoje em dia. Primeiramente, três métodos elementares serão estudados: o método de Fermat e os métodos Rho e p - 1 de Pollard. A seguir, os dois mais poderosos métodos de fatoração para inteiros sem forma especial: o método de curvas elípticas, e o método de peneira quadrática, os quais tomam como base os métodos p - 1 e de Fermat, respectivamente.|http://hdl.handle.net/10183/1626
Implementação de um sistema de síntese de alto nível baseado em modelos java|2002|Open Access|Dissertação|Microeletrônica;Sintese : Alto nivel;Vhdl;Sistemas embarcados|por|Este trabalho apresenta uma metodologia para a geração automática de ASICs, em VHDL, a partir da linguagem de entrada Java. Como linguagem de especificação adotou-se a Linguagem Java por esta possuir características desejáveis para especificação a nível de sistema, como: orientação a objetos, portabilidade e segurança. O sistema é especificamente projetado para suportar síntese de ASICs a partir dos modelos de computação Máquina de Estados Finita e Pipeline. Neste trabalho, adotou-se estes modelos de computação por serem mais usados em sistemas embarcados As principais características exploradas são a disponibilização da geração de ASICs para a ferramenta SASHIMI, o alto nível de abstração com que o projetista pode contar em seu projeto, as otimizações de escalonamento realizadas automaticamente, e o sistema ser capaz de abstrair diferentes modelos de computação para uma descrição em VHDL. Portanto, o ambiente permite a redução do tempo de projeto e, consequentemente, dos custos agregados, diminuindo a probabilidade de erros na elaboração do projeto, portabilidade e reuso de código – através da orientação a objetos de Java – podendo-se proteger os investimentos prévios em desenvolvimento de software. A validação desses conceitos foi realizada mediante estudos de casos, utilizando-se algumas aplicações e analisando os resultados obtidos com a geração dos ASICs.|http://hdl.handle.net/10183/1627
Extração de informação sobre bases de dados textuais|2002|Open Access|Dissertação|Armazenamento : Dados;Recuperacao : Informacao;Base : Dados textuais|por|Com a crescente popularização dos microcomputadores e da rede mundial de informação, Internet, uma enorme variedade e quantidade de informações estão se tornando acessíveis a um número cada vez maior de pessoas. Desta forma, também cresce a importância de se extrair a informação útil que está no grande conjunto das informações disponibilizadas. Hoje há muito mais dados na forma de textos eletrônicos do que em tempos passados, mas muito disto é ignorado. Nenhuma pessoa pode ler, entender e sintetizar megabytes de texto no seu cotidiano. Informações perdidas, e conseqüentemente oportunidades perdidas, estimularam pesquisas na exploração de várias estratégias para a administração da informação, a fim de estabelecer uma ordem na imensidão de textos. As estratégias mais comuns são recuperação de informações, filtragem de informações e outra relativamente nova, chamada de extração de informações. A extração de informações tem muitas aplicações potenciais. Por exemplo, a informação disponível em textos não-estruturados pode ser armazenada em bancos de dados tradicionais e usuários podem examiná-las através de consultas padrão. Para isso, há um complexo trabalho de gerenciamento, que é conseqüência da natureza não estruturada e da difícil análise dos dados. Os dados de entrada, que são os textos semi ou não-estruturados, são manipulados por um processo de extração configurado através de bases de conhecimento criadas pelo usuário do sistema. Esta dissertação tem como objetivo a definição de uma linguagem, com base em uma arquitetura de múltiplos níveis, para extrair satisfatoriamente as informações desejadas pelo usuário, presentes em bases de dados textuais. Também faz parte deste trabalho a implementação de um protótipo que utiliza a linguagem proposta.|http://hdl.handle.net/10183/1628
O Uso de ícones na visualização de informações|2000|Open Access|Dissertação|Computação gráfica;Visualizacao cientifica;Icone;Mineracao : Dados;Visualizacao : Informacao|por|A visualização de informações congrega um conjunto de técnicas que têm por objetivo facilitar o entendimento de informações a partir de representações visuais. Nesta área, parte-se do pressuposto de que uma representação visual de dados ou informações proporciona uma forma mais simples e intuitiva de entendê-los e, com isso, inferir mais rapidamente e com maior precisão o seu significado. Normalmente, a tarefa de análise de dados é realizada com o auxílio de ferramentas de acordo com a natureza dos dados e do estudo sendo realizado. As técnicas de visualização de dados e informações não substituem ferramentas de análise específicas. Elas podem ser usadas como primeira aproximação do processo de análise, quando sintetizam de forma visual uma grande massa de dados, permitindo escolher partes do volume de dados para análise mais detalhada, ou como forma de apresentação de dados já reduzidos. Quando o subconjunto dos dados de interesse está disponível, a utilização de uma ferramenta de visualização proporciona maior agilidade nas análises e padrões na massa de dados podem ser descobertos visualmente. Uma das classes de técnicas de visualização utilizada nesta área é a icônica. Um ícone (ou glifo) é um objeto com geometria e aparência paramétricas, as quais podem ser arbitrariamente vinculadas a dados.  A função de um ícone é agir como uma representação simbólica, que mostra as características essenciais de um domínio de dados ao qual o ícone se refere. Assim é possível obter uma visualização dos dados de uma forma mais clara e compacta. Em geral, ícones são utilizados para representar dados multidimensionais, ou seja, múltiplos atributos associados a uma posição num espaço qualquer, ou a entidades em estudo. O presente trabalho analisa o uso de ícones na visualização de informações. São discutidos os conceitos fundamentais de visualização de informações e sua relação com a área de mineração de dados. O uso de ícones em diversos trabalhos apontados na literatura é apresentado, sendo abordada a questão de geração automática de ícones, mais flexível do que os conjuntos fixos providos pelos sistemas de visualização estudados. Uma proposta para gerar ícones de forma automática baseada numa especificação paramétrica dos ícones é utilizada em um conjunto de dados característicos de espécimes animais estudados por biólogos do Departamento de Genética da UFRGS.|http://hdl.handle.net/10183/1629
Suporte a consultas no ambiente temporal de versões|2002|Open Access|Dissertação|Banco : Dados;Banco : Dados temporais;Versoes : Banco : Dados;Linguagens : Consulta|por|O Modelo Temporal de Versões (TVM Vesions Model) foi proposto com base na união de um modelo de versões com informações temporais. Esse modelo permite o armazenamento de alternativas de projeto, o armazenamento da história dos dados em evolução, bem cmoo a reconstrução do estado da base em qualquer data passada, sem o uso de operações complexas de backup e recovery. Para realizar consultas nesse modelo foi definida uma linguagem de consulta, a TVQL (Temporal Versioned Query Language). Além das consultas básicas realizadas pela linguagem padrão AQL, a TVQL permite novas consultas que retornam valores específicos das características de tempo e versões, estabelecendo um comportamento o mais homogêneo possível para elementos normais e temporais vesionados. O objetivo principal deste trabalho e possibilitar a realização de consultas TVQL em um banco de dados convencional. Nesse contexto, o mapeamento da TVQL é implementando através da tradução de todas as propriedades e funções definidas na TVQL para SQL. Para que isso seja possível é necessário queos dados também estejam nesse banco de dados. Então, faz-se necessário o mapeamento das classes da hierarquia do TVM, bem como das classes da aplciação, para o banco de dados. Adicionalmente, é implementado um protótipo de uma interface de consultas realizadas em TVQL, para testar o funcionamento tanto da TVL como do seu mapeamento.|http://hdl.handle.net/10183/1630
Desenvolvimento de regras de pronúncia para a síntese de fala em língua portuguesa|2002|Open Access|Dissertação|Linguística computacional;Síntese : Fala;Portugues|por|Sabe-se que a fala é a principal maneira de comunicação entre as pessoas. A Síntese de fala (geração automática da fala pelo computador) tem recebido atenção da comunidade acadêmica e profissional por várias décadas. Ela envolve a conversão de um texto de entrada em fala, usando algoritmos e algumas formas de fala codificada. O texto pode ser digitado pelo teclado ou obtido por reconhecimento de caracteres ou, ainda, obtido de um banco de dados. A síntese de fala pode ser usada em vários domínios de aplicação, tais como: auxílio para deficientes visuais, telecomunicações, multimídia, etc. Este trabalho apresenta um estudo sobre a produção da fala e da área de síntese de fala visando servir de subsídio para dissertações e pesquisas futuras, bem como para o Projeto Spoltech, um projeto de cooperação entre os Estados Unidos e o Brasil para o avanço da tecnologia da língua falada no Brasil (Português Brasileiro). Dentro deste estudo serão apresentadas as principais técnicas de síntese de fala, entre as quais destaca-se: Texto para Fala (TPF). Problemas de separação de sílabas, determinação da sílaba tônica, pronunciação das vogais “e” e “o” como um fonema aberto ou fechado, etc, são enfrentados dentro do contexto da área de síntese de fala para o português falado no Brasil.  Tendo conhecimento destes problemas, o principal objetivo deste trabalho será criar regras para resolver o problema de pronunciação das vogais “e” e “o” de forma automática, visando obter produção sonora mais inteligível, por intermédio da implementação de um analisador estatístico, o qual verificará a letra anterior e posterior ao “e” ou “o” de uma palavra e, com isso, determinar a pronúncia dos mesmos para aquela seqüência de letras. As mesmas poderão tornar-se regras válidas para a solução do problema se atingirem 80% dos casos de ocorrência no dicionário com fonema “e” ou “o” aberto (limiar), sendo que elas serão lidas por um interpretador Scheme utilizado pelo programa Festival - ferramenta para a construção de sistemas de síntese de fala desenvolvida pelo Centre for Speech Technology Research (University of Edinburgh, Reino Unido), a qual utiliza TPF como método de síntese. Sabendo-se que o Festival gera os fonemas “e” e “o” como fechados se não há uma regra para inferir o contrário, serão consideradas apenas as regras encontradas para os fonemas abertos. Para possibilitar esta análise será utilizado um dicionário eletrônico de pronunciação (com 19.156 palavras), o qual possui a palavra e a sua respectiva pronúncia, conforme pode-se verificar no exemplo do Anexo 1.|http://hdl.handle.net/10183/1631
Modelo de sacola quiral com superfície difusa : um estudo das propriedades dos hádrons|2003|Open Access|Tese|Modelo de sacola;Simetrias quiral;Hadrons relativisticos;Pions;Constantes de acoplamento de partículas elementares;Massa bariônica|por|o modelo de sacola difusa é um modelo hadrônico que possui aspectos tanto do modelo de sacola do MIT (conservação da energia e momentum, energia de vácuo da QCD) quanto dos modelos de potencial relativísticos (confinamento obtido através de um potencial). O modelo desenvolvido também é um modelo quiral, com a propriedade única de que o campo piônico é suprimido no interior da sacola por meio de um potencial escalar, e no entanto a simetria quiral é preservada. O modelo também é único em que podese controlar o quanto o campo piônico pode penetrar no interior da sacola (em todos os outros modelos, os píons ou entram livremente na sacola ou permanecem totalmente excluídos de seu interior). Nós calculamos as massas do octeto fundamental dos bárions levando em conta as correções de centro de massa, troca de um glúon e troca de um píon. Também calculamos a constante de acoplamento píon-núcleon, a carga axial do núcleon, assim como os raios de carga, momentos magnéticos e fatores de forma eletromagnéticos do próton e do neutron. Exceto pelos fatores de forma eletromagnéticos, a concordância com os resultados experimentais foi excelente, e os resultados indicam que o campo piônico é suprimido somente na vizinhança do centro da sacola.|http://hdl.handle.net/10183/1632
Especificação de uma metodologia de avaliação para ambientes de gerenciamento de cursos a distância|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Gerência : Cursos a distância|por|A Internet pelos mecanismos de informação, comunicação e cooperação que proporciona, vem se afirmando ao longo dos últimos anos, como uma interessante opção à viabilização da educação a distância e a aprendizagem virtual, através do uso de Ambientes de Gerenciamento de Cursos a Distância, que procuram oferecer um amplo e consistente conjunto de ferramentas de suporte à comunicação, às atividades de alunos e de professores, à avaliação e monitoração dessas atividades, bem como à coordenação e à administração do sistema [KIS 2002]. Procurando identificar as características, propriedades e atributos mais relevantes e usualmente oferecidos por ambientes, foram estudados os ambientes: WebCT, TopClass e Learning Space, como referências de ambientes comerciais e AulaNet, TelEduc e EAD UNISC, como exemplos de ambientes acadêmicos. Como resultado desse estudo, foi elaborada uma estrutura hierárquica de três níveis: áreas, funcionalidades e características, onde o primeiro nível, representa as principais áreas de abrangência de um ambiente, o segundo, as funcionalidades consideradas em cada área e, o terceiro, as características que definem cada uma das funcionalidades.  A partir dessa estruturação em níveis, foi elaborada e proposta uma metodologia de avaliação para permitir a verificação quantitativa e qualitativa das funcionalidades e características apresentadas por Ambientes de Gerenciamento de Cursos a Distância. Como principais características desta metodologia, podem ser destacadas: a estrutura hierárquica em níveis, a atribuição de pesos aos diferentes níveis de acordo com o grau de importância, a possibilidade de inclusão de ambientes pelos próprios fabricantes ou responsáveis, a disponibilização de um perfil default de avaliação, a apresentação do ranking de ambientes segundo um perfil de avaliação, a comparação de ambientes quanto as suas características e funcionalidades e a possibilidade de alteração ou criação de novos perfis de avaliação, segundo as diferentes necessidades de cada usuário. Para validar essa metodologia, foi desenvolvido e disponibilizado na Web para consulta geral, um sistema de Benchmark, que além de pontuar os sistemas cadastrados segundo o perfil criado nesse trabalho, possibilita a definição de novos perfis para avaliação, assim como consultas e comparações quanto às funcionalidades e às características apresentadas pelos ambientes, auxiliando a diminuir o nível de subjetividade dos usuários, nos processos relacionados à escolha do ambiente mais adequado as suas necessidades.|http://hdl.handle.net/10183/1633
Estudo exploratório da deposição de filmes de diamante em alguns substratos cerâmicos|2002|Open Access|Dissertação|Filmes de diamante;Deposição de vapor químico;Substratos;Cerâmica;Silício;Nucleação;Espectroscopia Raman;Difratometria de raios-x;Microscopia;Zircônio|por|o presente trabalho é um estudo exploratório a respeito da síntese de filmes de diamante via deposiçãoquímica a vapor (CVD) sobre alguns substratos cerâmicos: diboreto de titânio (TiB2), ítria (Y20a), zircão (ZrSi04), zircônia parcialmente e totalmente estabilizada com ítria (Zr02), pirofilita ( Al2Si4OlO(OHh), .alumina (Al2Oa) e nitreto de boro hexagonal (h-BN). Estes substratos foram produzidos, em sua maioria, a partir da sinterização de pós micrométricos em altas temperaturas. Além do estudo em relação a possíveis candidatos alternativos ao tradicional silício para o crescimento de filmes auto-sustentáveis, procuramos encontrar substratos onde o filme aderisse bem e cujas propriedades tribológicas pudessem ser melhoradas com o recobrimento com filme de diamante.Dentre os materiais selecionados, constatamos que a topografia da superfície relacionada à densidade de contornos de grão, desempenha um papel relevante na nucleação do diamante. Além disso, os materiais que favorecem a formação de carbonetos conduziram a melhores resultados na nucleação e crescimento do filme, indicando que a ação da atmosfera reativa do CVD com o substrato também contribui decisivamente para o processo de nucleação. A partir dos resultados obtidos, concluímos que a aderência do filme de diamante ao zircão é excelente, assim como a qualidade do filme, o que pode serexplorado convenientemente caso as propriedades mecânicas do sinterizado de zircão sejam adequadas. No caso da zircônia parcialmente estabilizada, os resultados obtidos foram surpreendentes e este material poderia substituir o convencional substrato de silício para a deposição de filmes auto-sustentados de diamante, com inúmeras vantagens, dentre elas o fato de ser reutilizável e de não ser necessário ataque com ácidos para remoção do substrato, o que evita a geração de resíduos químicos.;This work presents the results of an exploratory study about the diamond synthesis through Chemical Vapor Deposition (CVD) over some ceramic substrates: titanium diboredo (TiB2), yttria (1203), zircon (ZrSi04), zirconia partially and totally stabilizade (Zr02), pyrophyllite (Al2Si401O(OHh), alumina (Al203) and hexagonal boron nitride (h - BN). Most of these substrates were prepared from the sintering of micrometer-size grains under high temperature. Beside the search for possible candidates to replace the conventionally used silicon substrate to grow self-standing films, we also search for substrates with interesting tribological properties that could be improved if the substrates were coated with a well adherent diamond filmo From the set of ceramic materials studied, we found out that the surface topography, related to the grain boundary density, plays, an important role in diamond nucleation. Moreover, the materiaIs where the carbide formation is favored showed better results concerning diamond nucleation and growth, compared to the materiaIs where there is no carbide formation. The reaction of thesubstrate material with the CVD environment also plays an important role in nucleation processo In the particular case of zircon substrate, the adhesion of the diamond filmis excellent, as well as the quality of the film, that can be very interesting from 'the technological point of view. In the case of zirconia partially stabilizade, the results are very interesting and this material could be replace the traditional silicon substrate to grow self-standing diamond films, with advantages such as it can be used several times and there is no need to use chemical etching to remove the substrate, avoiding chemical waste disposal.|http://hdl.handle.net/10183/1634
Estudo da perda de energia e da flutuação estatística da perda de energia de íons de lítio em direções aleatórias do silício|2000|Open Access|Dissertação|Retroespalhamento rutherford;Perda de energia de particulas;Litio;Silício;Implantacao ionica;Estado amorfo;Poder de freamento nuclear;Poder de freamento eletronico;Altas energias;Epitaxia de feixe molecular|por|Usando a técnica de Retroespalhamento Rutherford (RBS). no presente trabalho medimos a perda de energia e a flutuação estatística da perda de energia (straggling) como função da energia para íons de Li em alvos de silicio amorfo. Através do método dos marcadores. com amostras produzidas por implantação iônica e por epitaxia de feixe molecular,o poder de freamento foi medido em um intervalo de energia entre 250keV e 9MeV enquanto que o straggling foi determinado em um intervalo que foi de 450keV a 3MeV. Os resultados experimentais foram comparados com medidas prévias e com cálculos semi-empiricos realizados por Ziegler. Biersack e Littmark e por Konac et el., sendo encontrado um razoável acordo teórico-experimental. Foram também realizados cálculos baseados nos modelos teóricos de Aproximação por Convolução Perturbativa (PCA) e Aproximação por Convolução Unitária (UCA) propostos por Grande e Schiwielz  O modelo PCA apresentou resultados aceitáveis para energias acima de 6MeV enquanto que os valores apresentados pelo divergem dos valores experimentais para energias abaixo de 2MeV. As medidas de straggling quando comparados com as predições da teoria de Bohr mostraram discrepâncias que foram entre 20% e 60%. Cálculos feitos com a aproximação para encontros binários (BEA) usando-se a secção de choque de Thompsom e a secção de choque proposta no trabalho de Vriens foram comparados com as medidas experimentais. Os resultados usando-se a secção de choque de Thompsom divergem por até 50%, enquanto que os valores calculados a secção de choque proposta por Vriens mostram uma concordância razoável para energias abaixo de 1MeV.|http://hdl.handle.net/10183/1635
O método de divisão-e-conquista na solução de auto-sistemas de matrizes simétricas|2002|Open Access|Dissertação|Matrizes tridiagonais simétricas;Método de divisão-e-conquista;Software MAPLE;Método QR|por|O presente trabalho apresenta um estudo do método de divisão-e-conquista para solução dos auto-sistemas de matrizes tridiagonais simétricas. Inicialmente, explanamos a parte teórica, e posteriormente, por meio de exemplos numéricos mostramos seu funcionamento. Para a realização deste estudo, utilizou-se o software Maple como ferramenta auxiliar. Realizamos comparações e análises dos auto-sistemas encontrados com as rotinas DSTEDC e DSTEQR do LAPACK, que utilizam respectivamente o método de divisão-e-conquista e o método QR e também comparamos estes com os resultados encontrados por nós. Verificamos por meio de testes os tempos, que as rotinas citadas, dispendem na resolução de alguns auto-sistemas. Os resultados apresentados mostram que o método de Divisão-e-Conquista é competitivo com o método tradicional, QR, para o cálculo de autovalores e autovetores de matrizes tridiagonais simétricas.|http://hdl.handle.net/10183/1642
Um Sistema multiagente para o Simulador Soccerserver|2001|Open Access|Dissertação|Inteligência artificial;Robótica;Sistemas multiagentes;Jogos : Estrategia;Simulação;Futebol : robôs|por|O interesse de pesquisa da comunidade de Inteligência Artificial em Sistemas Multiagentes tem gerado o crescimento da utilização de técnicas de agentes nas mais diversas áreas da ciência da computação. Isso ocorre, principalmente, devido à variedade de aplicações em que esses sistemas podem ser usados, como por exemplo: jogos de computadores, interfaces adaptativas, simulação e controle de processos industriais. The Robot World Cup Initiative (RoboCup) é uma tentativa de estimular a área de Inteligência Artificial e, principalmente de Sistemas Multiagentes, por promover um problema padrão, jogar futebol, onde uma ampla cadeia de tecnologias podem ser integradas, examinadas e comparadas. A utilização do ambiente da RoboCup para a simulação de uma partida de futebol (simulador Soccerserver) permite a avaliação de diferentes técnicas de Sistemas Multiagentes (planejamento de estratégias, conhecimento em tempo real, colaboração de agentes, princípios de agentes autônomos, entre outros) e estimula as pesquisas, investigações e testes que possibilitem a construção gradativa de agentes avançados. O presente trabalho tem por objetivo o desenvolvimento de um time de futebol para o simulador Soccerserver.  A idéia principal é desenvolver agentes jogadores que demonstrem um nível considerável de competência para a realização de suas tarefas, como percepção, ação, cooperação, estratégias pré-definidas, decisão e previsão. Inicialmente, apresenta-se uma visão geral sobre Inteligência Artificial Distribuída e sobre o simulador Soccerserver, pré-requisitos para o restante do trabalho. A seguir, é realizado um estudo sobre algumas arquiteturas de agentes (clientes) do Soccerserver. A arquitetura proposta na dissertação, suas principais características e a sua materialização em um protótipo desenvolvido correspondem à parte principal do trabalho. Finalmente são apresentados os testes realizados e as conclusões do trabalho.|http://hdl.handle.net/10183/1650
Estimativa da produtividade no desenvolvimento de software|2001|Open Access|Dissertação|Engenharia : Software;Produtividade : Software;Qualidade : Software;Metricas : Software|por|Este trabalho apresenta uma ferramenta para gerenciamento de projetos, priorizando as fases de planejamento e o controle do desenvolvimento de software. Ao efetuar o planejamento de um projeto é necessário estimar o prazo, o custo e o esforço necessário, aplicando técnicas já aprovadas, existentes na literatura, tais como: Estimativa do Esforço, Estimativa de Putnam, Modelo COCOMO, Análise de Pontos por Função, Pontos de Particularidade e PSP. É necessária a utilização de uma ferramenta que automatizem o processo de estimativa. Hoje no mercado, encontram-se várias ferramentas de estimativas, tais como: ESTIMACS, SLIM, SPQR/20, ESTIMATE Professional. O controle do desenvolvimento do projeto está relacionado ao acompanhamento do projeto, do profissional e da própria estimativa de custo e esforço de desenvolvimento. Nenhuma das ferramentas estudadas permitiu o controle do projeto por parte da gerência, por isto esta se propondo o desenvolvimento uma nova ferramenta que permita o planejamento e controle do processo de desenvolvimento. Esta ferramenta deve permitir a comparação entre as diversas técnicas de estimativas, desde que baseadas na mesma medida de tamanho: pontos por função. Para exemplificar o uso desta ferramenta, foram aplicados dois estudos de casos desenvolvidos pela empresa Newsoft Consultoria de Informática.|http://hdl.handle.net/10183/1651
Study of the audio coding algorithm of the MPEG-4 AAC standard and comparison among implementations of modules of the algorithm|2002|Open Access|Dissertação|Processamento : Sinais;Processamento de sinais acústicos;MPEG-4 AAC;Audio coding;Perceptual coders;Psychoacoustics;PC;MMX;DSP;VHDL|eng|Audio coding is used to compress digital audio signals, thereby reducing the amount of bits needed to transmit or to store an audio signal. This is useful when network bandwidth or storage capacity is very limited. Audio compression algorithms are based on an encoding and decoding process. In the encoding step, the uncompressed audio signal is transformed into a coded representation, thereby compressing the audio signal. Thereafter, the coded audio signal eventually needs to be restored (e.g. for playing back) through decoding of the coded audio signal. The decoder receives the bitstream and reconverts it into an uncompressed signal. ISO-MPEG is a standard for high-quality, low bit-rate video and audio coding. The audio part of the standard is composed by algorithms for high-quality low-bit-rate audio coding, i.e. algorithms that reduce the original bit-rate, while guaranteeing high quality of the audio signal. The audio coding algorithms consists of MPEG-1 (with three different layers), MPEG-2, MPEG-2 AAC, and MPEG-4.  This work presents a study of the MPEG-4 AAC audio coding algorithm. Besides, it presents the implementation of the AAC algorithm on different platforms, and comparisons among implementations. The implementations are in C language, in Assembly of Intel Pentium, in C-language using DSP processor, and in HDL. Since each implementation has its own application niche, each one is valid as a final solution. Moreover, another purpose of this work is the comparison among these implementations, considering estimated costs, execution time, and advantages and disadvantages of each one.|http://hdl.handle.net/10183/1697
Correlação de alarmes e diagnóstico no gerenciamento de sistemas supervisionados por computador|2002|Open Access|Dissertação|Engenharia elétrica;Gerencia : Falhas;Correlação : Alarmes|por|Este trabalho investiga a aplicação de métodos e técnicas de correlação de alarmes na detecção e diagnóstico de falhas em sistemas supervisionados por computador. Atualmente, alguns centros de supervisão ainda não se utilizam destas técnicas para o tratamento das informações recebidas de suas redes de supervisão, ficando para os operadores a responsabilidade de identificar estas correlações. Com o crescente volume de informações recebidas pelos centros de supervisão, devido ao aumento da heterogeneidade e do número de equipamentos supervisionados, torna a identificação manual da correlação dos alarmes lenta e pouco precisa. Objetivando melhorar a qualidade do serviços prestados pelos centros de supervisões, este trabalho propõe o uso de uma rede Bayesiana como método de correlação de alarmes e uma técnica de limitação de escopo para atingir uma melhor performance na propagação desta correlação. Através dos conceitos desenvolvidos neste trabalho, foi implementado um protótipo de correlação de alarmes para um sistema de supervisão existente no mercado, neste protótipo modela-se a rede Bayesiana em um banco de dados relacional e, como resultado desta implementação apresenta-se a interface desenvolvida para a supervisão.|http://hdl.handle.net/10183/1702
Uma Proposta de apoio para decisões de grupo no ambiente PROSOFT|2002|Open Access|Dissertação|Engenharia : Software;Processo : Software;Sistemas : Informacao gerencial|por|O processo de desenvolvimento de software implica na necessidade constante de tomadas de decisão. A cada etapa do processo, torna-se necessário estabelecer a comunicação e interação entre usuários, gerentes, analistas, programadores e mantenedores numa constante troca de informações. O registro dos artefatos produzidos durante todo o processo é uma questão que norteia as pesquisas em ambiente de desenvolvimento de software. Quando se fala em suporte ao processo de colaboração entre os elementos de uma equipe de desenvolvimento, este registro torna-se ainda mais necessário. Neste contexto, a modelagem dos dados a serem armazenados se amplia para comportar outras informações provenientes da interação do grupo além dos artefatos gerados. As informações trocadas durante este processo interativo que incluem fatos, hipóteses, restrições, decisões e suas razões, o significado de conceitos e, os documentos formais formam o que é denominado pela literatura especializada como memória de grupo. A proposta da arquitetura SaDg PROSOFT visa fornecer suporte a memória de grupo, no que diz respeito ao registro das justificativas de projeto(Design Rationale), através de uma integração com o gerenciador de processos (GP) provido pelo ADS PROSOFT. Esta integração se dá através das ferramentas inseridas no modelo, assim desenhadas: Editor de Norma, Editor de Argumentação, Extrator de Alternativas, Editor de Votação. O ADS PROSOFT integra ferramentas para desenvolvimento de software.  Este ADS foi escolhido para o desenvolvimento do modelo SADG, pois baseia-se na construção formal de software, mas particularmente no método algébrico, por ser um ambiente estendível, possibilitando a inclusão do modelo SaDg PROSOFT ao seu conjunto de ferramentas, por ter características de um ambiente distribuído e cooperativo e por não dispor de nenhum suporte à discussões e decisões em grupos. São apresentados os fundamentos de modelos SADG e algumas ferramentas. Alguns dos principais requisitos desses ambientes foram coletados e são apresentados a fim de embasar a proposta do trabalho. O modelo SADG é apresentado na forma de ferramentas PROSOFT(chamadas ATOs) e permite a definição de atividades como: Atividade de argumentação, atividade de extração e a atividade de votação. Além disso, permite a coordenação destas atividades através de um facilitador e do próprio GP, e também, possui um mecanismo para a configuração do processo decisório.|http://hdl.handle.net/10183/1703
Estudo do comportamento cinético e mecânico-dinâmico de resinas epóxi a base de óleo de soja epoxidado|2001|Open Access|Dissertação|Oleo de soja : Epoxidacao;Resina epoxi : Dinamica quimica;Resina epoxi : Reacao de cura|por|Óleo de soja epoxidado foi curado com os anidridos: dodecenilsuccínico (DDS), maleico (MAL), ftálico (FTA), succínico e hexahidroftálico (CH). As reações foram iniciadas com aminas terciárias tais como trietilamina (TEA), N,N’-dimetilanilina (ARO) e 1,4- diazobiciclo[2,2,2]octano (DABCO). Calorimetria diferencial de varredura (DSC) foi aplicada com êxito no estudo da reação de cura e usada para a determinação dos parâmetros cinéticos e termodinâmicos. Foram estudados os efeitos eletrônicos, os fatores estéreos e a rigidez do segmento diéster formado devido na conversão de grupos anidridos. Foi observado que os anidridos maleico e ftálico reagem mais rápido do que os outros anidridos. A influência do iniciador e da razão molar (anidrido/grupo epóxido) foi igualmente considerada. As propriedades mecânicas e térmicas das resinas epóxi curadas usando TEA como iniciador foram investigadas por análise térmica dinâmico-mecânica (DMTA) e termogravimétrica (TGA). Todas as amostras apresentaram características de materiais termofixos. Os materiais termofixos obtidos a partir de anidridos com estruturas rígidas (FTA, MAL e CH) mostraram alta temperatura de transição vítrea e densidade de reticulação. Quando um excesso de grupos epóxido foi usado (R= 0,5), a temperatura de transição vítrea do material diminuiu.  As resinas epóxi exibiram estabilidade térmica até 300ºC. Um comportamento inesperado foi observado para aquelas resinas obtidas com o anidrido dodecenilsuccínico (DDS). A influência do grau de epoxidação do óleo de soja nas propriedades mecânicas e na temperatura de transição vítrea foi investigada. Quanto maior o conteúdo de grupos epóxido, maior é a Tg e a dureza do material. As resinas epóxi preparadas a partir de óleo de soja epoxidado mostram excelente resistência química em NaOH e ácido sulfúrico, mas resistência pobre quando em contato com solventes orgânicos (tolueno, acetona, gasolina e etanol).|http://hdl.handle.net/10183/1707
Dessimetrização e enriquecimento enantiomérico de derivados do triciclo[5.2.1.0 2,6] decano através de enzimas e de indutores quirais. Preparação dos respectivos pseudopeptídeos restritos|2001|Open Access|Dissertação|Policiclicos;Peptídeos : Síntese;Catálise enzimática|por|Uma série de derivados quirais (e.e. > 99%) foram sintetizados a partir do meso- exo-(3R,5S)-3,5-dihidróximetilenotriciclo[5,2.1.0²,⁶]decano com altos rendimentos, usando catálise enzimática (lipases) em reações de transesterificação. A resolução do respectivo diéster racêmico através da hidrólise catalisada com esterase (PLE) não forneceu o monoéster opticamente enriquecido; enquanto que a dessimetrização do anidrido usando indutores quirais (quinina e quinidina) resultou no monoéster opticamente enriquecido (e.e.≅ 60%). O respectivo amino-álcool protegido foi preparado. Alguns análogos inéditos de peptídeos restritos incorporados do triciclodecano foram sintetizados.;A serie of chiral derivativas (e.e.>99%) from meso-endo,exo-3,5- dihydroxymethylenetricyclo[5.2.1.0²,⁶]decane were synthesized with hight yields using enzyme (lipase) catalysis in transesterification reactions. The respective aminoalcohol with protected amino group was prepared. Resolution of the respective racemic diester through enzymatic hydrolyses with esterase catalysis (PLE) didn't give rise to an optically enriched monoester; while the desymmetrization of the anhydride using inductors (quinine and quinidine) furnished optically enriched monoester (e.e.≅ 60%). Some unpublished constrained peptides analogues incorporating the tricyclo decane residue were synthesized.|http://hdl.handle.net/10183/1708
Posicionamento de réplicas em sistemas distribuídos|2001|Open Access|Dissertação|Confiabilidade : Computadores;Tolerancia : Falhas;Sistemas distribuídos;Replicacao : Arquivos|por|Replicação de objetos é usada para garantir uma maior disponibilidade de recursos em um sistema distribuído. Porém, com a replicação, surgem problemas como o controle da consistência das réplicas e onde estas réplicas devem estar posicionadas. A consistência é garantida por um protocolo de consistência de réplicas. Para facilitar a implementação dos protocolos de controle de réplicas, pode-se utilizar mecanismos de comunicação de grupo como suporte para a replicação. Outro problema importante que surge com a replicação é o posicionamento das réplicas. A carga de processamento em um sistema distribuído muda continuamente e num determinado instante pode ser necessário mudar a distribuição atual das réplicas pela adição de novas réplicas, remoção de réplicas desnecessárias ou pela mudança de posicionamento das réplicas. Um sistema de gerenciamento de réplicas pode realizar esta tarefa. Este trabalho apresenta o sistema RPM – Replica Placement Manager – responsável por fornecer ao serviço de gerenciamento de réplicas uma lista ordenada de nodos potencialmente ideais, num determinado momento do processamento, para receber uma réplica de um objeto. Esta lista é criada pelo RPM, considerando um pequeno conjunto de variáveis estáticas e dinâmicas, facilmente obtidas nos nodos do sistema distribuído.|http://hdl.handle.net/10183/1729
Gerenciamento de documentação técnica para ambientes de engenharia/CAD com suporte a versões|2001|Open Access|Dissertação|Banco : Dados;Documentacao : Sistemas;Framework;Versoes : Banco : Dados|por|Ambientes de engenharia apresentam a forte característica da necessidade de cooperação entre projetistas na concepção de projetos CAD, o que provoca uma série de problemas em relação aos ambientes usuais encontrados em aplicações convencionais. Na busca de solucionar tais problemas, vários recursos e mecanismos relativos às gerências de dados e do processo de projeto são apresentados em vários estudos encontrados na literatura. Boa parte desses recursos estão embutidos nesse trabalho, que visa apresentar um sistema gerenciador de documentação técnica para ambientes de engenharia/CAD chamado GerDoc Ábacus. A proposta da construção do GerDoc Ábacus é baseada na busca da solução dos problemas relativos à consistência dos dados de projetos e da integração de tarefas de projetistas que interagem em ambientes distribuídos de projeto. Unindo vários mecanismos, é proposta uma interface totalmente interativa, objetivando manter a harmonia entre projetistas que fazem parte de equipes de projetos que são mantidos em atividade durante longos períodos de tempo, além de documentar todos os passos realizados acerca de cada um desses projetos. Dessa forma, o GerDoc Ábacus é uma ferramenta organizacional e administrativa para projetos de engenharia, sendo de fácil operacionalização, buscando altos níveis de integridade dos dados mantidos.|http://hdl.handle.net/10183/1730
Estudo dos compostos voláteis de algumas espécies de eucalipto através do uso de microextração em fase sólida no modo headspace (HS-SPME)|2002|Open Access|Tese|Eucalipto : Microextração em fase sólida;Compostos orgânicos voláteis;Cromatografia gasosa|por|Neste trabalho utilizou-se a técnica de HS-SPME/GC/ITMS para o estudo qualitativo dos compostos voláteis (VOC) emitidos pelas folhas de três espécies de Eucalyptus (E.): E. citriodora, E. dunnii e E. saligna tanto em laboratório como in situ, além do estudo da relação entre folhas de eucalipto e rãs Litoria ewingi. Para este fim, foram desenvolvidos métodos análiticos. Os compostos tentativamente como (E,E)-a-farseno, (E)-4,8-dimetil-1,3,7-nonatrieno (DMNT),(E,E)-4,8,12-trimetil-1,3,7,11-tridecatetraeno (TMTT),B-cariofileno,a-cariofileno, gerrmacreno D e (E,E,E)-3,7,11,15-tetrametil-1,3,6,10,14-hexadecapentaeno (TMHP) foram encontrados no headspace de folhas picadas de eucalipto, sendo que os três primeiros também foram nas emissões áreas de E. saligna in situ, não tendo sido encontrados nos óleos voláteis das mesmas árvores, obtidos por hidrodestilação. Nas amostragens in situ, foram observados dois tipos de perfis circadianos nas emissões voláteis, incluindo compostos como cis-, trans-óxido de rosa, trans-B-ocimeno, citronelal, citronelol, entre outros. O comportamento dos compostos citados sugere que os mesmos sejam semioquímicos.  Os resultados obtidos com HS-SPME (PDMS/DVB) mostraram que esta é uma ferramenta analítica relativamente simples, que preserva a vida devido ao seu caráter não invasivo, de mínima pertubação do sistema vivo sob amostragem e que, devido a sua rapidez (1 min), permite o acompanhamento de alteraçãoes rápidas provocadas por processos endógenos e/ou exógenos nos seres vivos. O monitoramento da emissão de voláteis de folhas in situ durante 8 a 10 h por vários dias consecutivos, e a extração dos voláteis de rãs vivas sob estresse, submetidas a diferentes condições de dieta e meio ambiente, exemplificam o potencial desta técnica, que abre novos horizontes na investigação de voláteis de seres vivos.|http://hdl.handle.net/10183/1738
Estudo da agregação de copolímeros anfifílicos de poliestireno e Poli[5-(N,N-Dialquilamino) Isopreno]|2002|Open Access|Tese|Copolímeros em bloco anfifílicos;Copolímeros assimétricos;Agregados micelares;Microscopia eletrônica|por|As propriedades em solução e a auto-associação de copolímeros em bloco de estireno e 5- (N,N-dialquilamino)isopreno foram estudados. Copolímeros di e tribloco são formados por um longo bloco de poliestireno, com um ou dois blocos menores de poli[5-(N,Ndialquilamino) isopreno] em uma ou ambas as extremidades. A polaridade do solvente é determinante na afinidade deste com um ou ambos blocos constituintes do polímero e, conseqüentemente, afetará as propriedades dinâmicas e estruturais do mesmo. Após a quaternização dos sistemas com dimetil sulfato, os agregados foram preparados pela prévia dissolução dos polímeros em um solvente orgânico e subsequente adição de água para induzir a agregação das cadeias insolúveis de PS. Solventes puros (DMF, THF e dioxano), bem como misturas de DMF e THF foram empregados como solvente comum. A concentração crítica de água (cwc) e as morfologias foram estudadas em função da natureza do solvente comum, da concentração inicial, arquitetura e massa molecular do copolímero, por espalhamento de luz estático e microscopia eletrônica de transmissão (MET), respectivamente.  Determinou-se que tanto a cwc quanto as morfologias, são predominantemente influenciadas pela natureza do solvente comum. Alguns resultados inesperados foram encontrados para os copolímeros tribloco, incluindo a descoberta de uma nova morfologia formada a partir desse tipo de copolímero- a morfologia bowl-shape. Os agregados bowl-shaped, são essencialmente esferas bastante polidispersas, contendo um vazio alocado assimetricamente em seu interior. A fase contínua é composta de um arranjo de micelas inversas (núcleo de PAI e coroa de PS), sendo que as cadeias hidrofílicas de PAI circundam toda a estrutura na interface polímero/água. Acredita-se que a formação desta morfologia está sob controle cinético e não representa uma estrutura de equilíbrio. Foi proposto um possível mecanismo para a formação deste novo tipo de agregado.|http://hdl.handle.net/10183/1739
Estudo de diferentes métodos de extração por Ultra-Som de pesticidas em latossolo utilizando técnicas cromatográficas|2002|Open Access|Dissertação|Pesticidas;Herbicida;Analise do solo;Cromatografia;Latossolo|por|Amostras de solo (latossolo vermelho destroférrico) coletadas em três pontos distintos, no município de Caibaté (Região das Missões, RS), na profundidade de aproximadamente 10 cm (da superfície do solo), em diferentes estações do ano e com diferentes tipos de manejo de solo (plantio convencional; plantio direto e cultivo mínimo), foram analisadas com o intuito de verificar a presença e a persistência de pesticidas no solo. Adicionalmente foi avaliada a relação entre a concentração dos produtos e tipo de manejo de solo aplicado. Este estudo é justificado por ser esta região de caráter tipicamente agrícola, com uso sistemático de pesticidas, tais como: monocrotofós, tiabendazole, triadimenol, lufenuron e imazetapir. Primeiramente foram estudadas as melhores condições de extração, considerando as técnicas mais comumente utilizadas, como o soxhlet, o banho e a sonda de ultra-som. As análises foram realizadas por Cromatografia Líquida de Alta Eficiência com Detector UV-visível (lufenuron e imazetapir) e Cromatografia Gasosa com Detector Seletivo de Massas (monocrotofós, tiabendazole e triadimenol), dependendo das características do analito em estudo. Os resultados mostram a presença de todos os pesticidas estudados, permitindo inferir que os mesmos persistem no solo. Além disto, os resultados indicam que não há uma uniformidade da extração dos analitos nas amostras dos solos, que foram extraídas com as técnicas de soxhlet, banho e sonda de ultra-som. Tais diferenças podem ser devido à heterogeneidade das amostras ou uma extração seletiva dos analitos. Quanto as influências do tipo de manejo das lavouras, não foi possível constatar a contribuição destas formas de plantio para a persistência de resíduos de pesticidas no solo.|http://hdl.handle.net/10183/1757
Modelo temporal de versões|2001|Open Access|Dissertação|Banco : Dados;Banco : Dados temporais;Versoes : Banco : Dados|por|O objetivo principal desse trabalho é apresentar uma alternativa para a união de um modelo de versões e dados temporais. O resultado, o Modelo Temporal de Versões – TVM (Temporal Versions Model), é capaz de armazenar as versões do objeto e, para cada versão, o histórico dos valores das propriedades e dos relacionamentos dinâmicos. Esse modelo difere de outros modelos de dados temporais por apresentar duas diferentes ordens de tempo, ramificado para o objeto e linear para cada versão. O usuário pode também especificar, durante a modelagem, classes normais sem tempo e versionamento, o que permite a integração deste modelo com outros modelos existentes. A utilização de um modelo de dados temporal semanticamente rico não requer necessariamente a existência de um SGBD próprio para este modelo. A tendência é implementar o modelo sobre banco de dados convencionais, através do mapeamento das informações temporais para atributos explícitos. Como objetivo complementar, é apresenta do um ambiente para o suporte do TVM e de todas suas características. Especificamente, são detalhados o mapeamento da hierarquia base do modelo para um banco de dados objeto-relacional e sua implementação em um banco de dados comercial. Desse ambiente, foi implementado um protótipo da ferramenta para o auxílio na especificação de classes da aplicação.|http://hdl.handle.net/10183/1762
Gerenciamento integrado de QoS em redes de computadores|2001|Open Access|Tese|Redes : Computadores;Gerencia : Redes : Computadores;Qualidade : Servico|por|O gerenciamento de redes de computadores é uma tarefa complexa de ser realizada porque a redes atualmente possuem muitos equipamentos, protocolos, serviços e usuários. Como regra geral, redes de computadores são heterogêneas e a introdução de facilidades para o fornecimento de qualidade de serviço (QoS) faz com que a gerência de redes se torne ainda mais complexa, e neste cenário as soluções tradicionais de gerenciamento podem falhar. O gerenciamento de redes com QoS vem sendo investigado por diversos grupos, e soluções comerciais para o problema já podem ser encontradas. Entretanto, a gerência de QoS possui aspectos diversos que são investigados separadamente. Do ponto de vista do administrador de redes, soluções pontuais são importantes, mas a integração entre os diversos aspectos de gerência de QoS também é uma necessidade. Tal necessidade, entretanto, não tem encontrado respaldo nas pesquisas desenvolvidas ou nos produtos de gerenciamento disponibilizados no mercado. Nesta situação, o administrador da rede é obrigado a trabalhar ao mesmo tempo com soluções disjuntas que não oferecem integração. Isso acaba por complicar a gerência de QoS, que por si só já é uma atividade complexa. QoS e discutida a necessidade de integração. As soluções existentes e pesquisas desenvolvidas são classificadas para que possam ser mais bem avaliadas em uma visão mais global.  A classificação realizada organiza os aspectos de QoS em tarefas de gerência de QoS que devem ser executadas por um administrador de redes que possuam facilidades de QoS. Para que a integração das tarefas de gerência de QoS seja possível, um modelo para gerência integrada de QoS em redes de computadores é proposto. O modelo, definido em camadas, utiliza elementos distribuídos na rede para executar tarefas específicas e um ambiente de gerência central fornece uma interface única de acesso ao sistema de gerência ao administrador da rede. O modelo proposto é analisado de acordo com alguns aspectos (por exemplo, em relação à sua escalabilidade, flexibilidade e exeqüibilidade). Uma implementação do mesmo é apresentada para a gerência de redes baseadas em IP que possuam mecanismos para fornecimento de QoS. A avaliação do modelo mostra, ao final, que a gerência de QoS, além de ser uma necessidade real, é possível de ser executada de forma integrada, como desejam os administradores de rede.|http://hdl.handle.net/10183/1763
Análise da formulação funcional da mecânica quântica não-relativística|1980|Open Access|Dissertação|Fisica de particulas elementares e campos;Teoria quântica;Mecânica relativística|por|Resumo não disponível.|http://hdl.handle.net/10183/1784
Análise e simulação de um sistema de equações químicas do tipo difusão-reação|2003|Open Access|Tese|Sistema de equações químicas;Tipo difusão-reação;Simulação numérica;Shadowing finito|por|Neste trabalho estudamos um sistema de equações diferenciais parabólicas que modelam um processo de difusão-reação em duas dimensões da mistura molecular e reação química irreverssível de um só passo entre duas espécies químicas A e B para formar um produto P. Apresentamos resultados analíticos e computacionais relacionados à existência e unicidade da solução, assim como estimativas do erro local e global utilizando elementos finitos. Para os resultados analíticos usamos a teoria de semigrupos e o principio do m´aximo, e a simulação numérica é feita usando diferenças finitas centrais e o esquema simplificado de Ruge-Kutta. As estimativas do erro local para o problema semi-discretizado são estabelecidas usando normas de Sobolev, e para estimar o erro global usamos shadowing finito a posteriori. Os resultados computacionais obtidos mostram que o comportamento da solução está dentro do esperado e concorda com resultados da referências. Assim mesmo as estimativas do erro local e global são obtidas para pequenos intervalos de tempo e assumindo suficiente regularidade sobre a velocidade do fluído no qual realiza-se o processo. Destacamos que a estimativa do erro global usando shadowing finito é obtida sob hipóteses a posteriori sobre o operador do problema e o forte controle da velocidade numa vizinhança suficientemente pequena.|http://hdl.handle.net/10183/1807
Análise por feixes de íons de filmes finos dielétricos depositados por sputtering reativo e crescidos termicamente|1993|Open Access|Tese|Física da matéria condensada;Filmes finos dieletricos;Sputtering;Filmes finos;Química : Materiais|por|Esta tese trata de filmes finos de materiais dielétricos depositados por sputtering reativo e crescidos termicamente que são analisados por métodos de feixes de íons. Como introdução aos nossos trabalhos, são abordados os princípios da deposição por sputtering com corrente contínua, com rádio-freqüências e em atmosferas reativas. Além disso, são apresentados os princípios de crescimento térmico em fornos clássicos e em fornos de tratamento térmico rápido. Também são descritos em detalhe os métodos análiticos de espectropia de retroespalhamento Rutheford e reações nucleares ressonantes e não-ressonantes para medir as quantidades totais e os perfis de concentração em função da profundidade dos vários isótopos utilizados, bem como o método de difração de raios-X. No que concerne à deposição por sputtering reativo de filmes finos de nitreto de silicio, nitreto de alumínio e nitreto duplo de titânio e alumínio, são estudados os processos de deposição, são caracterizados os filmes desses materiais e são estabelecidas correlações entre características dos filmes e os parâmetros de deposição. Quanto ao crescimento térmico em atmosferas reativas de filmes muito finos de óxido, nitreto e oxinitreto de sílicio, são apresentados modelos para o crescimento dos filmes de óxido de sílicio, é estudada a influência da limpeza do substrato de sílicio nos mecanismos de crescimento dos filmes de óxido de sílicio e são elucidados aspectos dos mecanismos de nitretação térmica do sílicio e de filmes de óxido de sílicio.|http://hdl.handle.net/10183/1832
Visualização de informações aplicada à gerência de software|2001|Open Access|Dissertação|Computação gráfica;Visualizacao : Informacao;Visualizacao : Software|por|O desenvolvimento e manutenção de software fazem parte de um processo intrinsecamente difícil e que consome tempo e custos, principalmente quando o sistema consiste de milhares de linhas de código. Por isso, sistemas de visualização de software objetivam prover mecanismos para construir representações visuais de informações sobre programas e sistemas, através das quais o programador pode analisar e compreender características de sua estrutura e funcionamento em um maior nível de abstração do que o código fonte. Assim, ferramentas visuais de software que suportam as tarefas de desenvolvimento, depuração, manutenção e reutilização tornam-se mais necessárias pelo fato de ajudarem a reduzir a complexidade inerente do processo de compreensão. Esse trabalho tem como objetivo principal o desenvolvimento de um visualizador que exiba as informações existentes nos programas de forma mais rápida e legível, evitando que o programador/analista tenha que percorrer as linhas de código. O texto inicialmente situa a área de visualização de informações, abordando a área de visualização de software, uma vez que a visualização de software é assim chamada por tratar da visualização de informações que são extraídas de programas.  Em seguida, é apresentado um estudo de caso baseado no desenvolvimento dos sistemas da empresa Benfare Informática, no qual caracteriza-se a necessidade de ferramentas que auxiliem a compreensão de programas, com o objetivo de otimizar as operações de manutenção e desenvolvimento de programas. O restante do trabalho trata do sistema VisProgress que foi um protótipo desenvolvido como ferramenta de apoio para a equipe de desenvolvimento da empresa e como forma de avaliar o uso de técnicas de visualização em tal situação. A ferramenta desenvolvida é um visualizador de informações que percorre programas escritos em Progress, ferramenta de desenvolvimento utilizada pela empresa, e extrai as informações encontradas nos programas. A visualização é dividida em três partes. A primeira permite a visualização de informações textuais extraídas diretamente do código fonte dos programas que compõem os sistemas, a segunda faz a visualização de um grafo que representa a chamada de programas, e a terceira e última faz a visualização também de um grafo, porém representando o compartilhamento de variáveis entre os programas. A obtenção dos grafos foi construída em Delphi, porém a visualização gráfica é feita através da ferramenta Dotty, ferramenta específica para visualização de grafos. Após a descrição do protótipo implementado, são apresentados os resultados obtidos com a avaliação da ferramenta feita nas empresas Benfare Informática e Dzset Soluções e Sistemas para Computação.|http://hdl.handle.net/10183/1838
Se In é um ideal finitamente gerado então I é um ideal finitamente gerado?|2001|Open Access|Dissertação|Anéis comutativos;Ideais;Anéis noetherianos|por|Suponhamos que M seja um ideal maximal de um domínio R e que alguma potência de M seja finitamente gerada. Vamos mostrar que M será finitamente gerado em cada um dos seguintes casos: i M tem altura um, ii R é inteiramente fechado e altura de M é 2, iii R K X,S é um domínio monóide sobre um corpo K, onde S S 0 é um monóide cancelativo e livre de torção, tal que i 1 iS e M é o ideal maximal gerado por Xs/s S . Estendemos os resultados anteriores aos ideais I de um anel reduzido R tal que RI é anel Noetheriano. Provamos que um anel reduzido R é Noetheriano se cada ideal primo de R possui uma potência que é finitamente gerada. Para cada d tal que 3 d , estabelecemos a existência de um domínio de integridade d-dimensional que possui um ideal maximal M não finitamente gerado, de altura d tal que M2 é 3-gerado.|http://hdl.handle.net/10183/1847
Abordagem baseada em conceitos para descoberta de conhecimento em textos|2001|Open Access|Tese|Armazenamento : Dados;Recuperacao : Informacao;Descoberta : Conhecimento;Mineracao : Dados|por|Esta tese apresenta uma abordagem baseada em conceitos para realizar descoberta de conhecimento em textos (KDT). A proposta é identificar características de alto nível em textos na forma de conceitos, para depois realizar a mineração de padrões sobre estes conceitos. Ao invés de aplicar técnicas de mineração sobre palavras ou dados estruturados extraídos de textos, a abordagem explora conceitos identificados nos textos. A idéia é analisar o conhecimento codificado em textos num nível acima das palavras, ou seja, não analisando somente os termos e expressões presentes nos textos, mas seu significado em relação aos fenômenos da realidade (pessoas, objetos, entidades, eventos e situações do mundo real). Conceitos identificam melhor o conteúdo dos textos e servem melhor que palavras para representar os fenômenos. Assim, os conceitos agem como recursos meta-lingüísticos para análise de textos e descoberta de conhecimento. Por exemplo, no caso de textos de psiquiatria, os conceitos permitiram investigar características importantes dos pacientes, tais como sintomas, sinais e comportamentos. Isto permite explorar o conhecimento disponível em textos num nível mais próximo da realidade, minimizando o problema do vocabulário e facilitando o processo de aquisição de conhecimento. O principal objetivo desta tese é demonstrar a adequação de uma abordagem baseada em conceitos para descobrir conhecimento em textos e confirmar a hipótese de que este tipo de abordagem tem vantagens sobre abordagens baseadas em palavras.  Para tanto, foram definidas estratégias para identificação dos conceitos nos textos e para mineração de padrões sobre estes conceitos. Diferentes métodos foram avaliados para estes dois processos. Ferramentas automatizadas foram empregadas para aplicar a abordagem proposta em estudos de casos. Diferentes experimentos foram realizados para demonstrar que a abordagem é viável e apresenta vantagens sobre os métodos baseados em palavras. Avaliações objetivas e subjetivas foram conduzidas para confirmar que o conhecimento descoberto era de qualidade. Também foi investigada a possibilidade de se realizar descobertas proativas, quando não se tem hipóteses iniciais. Os casos estudados apontam as várias aplicações práticas desta abordagem. Pode-se concluir que a principal aplicação da abordagem é permitir análises qualitativa e quantitativa de coleções textuais. Conceitos podem ser identificados nos textos e suas distribuições e relações podem ser analisadas para um melhor entendimento do conteúdo presente nos textos e, conseqüentemente, um melhor entendimento do conhecimento do domínio.|http://hdl.handle.net/10183/1849
Concepção de uma biblioteca de classes para sistemas de manufatura|2003|Open Access|Dissertação|Administração : Produção;Simulação;Orientacao : Objetos|por|Neste trabalho é proposta uma biblioteca de classes para sistemas de manufatura que visa facilitar a construção de modelos de simulação, permitindo o reuso e a agilizando a modelagaem. A principal característica deste trabalho é sua abordagem que difere da maioria dos trabalhos correlatos na área. Ela baseia-se na utilização de conceitos de produção bastante conhecidos atualemnte, como os roteiros e atividades de produção. Isto permite a criação de novas simulações muito mais rapidamente do que em outras metodologias, já que não são necessárias traduções compelxas entre a realidade e as aplicações simuladas. A biblioteca desenvolvida foi validade com a aplicação dos conceitos à modelagem de uma linha de produção de pontas de eixo para tratores, produzidos pela empresa Pigozzi S/A. ALém disso, o trabalho discute a possibilidade de integração entre a biblioteca de classes proposta e a ferramente de simulação de sistemas de manufatura Automod. A simulação do estudo de caso da linha de produção de pontas de eixo, modelada tanto no Automod quanto na biblioteca de classes proposta, permitiu uma comparação quantitativa, o que viabilizou a validação este trabalho.|http://hdl.handle.net/10183/1851
Orpheo : uma estrutura de trabalho para integração dos paradigmas de aprendizado supervisionado e não-supervisionado|2001|Open Access|Tese|Banco : Dados;Descoberta : Conhecimento;Mineracao : Dados;Bases : Dados|por|Esta tese apresenta contribuições ao processo de Descoberta de Conhecimento em Bases de Dados (DCBD). DCBD pode ser entendido como um conjunto de técnicas automatizadas – ou semi-automatizadas – otimizadas para extrair conhecimento a partir de grandes bases de dados. Assim, o já, de longa data, praticado processo de descoberta de conhecimento passa a contar com aprimoramentos que o tornam mais fácil de ser realizado. A partir dessa visão, bem conhecidos algoritmos de Estatística e de Aprendizado de Máquina passam a funcionar com desempenho aceitável sobre bases de dados cada vez maiores. Da mesma forma, tarefas como coleta, limpeza e transformação de dados e seleção de atributos, parâmetros e modelos recebem um suporte que facilita cada vez mais a sua execução. A contribuição principal desta tese consiste na aplicação dessa visão para a otimização da descoberta de conhecimento a partir de dados não-classificados. Adicionalmente, são apresentadas algumas contribuições sobre o Modelo Neural Combinatório (MNC), um sistema híbrido neurossimbólico para classificação que elegemos como foco de trabalho. Quanto à principal contribuição, percebeu-se que a descoberta de conhecimento a partir de dados não-classificados, em geral, é dividida em dois subprocessos: identificação de agrupamentos (aprendizado não-supervisionado) seguida de classificação (aprendizado supervisionado). Esses subprocessos correspondem às tarefas de rotulagem dos itens de dados e obtenção das correlações entre os atributos da entrada e os rótulos.  Não encontramos outra razão para que haja essa separação que as limitações inerentes aos algoritmos específicos. Uma dessas limitações, por exemplo, é a necessidade de iteração de muitos deles buscando a convergência para um determinado modelo. Isto obriga a que o algoritmo realize várias leituras da base de dados, o que, para Mineração de Dados, é proibitivo. A partir dos avanços em DCBD, particularmente com o desenvolvimento de algoritmos de aprendizado que realizam sua tarefa em apenas uma leitura dos dados, fica evidente a possibilidade de se reduzir o número de acessos na realização do processo completo. Nossa contribuição, nesse caso, se materializa na proposta de uma estrutura de trabalho para integração dos dois paradigmas e a implementação de um protótipo dessa estrutura utilizando-se os algoritmos de aprendizado ART1, para identificação de agrupamentos, e MNC, para a tarefa de classificação. É também apresentada uma aplicação no mapeamento de áreas homogêneas de plantio de trigo no Brasil, de 1975 a 1999. Com relação às contribuições sobre o MNC são apresentados: (a) uma variante do algoritmo de treinamento que permite uma redução significativa do tamanho do modelo após o aprendizado; (b) um estudo sobre a redução da complexidade do modelo com o uso de máquinas de comitê; (c) uma técnica, usando o método do envoltório, para poda controlada do modelo final e (d) uma abordagem para tratamento de inconsistências e perda de conhecimento que podem ocorrer na construção do modelo.|http://hdl.handle.net/10183/1858
Um Mecanismo de notificação e propagação de mudanças para um modelo de versões|2000|Open Access|Dissertação|Banco : Dados;Versoes : Banco : Dados;Orientacao : Objetos|por|Um dos requisitos naturais na modelagem de diversas aplicações na área de banco de dados é a utilização de um mecanismo para controle de versões. Esse mecanismo fornece suporte a um processo evolutivo. Tal suporte permite armazenar os diferentes estágios de uma entidade em tempos distintos, ou sob diferentes pontos de vista. Estudos recentes nessa área mostram a importância de incorporar ao modelo conceitual de banco de dados, um mecanismo para auxiliar no controle da evolução de versões. A evolução de versões apresenta problemas principalmente quando ocorre em uma hierarquia de composição. Por exemplo, se existem objetos compostos fazendo referência à objetos componentes que representam versões, então modificações nos componentes podem causar alterações nos objetos que os referenciam. Normalmente as ações relativas a essas modificações são a notificação ou a propagação de mudanças. Algumas propostas adicionam mecanismos de notificação e propagação ao modelo conceitual utilizado por aplicações não convencionais. Isso é importante porque mecanismos deste tipo auxiliam no controle da integridade de dados e na divulgação de informações sobre as mudanças realizadas no banco de dados.  O objetivo do trabalho aqui descrito é apresentar um mecanismo de notificação e propagação, que trata da evolução de dados, para um modelo de versões. É definido um modelo de classes com propriedades e operações que permitem manter e manipular subscrições de eventos referentes à evolução de objetos e versões e reagir diante da ocorrência destes eventos. Para atender os requisitos das diferentes aplicações, esta proposta especifica três estratégias. Cada uma delas apresenta diferentes funcionalidades: notificação ativa (enviar mensagens sobre mudanças ocorridas); notificação passiva (armazenar informações sobre mudanças ocorridas) e propagação (alterar o conteúdo do banco de dados automaticamente). Para validar o mecanismo proposto, uma implementação é apresentada para o sistema Oracle 8.|http://hdl.handle.net/10183/1859
Solução de equações intervalares|2001|Open Access|Tese|Analise : Intervalos;Teoria : Intervalos;Equacoes intervalares|por|Este trabalho trata do tipo de dado intervalar e da importância da especificação de uma semântica para garantir a correção e a interpretação coerente de resultados gerados, tais como de soluções de equações envolvendo este tipo de dado. Para tanto, realiza um estudo comparativo das semânticas de envoltória intervalar de reais e de número-intervalo, procurando identificar a influência de cada uma sobre definições fundamentais, tais como as das operações aritméticas e a do tipo de solução encontrado. Uma vez caracterizadas as semânticas associadas ao tipo de dado intervalar, o trabalho apresenta resultados que permitem mapear algebricamente a operação de multiplicação de números-intervalo tanto na representação de extremo inferior e extremo superior como na representação por ponto médio e diâmetro. Com base nesses resultados apresenta os mapeamentos das expressões algébricas que definem as potências positivas inteiras tanto para a semântica de número-intervalo como para a de envoltória de reais. Conjugando os resultados obtidos com a semântica de número-intervalo, o trabalho apresenta procedimentos algorítmicos para a determinação de dois tipos de soluções de equações intervalares: solução própria, a obtida diretamente a partir da relação de igualdade estrutural algébrica entre intervalos, e envoltória intervalar de soluções reais, normalmente referenciada como a solução intervalar usual. Exemplos são apresentados para a validação dos procedimentos, bem como para a discussão do significado de cada tipo de solução sob o enfoque semântico.|http://hdl.handle.net/10183/1875
Um Estudo de metodologias para criação de um depósito de dados|2001|Open Access|Dissertação|Banco : Dados;Armazem : Dados|por|Este estudo tem como objetivo analisar as diferentes metodologias existentes para criação de Depósito de Dados (DD) e determinar uma metodologia que melhor atenda às necessidades de uma empresa de telecomunicações, iniciando um projeto de DD e identificando as causas mais comuns de insucesso, a serem evitadas em projetos desta natureza. E para comprovar esta metodologia foi construído um Data Mart utilizando dados da Cia. Rio-grandense de Telecomunicações, com objetivo de análise de qualidade dos bilhetes utilizados nos indicadores de desempenho de centrais bilhetadoras. Apresenta, também, as arquiteturas possíveis de um Depósito de Dados/Data Mart, suas características e diferenças. Esta Dissertação de Mestrado é uma contribuição à pesquisa e à análise de metodologias empregadas na criação e manutenção de Depósitos de Dados e a determinação de uma metodologia que atenda às necessidades de uma empresa de telecomunicações.|http://hdl.handle.net/10183/1876
Um Sistema de alertas inteligentes para ambientes de ensino na internet|2001|Open Access|Dissertação|Informática : Educação;Ensino a distância;Avaliacao pedagogica;Internet|por|A difusão da educação baseada na Web está trazendo uma série de mudanças nesta área. Uma dessas mudanças está na forma de como se avaliar as atividades dos alunos remotos, não só através de tarefas tradicionais como testes, mas verificando, em tempo-real, as ações dos alunos e assim possibilitando ao professor um acompanhamento mais completo das atividades dos estudantes. De acordo com os recursos computacionais existentes, a utilização de um Sistema de Alertas é a opção que melhor se adequa a estas finalidades, pois com este tipo de sistema é possível acompanhar as atividades dos alunos em cursos a distância. O objetivo deste trabalho é apresentar um Sistema de Alertas Inteligentes para apoio ao ensino, que detecta problemas nas atividades dos alunos em cursos na Web e realiza ações corretivas adequadas. Este sistema está parcialmente integrado ao ambiente Tapejara do Instituto de Informática da UFRGS – Sistemas Inteligentes de Ensino na Web - que consiste em um sistema de construção e acompanhamento de cursos disponibilizados via Internet. A principal característica do Sistema de Alertas Inteligentes é a busca de situações críticas como, por exemplo: aluno apresenta baixo desempenho nos exercícios, a estratégia de ensino não corresponde ao perfil do estudante, aluno não está comparecendo às atividades do curso, etc. Com isto, este sistema pode auxiliar o professor (tutor virtual) a ter um acompanhamento mais preciso sobre as atividades realizadas pelo estudante e assim, adaptar as aulas às características do aluno, sem, com isto, acarretar numa sobrecarga de trabalho.|http://hdl.handle.net/10183/1877
Functional timing analysis of VLSI circuits containing complex gates;Análise de timing funcional de circuitos VLSI contendo portas complexas |2000|Open Access|Tese|Microeletrônica;Cad : Microeletronica;Portas logicas;Análise : Timing;Design verification of VLSI circuits;Timing analysis;Functional timing analysis (FTA);Path sensitization problem;Critical delay estimation;Complex gates;Automatic test pattern generation (ATPG);Satisfiability (SAT)|eng|The recent advances in CMOS technology have allowed for the fabrication of transistors with submicronic dimensions, making possible the integration of tens of millions devices in a single chip that can be used to build very complex electronic systems. Such increase in complexity of designs has originated a need for more efficient verification tools that could incorporate more appropriate physical and computational models. Timing verification targets at determining whether the timing constraints imposed to the design may be satisfied or not. It can be performed by using circuit simulation or by timing analysis. Although simulation tends to furnish the most accurate estimates, it presents the drawback of being stimuli dependent. Hence, in order to ensure that the critical situation is taken into account, one must exercise all possible input patterns. Obviously, this is not possible to accomplish due to the high complexity of current designs. To circumvent this problem, designers must rely on timing analysis. Timing analysis is an input-independent verification approach that models each combinational block of a circuit as a direct acyclic graph, which is used to estimate the critical delay. First timing analysis tools used only the circuit topology information to estimate circuit delay, thus being referred to as topological timing analyzers. However, such method may result in too pessimistic delay estimates, since the longest paths in the graph may not be able to propagate a transition, that is, may be false. Functional timing analysis, in turn, considers not only circuit topology, but also the temporal and functional relations between circuit elements.  Functional timing analysis tools may differ by three aspects: the set of sensitization conditions necessary to declare a path as sensitizable (i.e., the so-called path sensitization criterion), the number of paths simultaneously handled and the method used to determine whether sensitization conditions are satisfiable or not. Currently, the two most efficient approaches test the sensitizability of entire sets of paths at a time: one is based on automatic test pattern generation (ATPG) techniques and the other translates the timing analysis problem into a satisfiability (SAT) problem. Although timing analysis has been exhaustively studied in the last fifteen years, some specific topics have not received the required attention yet. One such topic is the applicability of functional timing analysis to circuits containing complex gates. This is the basic concern of this thesis. In addition, and as a necessary step to settle the scenario, a detailed and systematic study on functional timing analysis is also presented.;Os recentes avanços experimentados pela tecnologia CMOS tem permitido a fabricação de transistores em dimensões submicrônicas, possibilitando a integração de dezenas de milhões de dispositivos numa única pastilha de silício, os quais podem ser usados na implementação de sistemas eletrônicos muito complexos. Este grande aumento na complexidade dos projetos fez surgir uma demanda por ferramentas de verificação eficientes e sobretudo que incorporassem modelos físicos e computacionais mais adequados. A verificação de timing objetiva determinar se as restrições temporais impostas ao projeto podem ou não ser satisfeitas quando de sua fabricação. Ela pode ser levada a cabo por meio de simulação ou por análise de timing. Apesar da simulação oferecer estimativas mais precisas, ela apresenta a desvantagem de ser dependente de estímulos. Assim, para se assegurar que a situação crítica é considerada, é necessário simularem-se todas as possibilidades de padrões de entrada. Obviamente, isto não é factível para os projetos atuais, dada a alta complexidade que os mesmos apresentam. Para contornar este problema, os projetistas devem lançar mão da análise de timing. A análise de timing é uma abordagem independente de vetor de entrada que modela cada bloco combinacional do circuito como um grafo acíclico direto, o qual é utilizado para estimar o atraso do circuito. As primeiras ferramentas de análise de timing utilizavam apenas a topologia do circuito para estimar o atraso, sendo assim referenciadas como analisadores de timing topológicos. Entretanto, tal aproximação pode resultar em estimativas demasiadamente pessimistas, uma vez que os caminhos mais longos do grafo podem não ser capazes de propagar transições, i.e., podem ser falsos.  A análise de timing funcional, por sua vez, considera não apenas a topologia do circuito, mas também as relações temporais e funcionais entre seus elementos. As ferramentas de análise de timing funcional podem diferir por três aspectos: o conjunto de condições necessárias para se declarar um caminho como sensibilizável (i.e., o chamado critério de sensibilização), o número de caminhos simultaneamente tratados e o método usado para determinar se as condições de sensibilização são solúveis ou não. Atualmente, as duas classes de soluções mais eficientes testam simultaneamente a sensibilização de conjuntos inteiros de caminhos: uma baseia-se em técnicas de geração automática de padrões de teste (ATPG) enquanto que a outra transforma o problema de análise de timing em um problema de solvabilidade (SAT). Apesar da análise de timing ter sido exaustivamente estudada nos últimos quinze anos, alguns tópicos específicos não têm recebido a devida atenção. Um tal tópico é a aplicabilidade dos algoritmos de análise de timing funcional para circuitos contendo portas complexas. Este constitui o objeto básico desta tese de doutorado. Além deste objetivo, e como condição sine qua non para o desenvolvimento do trabalho, é apresentado um estudo sistemático e detalhado sobre análise de timing funcional.|http://hdl.handle.net/10183/1883
Arquiteturas multi-tarefas simultâneas : SEMPRE : arquitetura SMT com capacidade de execução e escalonamento de processos|2000|Open Access|Tese|Arquitetura de computadores;Arquiteturas super escalares;Avaliacao : Desempenho;Simulação;Arquiteturas SMT|por|O avanço tecnológico no projeto de microprocessadores, nos recentes anos, tem seguido duas tendências principais. A primeira tenta aumentar a freqüência do relógio dos mesmos usando componentes digitais e técnicas VLSI mais eficientes. A segunda tenta explorar paralelismo no nível de instrução através da reorganização dos seus componentes internos. Dentro desta segunda abordagem estão as arquiteturas multi-tarefas simultâneas, que são capazes de extrair o paralelismo existente entre e dentro de diferentes tarefas das aplicações, executando instruções de vários fluxos simultaneamente e maximizando assim a utilização do hardware. Apesar do alto custo da implementação em hardware, acredita-se no potencial destas arquiteturas para o futuro próximo, pois é previsto que em breve haverá a disponibilidade de bilhões de transistores para o desenvolvimento de circuitos integrados. Assim, a questão principal a ser encarada talvez seja: como prover instruções paralelas para uma arquitetura deste tipo? Sabe-se que a maioria das aplicações é seqüencial pois os problemas nem sempre possuem uma solução paralela e quando a solução existe os programadores nem sempre têm habilidade para ver a solução paralela. Pensando nestas questões a arquitetura SEMPRE foi projetada. Esta arquitetura executa múltiplos processos, ao invés de múltiplas tarefas, aproveitando assim o paralelismo existente entre diferentes aplicações. Este paralelismo é mais expressivo do que aquele que existe entre tarefas dentro de uma mesma aplicação devido a não existência de sincronismo ou comunicação entre elas.  Portanto, a arquitetura SEMPRE aproveita a grande quantidade de processos existentes nas estações de trabalho compartilhadas e servidores de rede. Além disso, esta arquitetura provê suporte de hardware para o escalonamento de processos e instruções especiais para o sistema operacional gerenciar processos com mínimo esforço. Assim, os tempos perdidos com o escalonamento de processos e as trocas de contextos são insignificantes nesta arquitetura, provendo ainda maior desempenho durante a execução das aplicações. Outra característica inovadora desta arquitetura é a existência de um mecanismo de prébusca de processos que, trabalhando em cooperação com o escalonamento de processos, permite reduzir faltas na cache de instruções. Também, devido a essa rápida troca de contexto, a arquitetura permite a definição de uma fatia de tempo (fatia de tempo) menor do que aquela praticada pelo sistema operacional, provendo maior dinâmica na execução das aplicações. A arquitetura SEMPRE foi analisada e avaliada usando modelagem analítica e simulação dirigida por execução de programas do SPEC95. A modelagem mostrou que o escalonamento por hardware reduz os efeitos colaterais causados pela presença de processos na cache de instruções e a simulação comprovou que as diferentes características desta arquitetura podem, juntas, prover ganho de desempenho razoável sobre outras arquiteturas multi-tarefas simultâneas equivalentes, com um pequeno acréscimo de hardware, melhor aproveitando as fatias de tempo atribuídas aos processos.|http://hdl.handle.net/10183/1885
AAERO : ambiente de aprendizado para o ensino de redes de computadores orientado a problemas|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Redes : Computadores|por|Atualmente, a sociedade tem experimentado uma grande transformação devido à crescente incorporação da tecnologia em seu cotidiano. Estas mudanças demonstram o grande avanço tecnológico experimentado nas últimas décadas, principalmente na área de Redes e Telecomunicações. Este contexto tem gerado uma crescente procura por profissionais desta área, com um perfil que privilegie, além do conhecimento técnico, outras habilidades consideradas importantes, como o pensamento crítico, o auto-aprendizado e a habilidade para trabalhar em equipe, habilidades estas que não são normalmente focadas nos cursos atuais. Estas habilidades são estimuladas nas abordagens centradas nos alunos, com destaque ao Problem-Based Learning (PBL), uma abordagem na qual o aluno é exposto a problemas, sem nenhum conhecimento prévio, e que, para resolvê-los, precisa pesquisar e analisar novas informações, visando sua aplicação na solução dos mesmos. Apesar da grande utilização do PBL em diversas instituições no mundo, existem poucas ferramentas de software que dão apoio para sua total aplicação. Por outro lado, mesmo sendo bem estruturado, o PBL não sugere indicações de como assimilar melhor novas experiências, de como buscar o conhecimento em experiências anteriores e como desenvolver problemas atuais e interessantes com características reais. Estas dificuldades podem ser minimizadas com a utilização do CBR (Case-Based Reasoning).  Entre as aplicações CBR desenvolvidas especificamente na área de Redes de Computadores, pode-se destacar o sistema DUMBO, um sistema CBR, desenvolvido na UFRGS, especificamente para o diagnóstico de problemas em Redes de Computadores. A integração com o DUMBO permite à abordagem PBL ser aplicada com maior eficiência, utilizando sua biblioteca de casos como ferramenta de pesquisa e para a sugestão de novos problemas a partir de casos reais e atuais. Com base nestas afirmações, este trabalho apresenta a proposta e o desenvolvimento de um protótipo de ambiente de aprendizado para o ensino de Redes de Computadores, utilizando a abordagem PBL em conjunto com a abordagem CBR através do sistema DUMBO.|http://hdl.handle.net/10183/1892
Blade : um editor de esquemáticos hierárquico voltado à colaboração|2002|Open Access|Dissertação|Microeletrônica;Cad : Microeletronica;Editor : Esquematicos|por|Este trabalho apresenta a proposta de um editor de diagramas hierárquico e colaborativo. Este editor tem por objetivo permitir a especificação colaborativa de circuitos através de representações gráficas. O Blade (Block And Diagram Editor), como foi chamado, permite especificações em nível lógico, usando esquemas lógicos simples, bem como esquemas hierárquicos. Ao final da montagem do circuito, a ferramenta gera uma descrição textual do sistema num formato netlist padrão. A fim de permitir especificações em diferentes níveis de abstração, o editor deve ser estendido a outras formas de diagramas, portanto seu modelo de dados deve ter flexibilidade a fim de facilitar futuras extensões. O Blade foi implementado em Java para ser inserido no Cave, um ambiente distribuído de apoio ao projeto de circuitos integrados, através do qual a ferramenta pode ser invocada e acessada remotamente. O Cave disponibiliza um serviço de colaboração que foi incorporado na ferramenta e através do qual o editor suporta o trabalho cooperativo, permitindo que os projetistas compartilhem dados de projeto, troquem mensagens de texto e, de forma colaborativa, construam uma representação gráfica do sistema.  Objetivando fundamentar a proposta da nova ferramenta, é apresentado um estudo sobre ferramentas gráficas para especificação de sistemas, mais especificamente sobre editores de esquemáticos. A partir dessa revisão, do estudo do ambiente Cave e da metodologia de colaboração a ser suportada, fez-se a especificação do editor, a partir da qual implementou-se o protótipo do Blade. Além do editor, este trabalho contribuiu para a construção de uma API, um conjunto de classes Java que será disponibilizado no Cave e poderá ser utilizado no desenvolvimento de novas ferramentas. Foram realizados estudos sobre técnicas de projeto orientado a objeto, incluindo arquiteturas de software reutilizáveis e padrões de projeto de software, que foram utilizados na modelagem e na implementação da ferramenta, a fim de garantir a flexibilidade do editor e a reusabilidade de suas classes. Este trabalho também contribui com um estudo de modelagem de primitivas de projeto de sistemas. No modelo orientado a objetos utilizado no editor, podem ser encontradas construções muito utilizadas em diferentes ferramentas de projeto de sistemas, tais como hierarquia de projeto e instanciação de componentes e que, portanto, podem ser reutilizadas para a modelagem de novas ferramentas.|http://hdl.handle.net/10183/1893
Uma Infra-estrutura para controle de versões e adaptação de páginas web|2003|Open Access|Dissertação|Armazenamento : Dados;Internet;Configuracoes : Banco : Dados;Hiperdocumento|por|Conforme os sites Web crescem em número de páginas, sua manutenção torna-se mais complicada. Assim, os administradores dos sites de métodos e ferramentas que tornem sua manutenção mais organizada e automatizada. Entretanto, a criação de tais mecanismos é dificultada pelo formato das páginas Web (HTML), que mistura o conteúdo e a formatação da página em um mesmo arquivo. Uma solução usual para esse problema é separar estes componentes da página em documentos XML (conteúdo) e folhas de estilo XSLT (formatação). Pode-se notar várias semelhanças entre páginas Web e programas de computador (software), pois ambos têm componentes de tipos diferentes que evoluem com o tempo. Assim, técnicas oriundas da área de Gerência de Configuração de Software, como controle de versões, podem ser adaptadas para auxiliar a manuutenção de sites. Além da melhoria na manutenção, outra necessidade cada vez mais comum aos sites é a adaptação automática das páginas. Por meio desta, páginas podem ser automaticamente adequadas (adaptadas) e determinado usuário, o que potencialmente atrai um maior número de visitantes ao site. Se forem mantidas versões de cada componente de página, pode-se combiná-las para gerar muitas páginas alternativas. Através da escolha cuidadosa das versões dos ocmponentes que compõem uma página, é possível obter páginas adaptadas automaticamente.  Na área de Gerência de Configuração de Software, o chamado proceesso de configuração é responsável por selecionar automaticamente versões de módulos para compor um programa completo. O presente trabalho propõe uma infra-estrutura para um servidor Web que realiza controle de versões e suporta a adaptação de páginas Web de forma transparente ao visitante. Para tanto, é projetado um modelo de versões de páginas que separa conteúdo e formatação em componentes distintos. É proposto um processo de configuração que é responsável pela geração de páginas dinâmicas, o que é suportado por informações presentes no modelo de versões. Os autores de páginas e o próprio servidor Web podem interferir nas escolhas doprocesso de configuração, fornecendo critérios de seleção de versões. Esses critérios guiam as escolhas do processo de configuração, pois representam características que as versões escolhidas devem (necessariamente ou preferencialmente) apresentar.|http://hdl.handle.net/10183/1895
Vibrações naturais em um sistema de interação viga-água incluindo o efeito de onda de superfície|2003|Open Access|Dissertação|Vibracoes;Sistema de interação;Viga-água;Onda de superfície|por|Este trabalho tem como objetivo o estudo modal de um sistema estruturafluído, modelado pela equação de Euler-Bernoulli, para uma viga elástica, sujeita a pressão da água, e por ondas de superfície livre. O sistema acoplado possui condições para o domínio sólido (viga fixa-livre), para o domínio fluído (impermeabilidade e rigidez inferior), com ondas de superfície, e de interfacec fluido-estrutura (condições de continunidade na deflexão, ângulo de rotação, força de corte interno e momento de curvatura). Para deteerminar as vibrações livres e deslocamento no seco e no molhado, utiliza-se o método espectral para eliminar a dependência oscilatória temporal e concentrar-se na determinação dos modos através do estudo de problemas de contornos espaciais. Os modos podem ser calculados com o uso da base clássica de Euler ou da base dinâmica gerada pela resposta impulso. Foram feitas simulações para um material específico, e apresentados os resultados obtidos.|http://hdl.handle.net/10183/1901
Vibrações naturais em um sistema de interação viga-água sem o efeito de onda de superfície livre|2003|Open Access|Dissertação|Vibracoes;Sistema de interação;Viga-água;Onda de superfície livre|por|Neste trabalho é apresentado um estudo sobre o comportamento dinâmico de sistemas flexíveis com interação viga-água. Especificamente, é escolhida uma barragem freqüentemente encontrada em problemas de engenharia de irrigação e represas. O modelo matemático utilizado para descrever tal fenômeno tem como variáveis principais a pressão hidrodinâmica e os deslocamentos horizontais da barragem. Uma vez formuladas as equações governantes de tal sistema, que resultam em uma equação de onda para a pressão e uma equação de Euler-Bernoulli para os deslocamentos, é utilizada a técnica de separação de variáveis para proceder à sua solução. Por simplicidade, assumiu-se que no modelo não havia o efeito de onda da superfície livre. Primeiro, foi resolvida a equação para a pressão, sendo calculadas as suas componentes temporais e espaciais. Depois, foi resolvida a equação de Euler-Bernoulli junto com a condição de interface. Foram simulações para o material específico, e apresentados os resultados conseguidos. No final do trabalho são enunciadas as conclusões pertinentes.|http://hdl.handle.net/10183/1902
Recuperação com base em Checkpointing : uma abordagem orientada a objetos|2002|Open Access|Dissertação|Tolerancia : Falhas;Orientacao : Objetos;Recuperacao : Processos|por|Independentemente do modelo de programação adotado, no projeto e implementação de aplicações de alta disponibilidade, faz-se necessário usar procedimentos de tolerância a falhas. Dentre as atividades que trazem consigo interesse de pesquisa na área de Tolerância a Falhas, estão os mecanismos de recuperação em um sistema computacional. Do ponto de vista prático, estes mecanismos buscam manter próximo do mínimo o tempo total de execução de aplicações computacionais de longa duração, ao mesmo tempo em que as preparam para não sofrerem perdas significativas de desempenho, em caso de falhas. Paralelamente à evolução dos sistemas computacionais, foi possível observar também a evolução das linguagens de programação, principalmente as que utilizam o paradigma orientado a objetos. O advento da área de tolerância a falhas na orientação a objetos resultou em novos problemas na atividade de recuperação quanto aos mecanismos de salvamento de estados e retomada da execução, principalmente no que se refere às dificuldades de gerenciamento e controle sobre a alocação de objetos. Entretanto, observa-se que a complexidade de implementação dos mecanismos de recuperação, por parte dos programadores, exige deles conhecimentos mais especializados para o salvamento dos estados da aplicação e para a retomada da execução. Portanto, a simplificação do trabalho do programador, através do uso de uma biblioteca de checkpointing que implemente os mecanismos de salvamento de estados e recuperação é o ponto focal deste trabalho.  Diante do contexto exposto, nesta dissertação, são definidas e implementadas as classes de uma biblioteca que provê mecanismos de checkpointing e recuperação. Esta biblioteca, denominada de Libcjp, visa aprimorar o processo de recuperação de aplicações orientadas a objetos escritas na linguagem de programação Java. Esta linguagem foi escolhida para implementação devido à presença dos recursos de persistência e serialização. Para a concepção do trabalho, são considerados ambos os cenários no paradigma orientado a objetos: objetos centralizados e distribuídos. São utilizados os recursos da API de serialização Java e a tecnologia Java RMI para objetos distribuídos. Conclui-se o trabalho com a ilustração de casos de uso através de diversos exemplos desenvolvidos a partir de seus algoritmos originais inicialmente, e incrementados posteriormente com os mecanismos de checkpointing e recuperação. Os componentes desenvolvidos foram testados quanto ao cumprimento dos seus requisitos funcionais. Adicionalmente, foi realizada uma análise preliminar sobre a influência das ações de checkpointing nas características de desempenho das aplicações.|http://hdl.handle.net/10183/1929
Reação-difusão com difusividade variável para oxidação de silício|2001|Open Access|Dissertação|Sistemas de reação por difusão;Silício;Oxidação;Difusão;Cinética;Filmes finos|por|Neste trabalho, propomos uma modificação do modelo de reação-difusão (R. M. C. de Almeida et al., Physics Review B, 61, 19 (2000)) incluindo difusividade variável com o objetivo principal de predizer, ou no mínimo descrever melhor, o crescimento de oxido de Si no regime de filmes nos. Estudamos o modelo reação-difusão a coeficiente de difusão, D, fixo e D variável. Estudamos extensivamente o modelo reação-difusão com D fixo caracterizando seu comportamento geral, e resolvendo numericamente o modelo com D variável para um intervalo amplo de relações DSiO2=DSi. Ambos casos apresentam comportamento assintótico parabólico das cinéticas. Obtivemos as equações analíticas que regem o regime assintótico de tais casos. Ambos os modelos apresentam interface não abrupta. Comparações das cinéticas com o modelo linear-parabólico e com dados experimentais foram feitas, e também para as espessuras da interface. Contudo nenhum dos dois modelos de reação-difusãao, com D fixo e com D varáavel, podem explicar a região de filmes nos, que possui taxa de crescimento superior a taxa de crescimento da região assintótica. Porém, ao incluir taxa de reação variável dentro do modelo de reação-difusão com D variável, este aponta para uma solução do regime de filmes nos.|http://hdl.handle.net/10183/1939
Argilominerais e ostracodes da Formação Alagamar (cretáceo inferior), Bacia Potiguar, NE - Brasil : paleoambiente e indicadores térmicos|2002|Open Access|Dissertação|Ostracodes : Potiguar, Bacia;Formação Alagamar;Argilominerais;Geotermometria|por|A Fonnação Alagamar corresponde ao estágio transicional na evolução tectono-sedimentar da bacia Potiguar, representada inicialmente por um ambiente tlúvio-deltaico passando a lagunar restrito com indícios de intluência marinha. Os resultados obtidos nas análises por DRX, a partir das frações> 2 11me < 211m, demonstram variações na composição mineralógica ao longo das perfurações analisadas. A principal característica é a forte contribuição de minerais detríticos na fração >2 I-lme a predominância de argilominerais na fração <2 I-lmrepresentados por esmectita, ilita, caulinita, clorita, sepiolita e interestratificados de ilitalesmectita (IIE) e cloritalesmectita (CIE). As carapaças de ostracodes identificadas correspondem a espécies nãomarinhas de três famílias mais comuns no Cretáceo: Cyprididae, Limnocytheridae e Darwinulidae. A avaliação estatística da composição ontogenética das espécies fósseis, tomou-se útil neste trabalho para estimar os níveis de energia do paleoambiente deposicional. As associações de argilominerais e ostracodes, caracterizadas ao longo dos cilindros de sondagens estudados na Fonnação Alagamar, sugerem a variação dos ambientes deposicionais a partir de um ambiente lacustre e de clima árido. Ainda a passagem por uma fase transgressiva com o aumento da salinidade, condições de fundo redutoras e margens subaquosas, ocasionam exposições subaéreas intennitentes, culminandono topo com um período mais úmido e condições de águas salinas  O rico conteúdo em esmectita assemelha os depósitos da Fonnação Alagamar aos depósitos neoaptianos, os quais estão associados a condições climáticas com tendência à aridez. Sugere, ainda, morfologia pouco acidentada e drenagem reduzida ao redor de um paleolago restrito, o que favorece o desenvolvimento de solos do tipo vertissolo, fonte principal da esmectita da área. Os teores baixos de ilita e caulinita são, em grande parte, do resultado de erosão reduzida, atribuído ao relevo pouco acidentado da área. A associação dos argilominerais ilita, esmectita e interestratificados não ordenados indica condições rasas de soterramento e baixas temperaturas de diagênese nas amostras da base da perfuração RN6, alto de Macau. Ainda, a coloração apresentada pelas carapaças de ostracodes, entre o amarelo-laranja muito claro (2,5 Y 8/4) e amarelo amarronado(10 YR 6/6), sugerem níveis imaturos quanto à maturação da matéria orgânica. Na perfuração RN9, próxima a falha de Ubarana, condições de maturação da matéria orgânica são indicadas pelas cores cinza muito preto (5 Y 3/1) a preto (5 Y 2,5/1) apresentadas pelas carapaças de Candona sp.1, originalmente de cor branca (5 YR S/1 aiO YR 8/1). As intensidades e a fonna dos picos da ilita e da caulinita, a ausência de interestratificados e de argilominerais expansivos indicam condições de diagênese mais intensas que na perfuraçãoRN6. Associações de argilominerais identificadas em análises de difração de raios-X e mudanças na coloração das carapaças de ostracodes sugerem a utilização destes constituintes como indicadores ténnicos (geotennômetros) da maturação da matéria orgânica presente em rochas geradoras de hidrocarbonetos. Palavras chaves: Fonnação Alagamar, argilominerais, ostracodes, geotermômetros, bacia Potiguar.|http://hdl.handle.net/10183/1943
Análise de campos profundos da LMC imageados com o HST|2001|Open Access|Dissertação|Fotometria astronômica;Hubble space telescope;Nuvens de magalhaes;Populacoes estelares;Luminosidade;Idade das estrelas;Metalicidade;Densidade|por|Apresentamos fotometria profunda (V ~ 25,5) nas bandas V e I obtidas com a Wide Field and Planetary Camera 2 a bordo do telesc opio espacial Hubble para 7 campos distantes ~5º do centro da Grande Nuvem de Magalhães. Ajustamos isócronas aos diagramas cor-magnitude a fim de identficar diferentes populaões estelares nestes campos. Uma população velha (τ > 10¹º anos) foi encontrada em todos os campos. Alguns eventos de elevada formação estelar, com idades entre 2 x 109 e 4 x 109 anos, foram também encontrados em alguns campos localizados na região N/NO. Funções de luminosidade de estrelas de baixa massa (m ≤ 1; 1msol) foram obtidas para todos os campos. Aparentemente não há diferenças na mistura de populações entre os campos como sugerido através do teste Kolmogorov-Smirnov aplicados as funções de luminosidade. Finalmente, derivamos perfis de densidade para estrelas velhas e de idade intermediária. O primeiro apresenta uma inclinação levemente maior quando comparado com o último.|http://hdl.handle.net/10183/1946
Uma Avaliação de abordagens alternativas para armazenar RDF em banco de dados relacional|2001|Open Access|Dissertação|Banco : Dados;Banco : Dados relacionais;Metadados|por|O Resource Description Framework (RDF) é uma infra-estrutura, que possibilita a codificação, troca e reuso de metadata estruturado. Metadata é dados sobre dados. O termo refere a qualquer dado usado para ajudar a identificação, descrição e localização de recursos eletrônicos na rede. O RDF permite adicionar declarações, sinônimos e palavras que não estão presentes nos recursos, mas que são pertinentes a eles. Uma declaração RDF pode ser desenhada usando diagramas de arcos e nodos, onde os nodos representam os recursos e os arcos representam as propriedades nomeadas. O modelo básico consiste em recursos, propriedades e objetos. Todas as coisas sendo descritas pelas declarações RDF são chamadas de recursos. Um recurso pode ser uma página da Web inteira ou um elemento específico HTML ou XML dentro de um documento fonte. Uma propriedade é um aspecto específico, característica, atributo, ou relação usada para descrever um recurso. O objeto pode ser um outro recurso ou um literal. Estas três partes, juntas, formam uma declaração RDF. O resultado do parser para recursos com metadata RDF, é um conjunto de declarações referentes aquele recurso. A declaração destas propriedades e a semântica correspondente delas estão definidas no contexto do RDF como um RDF schema. Um esquema não só define as propriedades do recurso (por exemplo, título, autor, assunto, tamanho, cor, etc.), mas também pode definir os tipos de recursos sendo descritos (livros, páginas Web, pessoas, companhias, etc.).  O RDF schema, provê um sistema básico de tipos necessários para descrever tais elementos e definir as classes de recursos. Assim, os recursos constituindo este sistema de tipos se tornam parte do modelo RDF de qualquer descrição que os usa. A geração de modelos RDF pode ser conseguida através de algoritmos implementados com linguagens de programação tradicionais e podem ser embutidos em páginas HTML, documentos XML e até mesmo em imagens. Com relação a modelos em imagens, servidores Web específicos são usados para simular dois recursos sobre o mesmo URI, servindo ora a imagem ora a descrição RDF. Uma alternativa para armazenar e manipular grande quantidade de declarações RDF é usar a tecnologia de banco de dados relacional. Abordagens para armazenar declarações RDF em banco de dados relacional foram propostas, mas todas elas mantêm modelos diversos de diferentes fontes. Critérios de avaliação como tempo de carga, proliferação de tabelas, espaço, dados mantidos e custo de instruções SQL foram definidos. Duas abordagens apresentaram resultados satisfatórios. Com uma nova abordagem proposta por este trabalho se obteve melhores resultados principalmente no aspecto de consultas. A nova proposta provê mecanismos para que o usuário faça seu próprio modelo relacional e crie suas consultas. O conhecimento necessário pelo usuário se limita em parte aos modelos mantidos e ao esquema RDF.|http://hdl.handle.net/10183/2016
Proposta de modelo de agente EDI para uso como ferramenta de apoio a sistemas de informação baseado em web|2002|Open Access|Dissertação|Sistemas : Informação;Sistemas multiagentes;Edi|por|Este trabalho apresenta a proposta de uma arquitetura e o modelo de um Agente de Intercâmbio Eletrônico de Dados, Agente EDI, cuja função é, permitir a troca de dados estruturados entre Sistemas de Informações Distribuídos através da Internet. A estratégia de interação dos agentes possibilita uma maneira alternativa de tratar a recuperação, o armazenamento e a distribuição de dados, permitindo assim, o desenvolvimento de um modelo de Sistema de Informações baseado em Web, igualmente proposto neste trabalho. É apresentado também o desenvolvimento do Agente EDI proposto. O qual poderá ser utilizado por entidades que necessitam disponibilizar ou recuperar dados estruturados via Web, como por exemplo: informações de produtos, listas de preços, dados cadastrais, etc. A relevância deste trabalho está no fato de apresentar uma tecnologia simples e acessível, capaz de ser implementada sem a necessidade de altos investimentos e capaz de facilitar a implementação de Sistemas Distribuídos via Internet.|http://hdl.handle.net/10183/2017
Solução da equação de transporte multidimensional em geometria cartesiana e meio infinito usando derivada fracionária|2003|Open Access|Dissertação|Equação do transporte;Derivada fracionária|por|Neste trabalho, foi construída uma forma integral para a solução das equações de transporte em uma, duas e três dimensões, considerando o núcleo de espalhamento de Klein-Nishina, espalhamento isotrópico e o núcleo de espalhamento de Rutherford, respectivamente, seguindo a mesma idéia proposta em trabalhos recentes, nos quais foi construída uma solução para a equação de transporte de nêutrons em geometria cartesiana, usando derivada fracionária. A metodologia consiste em igualar a derivada fracionária do fluxo angular à equação integral, determinar a ordem da derivada fracionária comparando o núcleo da equação integral com o da definição de Riemann-Liouville. Essa formulação foi aplicada ao cálculo de dose absorvida. São apresentadas soluções geradas a partir do emprego do método da derivada fracionária e comparadas a resultados disponíveis na literatura.|http://hdl.handle.net/10183/2035
Um algoritmo numérico de integração de tempo para estruturas dinâmicas com dissipações : o método generalizado-α|2003|Open Access|Dissertação|Integração numérica;Matlab;Vibracoes|por|O presente trabalho tem por objetivo estudar e aplicar um método de integração numérica de tempo para estrutras dinâmicas com dissipação de energia. Nessa dissertação tal método é analisado e posteriormente implementado em MATLAB, afim de resolver algumas aplicações em sistemas dinâmicos dotados de massas, molas e amortecedores que são apresentados no primeiro capítulo. Usando o método implementado em MATLAB, também é apresentada uma aplicação para vibrações transversais em cordas axialmente.|http://hdl.handle.net/10183/2038
Avaliação do desempenho de classificadores neurais para aplicações em sensoriamento remoto|1998|Open Access|Dissertação|Sensoriamento remoto;Redes neurais;Classificacao : Imagem;Engenharia florestal;Geoinformática|por|Atualmente, pesquisadores das mais diversas áreas, tais como: Geologia, Física, Cartografia, Oceanografia, entre outras, utilizam imagens de satélite como uma fonte valiosa para a extração de informações sobre a superfície terrestre. Muitas vezes, a análise (classificação) destas imagens é realizada por métodos tradicionais sejam eles supervisionados (como o Método de Máxima Verossimilhança Gaussiana) ou nãosupervisionados (como o Método de Seleção pelo Pico do Histograma). Entretanto, pode-se utilizar as Redes Neurais Artificiais como uma alternativa para o aumento da acurácia em classificações digitais. Neste trabalho, utilizou-se imagens multi-espectrais do satélite LANDSAT 5-TM para a identificação de espécies vegetais (Mata Nativa, Eucalyptus e Acácia) em uma região próxima aos municípios de General Câmara, Santo Amaro e Taquari, no Estado do Rio Grande do Sul, Brasil. Comparou-se qualitativamente e quantitativamente os resultados obtidos pelo método de Máxima Verossimilhança Gaussiana e por uma Rede Neural Artificial Multinível com BackPropagation na classificação da área de estudo. Para tanto, parte desta área foi mapeada através de uma verificação de campo e com o auxílio de classificadores nãosupervisionados (Kohonen, que é uma Rede Neural, e o método de Seleção pelo Pico do Histograma).  Com isto, foi possível coletar dois conjuntos de amostras, sendo que um deles foi utilizado para o treinamento dos métodos e o outro (conjunto de reconhecimento) serviu para a avaliação das classificações obtidas. Após o treinamento, parte da área de estudo foi classificada por ambos os métodos. Em seguida, os resultados obtidos foram avaliados através do uso de Tabelas de Contingência, considerando um nível de significância de 5%. Por fim, na maior parte dos testes realizados, a Rede Neural Artificial Multinível com BackPropagation apresentou valores de acurácia superiores ao Método de Máxima Verossimilhança Gaussiana. Assim, com este trabalho observou-se que não há diferença significativa de classificação para as espécies vegetais, ao nível de 5%, para a área de estudo considerada, na época de aquisição da imagem, para o conjunto de reconhecimento.|http://hdl.handle.net/10183/2062
Estudo experimental e teórico das propriedades magnéticas e supercondutoras dos compostos borocarbetos da série Y(Ni 1-x Mn x) 2 B2 C com x = 0,0, 0,01, 0,025, 0,05, 0,10 e 0,15|2002|Open Access|Tese|Materiais supercondutores;Estrutura eletrônica;Momentos magnéticos;Impurezas;Manganês;Tratamento térmico;Temperatura crítica supercondutora;Supercondutores de acoplamento forte;Propriedades magnéticas;Processos de transporte;Sincrotrons;Transformações de fase|por|Apresentamos neste trabalho os resultados de um estudo experimental e teórico dos compostos borocarbetos supercondutores da série Y(Ni1-xMnx)2B2C com x = 0; 0,01; 0,025; 0,05; 0,10; 0,15. A principal motivação para este trabalho foi investigar a estrutura eletrônica e a possível formação do momento magnético sobre os átomos de impureza de Mn nos compostos Y(Ni1- xMnx)2B2C. O aparecimento do momento magnético localizado no sítio da impureza possibilitou estudar a influência do Mn sobre o mecanismo de quebra de pares supercondutores e sobre as propriedades magnéticas do composto. Os borocarbetos são compostos de estrutura cristalina tetragonal de corpo centrado e altamente anisotrópicos (c/a~3). São intermetálicos de alta temperatura crítica supercondutora Tc, com forte acoplamento elétron-fonon. Em alguns casos podem apresentar ordem magnética, supercondutividade e também coexistência ou competição energética entre ambos. As medidas de transporte eletrônico, em função da temperatura, foram feitas utilizando-se um detector síncroton baseado na técnica de quatro pontos operando na faixa de 4,2K até 300K. Essas medidas possibilitaram o estudo das propriedades relacionadas ao transporte eletrônico na fase supercondutora. Na fase normal, extraiu-se a dependência em energia da função espectral de fonons α² F (ω) para alguns compostos da série estudada.  As medidas magnéticas em função da temperatura e do campo magnético foram feitas utilizando-se um SQUID (Superconducting Quantun Interference Device – Quantun Design Model MPMS XL). Tais medidas permitiram a caracterização das propriedades magnéticas de nossas amostras. Em particular determinou-se o valor, em regime de saturação, do momento magnético associado ao sítio cristalino do Mn. Foram determinadas também as correntes críticas supercondutoras usando o Modelo de estado crítico de Bean e a variação da temperatura crítica supercondutora (Tc) com a mudança do campo externo aplicado. As medidas magnéticas permitiram a obtenção do diagrama que relaciona o campo crítico inferior (HC1) e a temperatura, variando-se a concentração do átomo dopante de manganês. Foi feito um esforço teórico no sentido de interpretar os resultados experimentais. Para isso foram usados três modelos: O modelo de estado crítico de Bean já citado acima e um modelo baseado na fórmula de Ziman usando uma aproximação para a função espectral de fonons para descrever a resistividade no regime de alta temperatura. Além disto, usou-se o modelo de duas sub-redes para a descrição do momento magnético das impurezas de Mn, em função da concentração, na série Y(Ni ) ( 2 ω α F 1-xMnx)2B2C.|http://hdl.handle.net/10183/2064
Definição de classes para comunicação Unicast e Multicast|2001|Open Access|Dissertação|Confiabilidade : Computadores;Tolerancia : Falhas;Comunicacao : Grupos;Protocolos : Difusao confiavel;Orientacao : Objetos;Java (Linguagem de programação);Framework|por|No projeto de arquiteturas computacionais, a partir da evolução do modelo cliente-servidor, surgiram os sistemas distribuídos com a finalidade de oferecer características tais como: disponibilidade, distribuição, compartilhamento de recursos e tolerância a falhas. Estas características, entretanto, não são obtidas de forma simples. As aplicações distribuídas e as aplicações centralizadas possuem requisitos funcionais distintos; aplicações distribuídas são mais difíceis quanto ao projeto e implementação. A complexidade de implementação é decorrente principalmente da dificuldade de tratamento e de gerência dos mecanismos de comunicação, exigindo equipe de programadores experientes. Assim, tem sido realizada muita pesquisa para obter mecanismos que facilitem a programação de aplicações distribuídas. Observa-se que, em aplicações distribuídas reais, mecanismos de tolerância a falhas constituem-se em uma necessidade. Neste contexto, a comunicação confiável constitui-se em um dos blocos básicos de construção. Paralelamente à evolução tanto dos sistemas distribuídos como da área de tolerância a falhas, foi possível observar também a evolução das linguagens de programação. O sucesso do paradigma de orientação a objetos deve-se, provavelmente, à habilidade em modelar o domínio da aplicação ao invés da arquitetura da máquina em questão (enfoque imperativo) ou mapear conceitos matemáticos (conforme o enfoque funcional).  Pesquisadores demonstraram que a orientação a objetos apresenta-se como um modelo atraente ao desenvolvimento de aplicações distribuídas modulares e tolerantes a falhas. Diante do contexto exposto, duas constatações estimularam basicamente a definição desta dissertação: a necessidade latente de mecanismos que facilitem a programação de aplicações distribuídas tolerantes a falhas; e o fato de que a orientação a objetos tem-se mostrado um modelo promissor ao desenvolvimento deste tipo de aplicação. Desta forma, nesta dissertação definem-se classes para a comunicação do tipo unicast e multicast, nas modalidades de envio confiável e não-confiável. Além destes serviços de comunicação básicos, foram desenvolvidas classes que permitem referenciar os participantes da comunicação através de nomes. As classes estão organizadas na forma de um pacote, compondo um framework. Sua implementação foi desenvolvida usando Java. Embora não tivessem sido requisitos básicos, as opções de projeto visaram assegurar resultados aceitáveis de desempenho e possibilidade de reuso das classes. Foram implementados pequenos trechos de código utilizando e testando a funcionalidade de cada uma das classes de comunicação propostas.|http://hdl.handle.net/10183/2073
Access Miner : uma proposta para a extração de regras de associação aplicada à mineração do uso da web|2000|Open Access|Dissertação|Banco : Dados;Mineracao : Dados;Regras : Associacao;Descoberta : Conhecimento;World Wide Web (WWW)|por|Este trabalho é dedicado ao estudo e à aplicação da mineração de regras de associação a fim de descobrir padrões de navegação no ambiente Web. As regras de associação são padrões descritivos que representam a probabilidade de um conjunto de itens aparecer em uma transação visto que outro conjunto está presente. Dentre as possibilidades de aplicação da mineração de dados na Web, a mineração do seu uso consiste na extração de regras e padrões que descrevam o perfil dos visitantes aos sites e o seu comportamento navegacional. Neste contexto, alguns trabalhos já foram propostos, contudo diversos pontos foram deixados em aberto por seus autores. O objetivo principal deste trabalho é a apresentação de um modelo para a extração de regras de associação aplicado ao uso da Web. Este modelo, denominado Access Miner, caracteriza-se por enfocar as etapas do processo de descoberta do conhecimento desde a obtenção dos dados até a apresentação das regras obtidas ao analista. Características específicas do domínio foram consideradas, como a estrutura do site, para o pósprocessamento das regras mineradas a fim de selecionar as potencialmente mais interessantes e reduzir a quantidade de regras a serem apreciadas. O projeto possibilitou a implementação de uma ferramenta para a automação das diversas etapas do processo, sendo consideradas, na sua construção, as características de interatividade e iteratividade, necessárias para a descoberta e consolidação do conhecimento. Finalmente, alguns resultados foram obtidos a partir da aplicação desta ferramenta em dois casos, de forma que o modelo proposto pôde ser validado.|http://hdl.handle.net/10183/2074
Concepção e implementação de um agente semiótico como parte de um modelo social de aprendizagem a distância|2001|Open Access|Dissertação|Informática : Educação;Ensino a distância;Sistemas multiagentes;Tutores inteligentes|por|Esta dissertação situa-se no projeto de pesquisa intitulado ""Um Modelo Computacional de Aprendizagem a Distância Baseada na Concepção Sócio- Interacionista"". Este projeto se enquadra na visão de aprendizagem situada, isto é, na concepção de cognição como uma prática social baseada na utilização de linguagem, símbolos e signos. O objetivo é a construção de um ambiente de Educação a Distância, implementado como um sistema multiagente composto por agentes artificiais e agentes humanos, inspirando-se na teoria sócio-interacionista de Vygotsky. Nesta sociedade, todos os personagens (aprendizes e agentes artificiais) são modelados como agentes sociais integrados em um ambiente de ensino-aprendizagem. A arquitetura deste sistema é formada pelos seguintes agentes artificiais: agente diagnóstico, agente mediador, agente colaborativo, agente semiótico e agente social. Os agentes humanos que interagem com o sistema desempenham o papel de tutores, aprendizes ou ambos. Esta dissertação visa à concepção e à implementação de um dos agentes desta arquitetura: o agente semiótico. Esta concepção foi baseada na Engenharia Semiótica, em particular para a apresentação do material instrucional utilizado no processo de ensinoaprendizagem.|http://hdl.handle.net/10183/2075
Distribuição de extinção na Galáxia e nas Nuvens de Magalhães|2001|Open Access|Tese|Poeira cosmica;Lei de avermelhamento interestelar;Galáxias;Nuvens de magalhaes;Aglomerados abertos e associações;Aglomerados globulares;Nebulosas;Fotometria;Centro galatico|por|o estudo da distribuição de extinção na Galáxia e nas Nuvens de Magalhães é feito através da análise dos mapas de avermelhamento derivados da emissão 100f-Lmda poeira E(B- V)FIR de Schlegel et ai. (1998). Comparamos valores de avermelhamento E(B- V)FIR com os derivados do conteúdo estelar de 103 aglomerados abertos velhos e 150 aglomerados globulares da Galáxia. As diferenças entre os dois avermelhamentos, quando significativas, ocorrem principalmente em baixas latitudes galáticas, no sentido de que os valores E(B-V)FIR são mais altos devido à contribuição do fluxo 100f-Lmda poeira que se encontra atrás dos aglomerados. As diferenças também podem surgir por um valor de E(B-V)FIR superestimado devido aos grãos de poeira terem temperatura T> 21 K o que parece ocorrer principalmente na direção do Centro da Galáxia. Construímos um catálogo geral de nebulosas escuras unificando 15 catálogos da literatura reunindo~ 6300 itens. Após cruzamentos, o catálogo unificado contém 4956 nebulosas escuras. Medimos valores de E(B-V)PIR no centro destas nebulosas escuras e amostramos seus arredores. Encontramos contraste preferencialmente para nebulosas escuras a médias e altas latitudes galáticas. Nebulosas escuras próximas ao Plano Galático apresentam flutuações maiores nos valores de E(B- V)FIR nos arredores, devido às contribuições de densidade de coluna de poeira das nebulosas e meio difuso acumulados em profundidade ao longo da linha de visada. Utilizamos a fotometria JHKs do 2MASS para obter mapas de extinção em regiões candidatas a regiões de baixa extinção (janelas) na direção do Bojo Galático  Confirmamos a existência das janelas e encontramos uma grande semelhança entre os mapas de extinção na banda K derivados a partir do conteúdo estelar e os derivados da emissão da poeira. Tal semelhança na distribuição do avermelhamento nos mapas se deve à maior parte das nuvens de poeira estar localizada entre nós e as estrelas do Bojo. Realizamos a busca de aglomerados infravermelhos jovens compactos (semelhantes aos aglomerados Arches e Quintuplet) próximo ao Centro Galático utilizando o Atlas de imagens JHKs do 2MASS. Encontramos 58 candidatos a aglomerados, importantes alvos para grande telescópios. Nas direções das Nuvens de Magalhães, testamos os valores de E(B- V)FIR nas linhas de visada de galáxias de fundo comparando esses valores com os avermelhamentos derivados através dos espectros observados das galáxias. A obtenção do avermelhamento foi feita comparando a distribuição de contínuo dos espectros das galáxias observadas na direção das Nuvens com o contínuo de espectros médios de semelhante população estelar (formados por galáxias em altas latitudes galáticas) corrigidos por extinção . O avermelhamento foi derivado para 36 galáxias projetadas sobre as Nuvens de Magalhães e obtivemos um avermelhamento médio total (galático + interno) de E(B-V) = 0.12 para a Grande Nuvem e E(B-V) = 0.05 para Pequena Nuvem, sendo o avermelhamento interno estimado em E(B- V)i = 0.06 e E(B- V)i = 0.04 para Grande e Pequena Nuvem respectivamente. Para 86 % da amostra obtivemos uma boa concordância entre os valores de avermelhamento espectroscópicos e os derivados da emissão da poeira  Os casos de diferenças significativas foram interpretados como devidos à distribuição de poeira ter uma escala menor que a resolução dos mapas de avermelhamento E(B-V)FIR ou ao superaquecimento da poeira, como ocorre principalmente na direção do Centro Galático. As presentes análises da extinção através da Galáxia e das Nuvens de Magalhães fornecem evidências da importância, utilidade e algumas limitações dos mapas de poeira de Schlegel et aI. (1998) para estudos galáticos e extragaláticos.|http://hdl.handle.net/10183/2102
Cálculo da complexidade exata de algoritmos do tipo divisão-e-conquista através das equações características|2000|Open Access|Dissertação|Análise matemática;Algoritmos recursivos;Complexidade : Algoritmos|por|A equação de complexidade de um algoritmo pode ser expressa em termos de uma equação de recorrência. A partir destas equações obtém-se uma expressão assintótica para a complexidade, provada por indução. Neste trabalho, propõem-se um esquema de solução de equações de recorrência usando equações características que são resolvidas através de um ""software"" de computação simbólica, resultando em uma expressão algébrica exata para a complexidade. O objetivo é obter uma forma geral de calcular a complexidade de um algoritmo desenvolvido pelo método Divisão-e-Conquista.|http://hdl.handle.net/10183/2133
Utilização de Multicast na Disseminação de Escritas em Ambientes Replicados|1999|Open Access|Dissertação|Tolerancia : Falhas;Sistemas distribuídos;Replicação : Servidores;Protocolos : Difusao confiavel|por|Este trabalho trata da utilização de protocolos de comunicação de grupo para a disseminação de escritas em arquivos replicados. A replicação de arquivos tem como objetivo aumentar a disponibilidade dos dados mesmo mediante a ocorrência de alguma falha. Existem duas abordagens principais para a replicação de arquivos: a da cópia primária e das cópias ativas. Em ambas as abordagens é necessário que seja mantida a integridade dos dados replicados, de forma que todos cópias dos arquivos replicados estejam no mesmo estado. Essa integridade pode ser mantida pela escolha correta de uma estratégia de disseminação de escritas. Como os servidores que mantém cópias do mesmo arquivo formam um grupo de replicação, a disseminação de escritas pode ser feita através de comunicação de grupos. Neste trabalho são apresentados os sistemas de comunicação de grupo xAMp, da Universidade de Lisboa; Totem, Universidade da Califórnia; Transis da Universidade de Hebréia de Jerusalém; Horus, da Universidade de Cornell e Newtop da Universidade de Newcastle. Todos os sistemas descritos possuem características de comunicação de grupo e membership que permitem a sua utilização na disseminação de escritas para arquivos replicados. Este trabalho descreve, também, o protótipo PDERM (Protótipo para a Disseminação de Escritas em arquivos Replicados, através de Multicast), implementado para analisar o comportamento de um sistema de comunicação de grupo, o xAMp, na disseminação de escritas em arquivos replicados pela estratégia da cópia primária. Foi analisado o aspecto da manutenção da integridade das réplicas mesmo na ocorrência de falha do servidor primário.|http://hdl.handle.net/10183/2138
JEduc : reflexão sobre a linguagem java na educação|2002|Open Access|Dissertação|Informática : Educação;Java (Linguagem de programação);Ensino : Programacao|por|Neste estudo são discutidos alguns aspectos relacionados à escolha da primeira linguagem de programação em currículos de ciência da computação, com interesse especial em Pascal e Java. A primeira linguagem é amplamente adotada para ensinar programação aos novatos, enquanto a segunda está ganhando popularidade como uma linguagem moderna e abrangente, que pode ser usada em muitas disicplinas ao longo de um curso degraduação em computação como ferramenta para ensinar desde recursos básicos de programação até tópicos mais avançados. Embora vários problemas quanto ao ensino de Java, com a primeira linguagem de programação, possam ser apontadas, consideramosque Java é uma boa escolha, visto que (a) oferece apoio a importantes questões conceituais e tecnológicos e, (b) é possível contornar algumas complexidades da linguagem e da plataforma Java para torná-las mais adequadas à alunos iniciantes. Além disso, considerando a grande popularidade de Pascal nos currículos de cursos de computação, uma eventual adoção de Java conduz à outro problema: a falta de professores aptos a lecionar programação orientada a objetos. Sugerimos que este problema de migração de Pascal para Java seja enfrentado através de smplificação do ambiente de desenvolvimento de programas, uso de um pacote com classes que facilitam a entrada e saída, e o desenvolvimento de um catálogo comparativo de programas implementados em ambas as linguagens.  Neste estudo também é apresentado o JEduc, um IDE muito simples com o objetivo de dar suporte ao ensino da linguagem de programação orientada a objetos Java aos novatos. Oferece componentes desenvolvidos em Java que integram edição, compilação e execução de programas Java. Além das funcionalidades comuns a um IDE, JEduc foi desenvolvido para gir como uma ferramente pedagógica: simplifica a maioria das mensagens do compilador e erros da JRE, permite a inserção de esqueletos de comandos, e incorpora pacotes especiais para esconder alguns detalhes sintáticos e semânticos indesejáveis.|http://hdl.handle.net/10183/2139
Utilização de técnicas de mineração de dados considerando aspectos temporais|2002|Open Access|Dissertação|Banco : Dados;Banco : Dados temporais;Mineracao : Dados;Descoberta : Conhecimento;Inteligência artificial|por|Atualmente, o enorme volume de informações armazenadas em bancos de dados de organizações ultrapassa a capacidade dos tradicionais métodos de análise dos dados baseados em consultas, pois eles se tornaram insuficientes para analisar o conteúdo quanto a algum conhecimento implícito e importante na grande massa de dados. A partir disto, a mineração de dados tem-se transformado em um tópico importante de pesquisa, porque provê um conjunto de técnicas e ferramentas capazes de inteligente e automaticamente assistir o ser humano na análise de uma enorme quantidade de dados à procura de conhecimento relevante e que está encoberto pelos demais dados. O presente trabalho se propõe a estudar e a utilizar a mineração de dados considerando os aspectos temporais. Através de um experimento realizado sobre os dados da Secretaria da Saúde do Estado do Rio Grande do Sul, com a aplicação de uma metodologia para a mineração de dados temporais, foi possível identificar padrões seqüenciais nos dados. Este experimento procurou descobrir padrões seqüenciais de comportamento em internações médicas, objetivando obter modelos de conhecimento dos dados temporais e representá-los na forma de regras temporais. A descoberta destes padrões seqüenciais permitiu comprovar tradicionais comportamentos dos tratamentos médicos efetuados, detectar situações anômalas, bem como, acompanhar a evolução das doenças existentes.|http://hdl.handle.net/10183/2144
Projeto cooperativo no Ambiente Cave baseado em espaço compartilhado de objetos|2002|Open Access|Dissertação|Microeletrônica;Cad : Microeletronica;Projeto : Circuitos integrados;Trabalho cooperativo|por|Este trabalho apresenta o módulo Collaborative Service, uma extensão do ambiente Cave, desenvolvido para suportar conceitos de trabalho cooperativo no projeto de circuitos integrados. Esta extensão por sua vez, é baseada na metodologia Pair- Programming e nas tecnologias Jini e Javaspaces. O módulo Collaborative Service foi desenvolvido para auxiliar a continuidade do processo de desenvolvimento de circuitos integrados complexos, inserindo uma dinâmica de grupo através da extensão de Pair-Programming para máquinas remotas. Esse modelo permite que dois ou mais projetistas interajam em um mesmo projeto ou blocos de projeto, independente de suas localizações geográficas e tipos de plataformas de hardware/software. Ele foi projetado para ser genérico e essa característica o torna capaz de suportar as ferramentas de CAD, atuais e futuras, do ambiente Cave (um framework de apoio ao projeto de circuitos integrados). Como estudo de caso, foram utilizadas duas ferramentas do Ambiente Cave. O primeiro caso mostra uma cooperação em nível de descrições gráficas, representada pela ferramenta Blade, um editor de esquemáticos hierárquico. O segundo caso foi representado pelo editor de descrições textuais (VHDL, Verilog e Linguagem C), chamado Homero.  No estudo de caso com a ferramenta Blade foi demonstrado que a cooperação proposta por esse modelo pode atuar sob diferentes níveis de hierarquia de projeto, além de suportar a interação de inúmeros projetistas em um mesmo bloco. Na ferramenta Homero, demonstrou-se a cooperação em nível de descrições textuais, representados por (códigos) projetos VHDL acrescidos da participação de vários projetistas. Com esses exemplos, foi possível demonstrar as estratégias de percepção e comunicação com os projetistas, além de descrever a criação de blocos de projeto de uma forma cooperativa. Como contribuição desse trabalho, acrescenta-se ao Ambiente Cave mais um recurso para o projeto de circuitos integrados. Nesse sentido, grupos de projetistas podem projetar um sistema ou circuito integrado de forma cooperativa utilizando-se das funcionalidades desse modelo.|http://hdl.handle.net/10183/2150
Simulação paralela de eventos discretos com uso de memória compartilhada distribuída|2000|Open Access|Dissertação|Simulação;Simulacao paralela;Memória compartilhada;Memoria distribuida|por|A simulação paralela de eventos é uma área da computação que congrega grande volume de pesquisas, pela importância em facilitar o estudo de novas soluções nas mais diferentes áreas da ciência e tecnologia, sem a necessidade da construção de onerosos protótipos. Diversos protocolos de simulação paralela podem ser encontrados, divididos em dois grandes grupos de acordo com o algoritmo empregado para a execução em ordem dos eventos: os conservadores e os otimistas; contudo, ambos os grupos utilizam trocas de mensagens para a sincronização e comunicação. Neste trabalho, foi desenvolvido um novo protocolo de simulação paralela, fazendo uso de memória compartilhada, o qual foi implementado e testado sobre um ambiente de estações de trabalho, realizando, assim, simulação paralela com uso de memória compartilhada distribuída. O protocolo foi desenvolvido tendo como base de funcionamento os protocolos conservadores; utilizou diversas características dos mesmos, mas introduziu várias mudanças em seu funcionamento. Sua execução assemelha-se às dos protocolos de execução síncrona, utilizando conceitos como o lookahead e janelas de tempo para execução de eventos. A principal mudança que o novo protocolo sofreu foi proporcionada pelo acesso remoto à memória de um LP por outro, produzindo diversas outras nas funções relativas à sincronização dos processos, como o avanço local da simulação e o agendamento de novos eventos oriundos de outro LP.  Um ganho adicional obtido foi a fácil resolução do deadlock, um dos grandes problemas dos protocolos conservadores de simulação paralela. A construção de uma interface de comunicação eficiente com uso de memória compartilhada é o principal enfoque do protocolo, sendo, ao final da execução de uma simulação, disponibilizado o tempo de simulação e o tempo de processamento ocioso (quantia utilizada em comunicação e sincronização). Além de uma implementação facilitada, propiciada pelo uso de memória compartilhada ao invés de trocas de mensagens, o protocolo oferece a possibilidade de melhor ocupar o tempo ocioso dos processadores, originado por esperas cada vez que um LP chega a uma barreira de sincronização. Em nenhum momento as modificações efetuadas infringiram o princípio operacional dos protocolos conservadores, que é não possibilitar a ocorrência de erros de causalidade local. O novo protocolo de simulação foi implementado e testado sobre um ambiente multicomputador de memória distribuída, e seus resultados foram comparados com dois outros simuladores, os quais adotaram as mesmas estratégias, com idênticas ferramentas e testados em um mesmo ambiente de execução. Um simulador implementado não utilizou paralelismo, tendo seus resultados sido utilizados como base para medir o speedup e a eficiência do novo protocolo. O outro simulador implementado utilizou um protocolo conservador tradicional, descrito na literatura, realizando as funções de comunicação e sincronização através de trocas de mensagens; serviu para uma comparação direta do desempenho do novo protocolo proposto, cujos resultados foram comparados e analisados.|http://hdl.handle.net/10183/2151
Contaminação do meio físico por hidrocarbonetos e metais na área da Refinaria Alberto Pasqualini, Canoas, RS|2003|Open Access|Dissertação|Hidrogeoquimica;Contaminação da água;Refinaria Alberto Pasqualini. Canoas (RS);Contaminação do solo|por|O presente estudo resulta de convênio entre a Fundação de Apoio da Universidade Federal do Rio Grande do Sul (FAURGS) e a Refinaria Alberto Pasqualini (REFAP S.A.), tendo como objetivo o diagnóstico das contaminações por metais e hidrocarbonetos dos solos e águas subterrâneas na área e seus domínios. Os poços tubulares profundos existentes ao redor da refinaria registram 559 unidades distribuídas num raio de 2.000 metros, que têm uso residencial dominante (81,5%) e, em menor escala, comercial (6,5%), industrial (2,6%) e público (0,9%). A caracterização geológica revelou grande variação lateral e vertical, marcada pela ocorrência de sedimentos quaternários síltico-argilosos e arenosos finos com lentes de argilas orgânicas plásticas, tendo espessura de até 14 metros cobrindo rochas sedimentares da Formação Sanga do Cabral. Esta última composta principalmente por arenitos finos a muito finos, siltitos e argilitos avermelhados. Uma parcela significativa da área encontra-se capeada por aterros de terraplenagem com espessuras entre 0,5 e 4 metros. Os estudos hidrogeológicos da área registram a existência de um aqüífero freático livre, constituído pelo material de aterro e camadas sedimentares superficiais, e outro semi-confinado, representado por níveis arenosos descontínuos intercalados no pacote dominantemente síltico-argiloso de sedimentos quaternários. Os sedimentitos da Formação Sanga do Cabral fazem parte do aqüífero semiconfinado.  O diagnóstico da contaminação de solos por hidrocarbonetos e metais contou com a análise química de 94 amostras coletadas em 48 sondagens manuais em diferentes profundidades. Os resultados apontaram a virtual ausência de vanádio e baixos níveis de contaminação por cromo, mercúrio, cádmio, cobre, chumbo e Hidrocarbonetos Totais de Petróleo (TPH). A grande quantidade de amostras permitiu a realização de uma avaliação geoestatística para determinação dos teores naturais do terreno e das concentrações oriundas de contaminações. Os solos presentes no entorno da bacia de aeração e da área de descarte de borras e caliças apresentaram as maiores concentrações de metais e, mesmo que inexpressivas, de TPH. A avaliação da qualidade das águas subterrâneas do aqüífero semiconfinado foi realizada através da instalação de 10 poços de monitoramento com profundidades da ordem de 20 metros e coleta de amostras para análises químicas. Também foram coletadas amostras do aqüífero freático em 9 poços de monitoramento rasos com profundidades da ordem de 5 metros, já existentes na ocasião. Os resultados obtidos para as águas do aqüífero semi-confinado retratam uma boa situação ambiental deste, com teores de metais inexistentes na grande maioria dos poços e concentrações abaixo dos limites de intervenção apenas em alguns. Hidrocarbonetos de petróleo (TPH) foram registrados em apenas um poço, em quantidade inexpressiva. Os poços rasos revelam um cenário menos favorável no aqüífero freático, apontando contaminação pouco significativa por hidrocarbonetos de petróleo em apenas 4 locais.|http://hdl.handle.net/10183/2164
Monitoração de protocolos de alto nível através da implementação de um agente RMON2|2001|Open Access|Dissertação|Redes : Computadores;Gerencia : Redes : Computadores;Protocolos : Alto nível;Tcp/ip|por|A área de gerência de rede cresce à medida que redes mais seguras e menos vulneráveis são necessárias, e as aplicações que concorrem pelo seu uso, necessitam de alta disponibilidade e qualidade de serviço. Quando estamos focando a gerência da infra-estrutura física das redes de computadores, por exemplo, a taxa de uso de um segmento de rede, podemos dizer que esse tipo de gerenciamento encontra-se em um patamar bastante sedimentado e testado. Por outro lado, há ainda lacunas para pesquisar na área de gerenciamento de protocolos de alto nível. Entender o comportamento da rede como um todo, conhecer quais hosts mais se comunicam, quais aplicações geram mais tráfego e, baseado nessas estatísticas, gerar uma política para a distribuição de recursos levando em consideração as aplicações críticas é um dever nas redes atuais. O grupo de trabalho IETF RMON padronizou, em 1997, a MIB RMON2. Ela foi criada para permitir a monitoração de protocolos das camadas superiores (rede, transporte e aplicação), a qual é uma boa alternativa para realizar as tarefas de gerenciamento recém mencionadas.  Outro problema para os gerentes de rede é a proliferação dos protocolos de alto nível e aplicações corporativas distribuídas. Devido a crescente quantidade de protocolos e aplicações sendo executados sobre as redes de computadores, os softwares de gerenciamento necessitam ser adaptados para serem capazes de gerenciá-los. Isso, atualmente, não é fácil porque é necessário usar linguagens de programação de baixo nível ou atualizar o firmware dos equipamentos de monitoração, como ocorre com os probes RMON2. Considerando este contexto, esse trabalho propõe o desenvolvimento de um agente RMON2 que contemple alguns grupos dessa MIB. O agente baseia-se na monitoração protocolos de alto nível e aplicações que são executados sobre o IP (Internet Protocol) e Ethernet (na camada de enlace). Além da implementação do agente, o trabalho apresenta um estudo de como obter estatísticas do agente RMON2 e usá-las efetivamente para gerenciar protocolos de alto nível e aplicações.|http://hdl.handle.net/10183/2176
Novos algoritmos para roteamento de circuitos VLSI|2001|Open Access|Tese|Microeletrônica;Cad : Microeletronica;Roteamento : Circuitos integrados;Sintese : Layout|por|Este trabalho apresenta novos algoritmos para o roteamento de circuitos integrados, e discute sua aplicação em sistemas de síntese de leiaute. As interconexões têm grande impacto no desempenho de circuitos em tecnologias recentes, e os algoritmos propostos visam conferir maior controle sobre sua qualidade, e maior convergência na tarefa de encontrar uma solução aceitável. De todos os problemas de roteamento, dois são de especial importância: roteamento de redes uma a uma com algoritmos de pesquisa de caminhos, e o chamado roteamento de área. Para o primeiro, procura-se desenvolver um algoritmo de pesquisa de caminhos bidirecional e heurístico mais eficiente, LCS*, cuja aplicação em roteamento explora situações específicas que ocorrem neste domínio. Demonstra-se que o modelo de custo influencia fortemente o esforço de pesquisa, além de controlar a qualidade das rotas encontradas, e por esta razão um modelo mais preciso é proposto. Para roteamento de área, se estuda o desenvolvimento de uma nova classe de algoritmos sugerida em [JOH 94], denominados LEGAL. A viabilidade e a eficiência de tais algoritmos são demonstradas com três diferentes implementações. Devem ser também estudados mecanismos alternativos para gerenciar espaços e tratar modelos de grade não uniforme, avaliando-se suas vantagens e sua aplicabilidade em outros diferentes contextos.|http://hdl.handle.net/10183/2177
Extração semântica de dados semi-estruturados através de exemplos e ferramentas visuais|2001|Open Access|Dissertação|Armazenamento : Dados;Recuperacao : Informacao;Banco : Dados semi-estruturados;Extração semântica|por|Existe uma necessidade latente de pesquisar, filtrar e manipular informações disponíveis em diversos formatos irregulares, entre elas as informações distribuídas na WWW (World Wide Web). Esses tipos de dados são semi-estruturados, pois não possuem uma estrutura explícita e regular, o que dificulta sua manipulação. Este trabalho apresenta como proposta o projeto de uma ferramenta para realizar a extração semântica e semi-automática de dados semi-estruturados. O usuário especifica, através de uma interface visual, um exemplo da estrutura hierárquica do documento e de seu relacionamento com os conceitos da ontologia, gerando uma gramática descritiva da estrutura implícita do mesmo. A partir dessa gramática, a ferramenta realiza a extração dos próximos documentos de forma automática, reestruturando o resultado em um formato regular de dados, neste caso, XML (eXtensible Markup Language). Além da conceituação do método de extração, são apresentados os experimentos realizados com o protótipo da ferramenta, bem como, os resultados obtidos nestes experimentos. Para a construção desta ferramenta, são analisadas características de outros métodos que constituem o estado da arte em extração de dados semi-estruturados.|http://hdl.handle.net/10183/2178
Testes de ajustamento de modelos em processos com longa dependência|2003|Open Access|Dissertação|Análise de séries temporais|por|Estudamos neste trabalho, o nível de significância empírico dos testes portmanteau baseados nas estatísticas propostas por Ljung e Box (1978), Monti (1994) e Pe˜na e Rodríguez (2002) nos processos ARFIMA(p; d; q). Consideramos o processo ARFIMA(p; d; q) nas situações adequadas para representar séries temporais com características de longa dependência. Para estimar o parâmetro de diferenciação d utilizamos os métodos de estimação propostos por Geweke e Porter-Hudak (1983), Reisen (1994), Robinson (1994) e Fox e Taqqu (1983).|http://hdl.handle.net/10183/2183
Solução da equação de transporte de fótons para uma placa plana heterogênea, modelo de multigrupo com núcleo de espalhamento de Klein-Nishina|2002|Open Access|Dissertação|Equação de transporte;Método LTSn;Fluxo de fótons;Técnica de expansão de Heaviside;Modelo de multigrupo|por|Neste trabalho o método LTSN é utilizado para resolver a equação de transporte de fótons para uma placa plana heterogênea, modelo de multigrupo, com núcleo de espalhamento de Klein-Nishina, obtendo-se o fluxo de fótons em valores discretos de energia. O fluxo de fótons, juntamente com os parâmetros da placa foram usados para o cálculo da taxa de dose absorvida e do fator de buildup. O método LTSN consiste na aplicação da transformada de Laplace num conjunto de equações de ordenadas discretas, fornece uma solução analítica do sistema de equações lineares algébricas e a construção dos fluxos angulares pela técnica de expansão de Heaviside. Essa formulação foi aplicada ao cálculo de dose absorvida e ao fator de Buildup, considerando cinco valores de energia. Resultados numéricos são apresentados.|http://hdl.handle.net/10183/2195
Um modelo partônico para a difração aplicado ao DIS|1998|Open Access|Dissertação|Espalhamento inelastico profundo;Espalhamento eletron proton;Trajetórias de Regge;Cromodinâmica quântica;Pomerons;Teoria de perturbacao|por|A observação de uma nova classe de eventos no espalhamento profundamente inelástico (D18) elétron-próton tem gerado grande interesse teórico e experimental. Estes eventos apresentam grandes intervalos na variável pseudo-rapidez (""7)sem deteção de partículas e são interpretados como oriundos de processos com caráter difrativo. A difração é descrita segundo uma base teórica fundamentada na Teoria de Regge, a qual já era utilizada na descrição das interações difrativas nas colisões hadrônicas. A Cromodinâmica Quântica (QCD) descreve com boa concordância todas as características usuais do D18, entretanto, frente a esses fenômenos novos elementos estão sendo incorporados na teoria. A questão principal é investigar se os processos difrativos são dominados pela troca da trajetória de Regge dominante, o Pomeron, ou por interações perturbativas descritas pela QCD. OS dados existentes não permitem distingüir qual a dominância e possivelmente há uma mistura de efeitos perturbativos QCD e não-perturbativos (Regge). Neste trabalho contrapomos o modelo de Regge e o Modelo Partônico para a difração. O observável físico utilizado nas análises é a função de estrutura difrativa Ff(xjp, {3,Q2). Apontamos as características e predições do Modelo Partônico para este observável. Encontramos que o modelo descreve os dados, entretanto há problemas em regiões específicas do espectro das variáveis cinemáticas. Recentes medidas da função de estrutura apresentam uma quebra de sua fatorização. No sentido de explicar esta nova característica introduzimos uma troca extra de reggeon, de caráter não-perturbativo, como uma extensão do Modelo Partônico. Os resultados mostram que a troca de reggeon é importante para {3< 0.4 e descreve bem a quebra de fatorização. Há melhoria na dependência para pequeno {3,entretanto a troca de reggeon é pouco sensível à variação do momento transferido Q2.|http://hdl.handle.net/10183/2203
Soluções reutilizáveis para a implementação de mecanismos de controle de atomicidade em programas tolerantes a falhas|2001|Open Access|Dissertação|Programação;Tolerancia : Falhas;Reflexao computacional;Reutilizacao : Software|por|Tolerância a falhas é um dos aspectos mais importantes a serem considerados no desenvolvimento de aplicações, especialmente com a participação cada vez maior de sistemas computacionais em áreas vitais da atividade humana. Dentro deste cenário, um dos fatores a serem considerados na persecução deste objetivo é o gerenciamento de atomicidade. Esta propriedade, por sua vez, apresenta duas vertentes principais: o controle de concorrência e a recuperação de estados. Considerando-se a tolerância a falhas e, particularmente, a atomicidade como requisitos com alto grau de recorrência em aplicações, verifica-se a importância de sua reutilização de forma simples e transparente e do estudo de meios de prover tal capacidade. O presente trabalho procurou pesquisar e aplicar meios de produzir soluções reutilizáveis para implementação de programas tolerantes a falhas, mais especificamente de técnicas de controle de atomicidade, utilizando vários paradigmas computacionais. Neste intuito, foram pesquisados mecanismos de introdução de atomicidade em aplicações e suas respectivas demandas, para então extrair critérios de análise dos paradigmas a serem utilizados na implementações das soluções. Buscou-se suporte nestes paradigmas às demandas previamente pesquisadas nos mecanismos de gerenciamento de atomicidade e procurou-se chegar a soluções reutilizáveis mantendo simplicidade de uso, possibilidade de alteração dinâmica, transparência, adaptabilidade e velocidade de desenvolvimento.  Devido à existência de uma grande diversidade de situações que requerem diferentes implementações de atomicidade, alguns cenários típicos foram selecionados para aplicação e avaliação das técnicas aqui sugeridas, procurando abranger o maior número possível de possibilidades. Desta maneira, este trabalho comparou situações opostas quanto à concorrência pelos dados, implementando cenários onde ocorrem tanto acesso cooperativo quanto competitivo aos dados. Dentro de cada um dos cenários estudados, buscaram-se situações propícias ao emprego das características dos paradigmas e analisou-se o resultado de sua aplicação quanto aos critérios definidos anteriormente. Várias soluções foram analisadas e comparadas. Além dos mecanismos de gerenciamento de atomicidade, também foram estudados vários paradigmas que pudessem ser empregados na implementação de soluções com alto grau de reutilização e adaptabilidade. As análises e sugestões posteriores às implementações serviram como substrato para conclusões e sugestões sobre a melhor maneira de empregar tais soluções nos cenários atômicos estudados. Com isso, foi possível relacionar características e capacidades de cada paradigma com a melhor situação de demanda de atomicidade na qual os mesmos são aplicáveis, moldando uma linha de soluções que favoreçam sua reutilização. Um dos objetivos mais importantes do trabalho foi, entretanto, observar o funcionamento conjunto destes paradigmas, estudando como os mesmos podem atuar de forma simbiótica e de que forma os conceitos de um paradigma podem complementar os de outro.|http://hdl.handle.net/10183/2242
Uma Técnica de depuração e teste de circuitos integrados usando um microscópio eletrônico|1986|Open Access|Dissertação|Microeletrônica;Testes : Circuitos integrados;Microscopio eletronico : Varredura;Depuração : Circuitos integrados|por|O trabalho tem por objetivo mostrar uma técnica de depuração de circuitos integrados VLSI, utilizando um microscópio eletrônico de varredura (MEV) aliado ao fenômeno de contraste por tensão. São abordadas a descrição da ferramenta, técnicas de observação e depuração dos circuitos, bem como, são sugeridas estratégias de concepção visando facilitar a depuração dos circuitos. Embora tenham sido utilizados circuitos NMOS para realizar as experiências, a técnica é aplicável a circuitos MOS em geral. Resultados experimentais, utilizando circuitos projetados no PGCC, são apresentados.|http://hdl.handle.net/10183/2245
Um estudo sobre o desempenho de alunos de física usuários da ferramenta computacional Modellus na interpretação de gráficos em cinemática|2002|Open Access|Dissertação|Ensino de física;Cinemática;Métodos computacionais|por|O objetivo deste trabalho foi o de investigar o desempenho de estudantes quando expostos a atividades complementares de modelagem computacional na aprendizagem de Física, utilizando o software Modellus. Interpretação de gráficos da Cinemática foi o tópico de Física escolhido para investigação. A fundamentação teórica adotada esteve baseada na teoria de Halloun sobre modelagem esquemática e na teoria de Ausubel sobre aprendizagem significativa. O estudo envolveu estudantes do primeiro ano do curso de Física da Universidade Federal do Rio Grande do Sul. Vinte seis destes estudantes - grupo experimental - foram submetidos a atividades de modelagem exploratórias e de criação durante um breve intervalo de tempo (quatro encontros, com 2h15min cada). Vinte e seis outros estudantes constituíram um grupo de controle, adotando-se um delineamento quasi-experimental. Os resultados deste trabalho mostram que houve melhorias estatisticamente significativas no desempenho dos alunos do grupo experimental, quando comparado aos estudantes do grupo de controle, submetidos apenas ao método tradicional de ensino. A percepção do aluno em relação à relevância de conceitos e relações matemáticas, bem como a motivação para aprender, gerada pelas atividades, desempenharam um papel fundamental nesses resultados. Além disso, registrou-se alta receptividade em relação ao tratamento utilizado.|http://hdl.handle.net/10183/2251
Holoparadigma : um modelo multiparadigma orientado ao desenvolvimento de software distribuído|2002|Open Access|Tese|Engenharia : Software;Holoparadigma;Software distribuido;Sistemas distribuídos|por|Este texto apresenta um novo modelo multiparadigma orientado ao desenvolvimento de software distribuído, denominado Holoparadigma. O Holoparadigma possui uma semântica simples e distribuída. Sendo assim, estimula a modelagem subliminar da distribuição e sua exploração automática. A proposta é baseada em estudos relacionados com modelos multiparadigma, arquitetura de software, sistemas blackboard, sistemas distribuídos, mobilidade e grupos. Inicialmente, o texto descreve o modelo. Logo após, é apresentada a Hololinguagem, uma linguagem de programação que implementa os conceitos propostos pelo Holoparadigma. A linguagem integra os paradigmas em lógica, imperativo e orientado a objetos. Além disso, utiliza um modelo de coordenação que suporta invocações implícitas (blackboard) e explícitas (mensagens). A Hololinguagem suporta ainda, concorrência, modularidade, mobilidade e encapsulamento de blackboards em tipos abstratos de dados. Finalmente, o texto descreve a implementação da Holoplataforma, ou seja, uma plataforma de desenvolvimento e execução para a Hololinguagem. A Holoplataforma é composta de três partes: uma ferramenta de conversão de programas da Hololinguagem para Java (ferramenta HoloJava), um ambiente de desenvolvimento integrado (ambiente HoloEnv) e um plataforma de execução distribuída (plataforma DHolo).|http://hdl.handle.net/10183/2255
A emissão das galáxias esferoidais no infravermelho médio|2002|Open Access|Tese|Galáxias;Fotometria;Fontes de infravermelho : Astronomia;Poeira cosmica;Bojos de galaxias|por|Propriedades fotométricas no infravermelho médio são apresentadas para uma amostra de 28 galáxias esferoidais, observadas em 6.75, 9.63 e 15 jlk com o instrumento ISOCAM a bordo do satélite ISO. As distribuições espectrais de energia (DEE) das galáxias foram derivadas usando dados do infravermelho médio junto com dados do UV, do ótico e do infravermelho próximo, previamente publicados. Estas DEE mostram duas componentes: a poeira quente aquecida até T ~ 260 K dominando a emissão no infravermelho médio e a população estelar com T ~ 4000 K, que domina no ótico. Da emissão no infravermelho médio pode ser visto que a morfologia em 6.75 μm, onde a contribuição estelar é importante, é esferoidal. Em comprimentos de onda mais longos, onde há majoritariamente emissão da poeira, a morfologia é menos suave, mostrando nuvens, filamentos e discos de poeira. Também pode ser inferido que há um gradiente de temperatura da poeira, que cresce em direção ao centro da galáxia. Os perfis de energia mostram bojos R¼ simples, composição de bojos e discos e perfis irregulares. As luminosidades no infravermelho médio estão na faixa de(3 - 42) x 10 L. As propriedades dos grãos de poeira são inferidos das cores no infravermelho médio. A emissão no infravermelho vem principalmente de grãos muito pequenos, que estão num equilíbrio térmico oscilante. As massas para a componente quente da poeira estão na faixa de 10 - 400 M.|http://hdl.handle.net/10183/2257
Conceitos e tecnologias para educação musical baseada na web|2002|Open Access|Dissertação|Informatica (Computação musical;Ensino a distância;Internet;Educação musical;Informática : Educação|por|O presente trabalho insere-se no contexto das pesquisas realizadas no Laboratório de Computação & Música do Instituto de Informática da UFRGS. Com ele pretendemos fundamentar e investigar possibilidades em educação musical através da World Wide Web (WWW ou, simplesmente, Web). Para isso, em um primeiro momento, investigamos como desenvolver adequadamente sistemas educativo-musicais para a Web. Queremos aproveitar uma das principais vantagens que a Web oferece para a educação: a de facilitar a disponibilização e o acesso ao conteúdo educativo. Especificamente nesta área do conhecimento - Música -, é rara a pesquisa visando utilizar a Web como suporte. A Internet continua impondo sérias limitações ao emprego de multimídia e ainda mais quando seus dados representam informações sonoras e musicais. Devido a isso, os poucos estudos existentes optam ou por uma simplificação exagerada do sistema ou por soluções proprietárias muito complicadas, que podem reduzir a facilidade de acesso do público-alvo. Assim, no presente trabalho procuramos encontrar um meio-termo: uma solução de compromisso entre a funcionalidade que se espera de tais sistemas, a sua operacionalidade e a simplicidade que a Internet ainda impõe.  Para atingir esse objetivo, nos concentramos em promover a interatividade entre o aluno e um ambiente de aprendizado distribuído para o domínio musical. Buscamos fundamentar essa interatividade a partir de: a) conceitos pertinentes a uma interação de boa qualidade para propósitos de ensino/aprendizagem; e b) adoção de tecnologias da Web para música que permitam a implementação adequada desses conceitos. Portanto este trabalho é eminentemente interdisciplinar, envolvendo principalmente estudos das áreas de Interação Humano-Computador, Educação Musical e Multimídia. Após essa fase inicial de fundamentação, investigamos uma solução possível para esse problema na forma de um protótipo de um sistema educativo-musical na Web, tendo em vista os seguintes requisitos: · Ser fácil de programar, mas suficiente para satisfazer os requisitos de sistemas musicais. · Ser acessível, útil e usável pelos seus usuários (notadamente alunos e educadores musicais). Esse protótipo - INTERVALOS, que visa auxiliar o ensino/aprendizagem da teoria de intervalos, arpejos e escalas musicais - é uma ferramenta que pode ser integrada a um ambiente mais completo de educação musical na Web, incluindo as demais tecnologias da Internet necessárias para implementar Ensino a Distancia de música nesse meio. INTERVALOS foi submetido a avaliações de usabilidade e avaliações pedagógicas, por meio das quais pretendemos validar o grau de adequação da fundamentação teórica (conceitos) e tecnológica (tecnologias) para educação musical baseada na Web.|http://hdl.handle.net/10183/2263
Ferramenta de apoio ao teste de aplicações java baseada em reflexão computacional|2001|Open Access|Dissertação|Engenharia : Software;Testes : Software;Orientacao : Objetos;Reflexao computacional|por|A atividade de teste constitui uma fase de grande importância no processo de desenvolvimento de software, tendo como objetivo garantir um alto grau de confiabilidade nos produtos desenvolvidos. O paradigma da Orientação a Objetos (OO) surgiu com o objetivo de melhorar a qualidade bem como a produtividade no desenvolvimento de aplicações. Entretanto, apesar do aumento constante de aceitação do paradigma OO pela indústria de software, a presença de algumas de suas características torna a atividade de teste de programas neste paradigma mais complexa do que o teste de sistemas tradicionais. Entre estas características cita-se a herança, o encapsulamento, o polimorfismo e a ligação dinâmica [EIS 97] [PRE 95] [UNG 97]. Algumas técnicas estão sendo implementadas para auxiliarem a atividade de teste através do uso da tecnologia de reflexão computacional [HER 99]. Estas técnicas permitem a realização de análises de aspectos dinâmicos dos programas, sem a necessidade de instrumentar o código-fonte das aplicações que estão sendo monitoradas. Com o objetivo de auxiliar o processo de teste de programas orientados a objetos, este trabalho aborda o desenvolvimento de uma ferramenta, a qual automatiza parcialmente o teste de programas escritos em Java. A ferramenta evidencia o teste de estados fazendo uso da tecnologia de reflexão computacional. Através da especificação de asserções, feitas pelo usuário da ferramenta, na forma de invariantes de classe, pré e pós-condições de métodos, é possível verificar a integridade dos estados dos objetos durante a execução do programa em teste. A ferramenta possibilita também, armazenar a seqüência de métodos chamados pelos objetos da aplicação em teste, tornando possível ao testador, visualizar o histórico das interações entre os objetos criados no nível-base.|http://hdl.handle.net/10183/2283
Interação de espirais em 2D : redução da dinâmica à interação de defeitos e exploração de novas estruturas espaço-temporais|2003|Open Access|Dissertação|Sistemas complexos;Configurações no plano espaço-tempo;Teoria de landau ginzburg;Vortices;Fluidos;Vidro;Turbulência;Biofísica|por|O presente trabalho apresenta um anova proposta de tratamento de estruturas espirais em meios contínuos oscilatórios na vizinhança de bifurcações de Hopf supercríticas. Tais estruturas são normalmente descritas pela Equação de Cinzburg-Landau Complexa a qual usa um campo complexo associado a essas oscilações. A proposta apresentada reduz a dinâmica de espirais à interação entre os centros das mesmas. Inicialmente, comparamos numericamente as duas descrições e com os ganhos computacionais decorrentes da abordagem reduzida caracterizamos finamente as estruturas espaço-temporais formadas nesses sistemas: em vez dos estados congelados mencionados anteriormente na literatura encontrou-se uma dinâmica espaço-temporal intermitente. Esse regime ocorre em duas fases distintas: Líquido de Vórtices e Vidros de Vórtices. Esta última evolui em escalas de tempo ultralentas como fenômenos semelhantes encontrados na Mecânica Estatística, apesar de sua origem puramente determinista.|http://hdl.handle.net/10183/2286
Uma Biblioteca para programação paralela por troca de mensagens de clusters baseados na tecnologia SCI|2001|Open Access|Dissertação|Arquitetura de computadores;Sci;Cluster;Redes : Alto desempenho|por|A presente Dissertação propõe uma biblioteca de comunicação de alto desempenho, baseada em troca de mensagens, especificamente projetada para explorar eficientemente as potencialidades da tecnologia SCI (Scalable Coherent Interface). No âmago da referida biblioteca, a qual se denominou DECK/SCI, acham-se três protocolos de comunicação distintos: um protocolo de baixa latência e mínimo overhead, especializado na troca de mensagens pequenas; um protocolo de propósito geral; e um protocolo de comunicação que emprega uma técnica de zero-copy, também idealizada neste Trabalho, no intuito de elevar a máxima largura de banda alcançável durante a transmissão de mensagens grandes. As pesquisas desenvolvidas no decurso da Dissertação que se lhe apresenta têm por mister proporcionar um ambiente para o desenvolvimento de aplicações paralelas, que demandam alto desempenho computacional, em clusters que se utilizam da tecnologia SCI como rede de comunicação. A grande motivação para os esforços envidados reside na consolidação dos clusters como arquiteturas, a um só tempo, tecnologicamente comparáveis às máquinas paralelas dedicadas, e economicamente viáveis.  A interface de programação exportada pelo DECK/SCI aos usuários abarca o mesmo conjunto de primitivas da biblioteca DECK (Distributed Execution Communication Kernel), concebida originalmente com vistas à consecução de alto desempenho sobre a tecnologia Myrinet. Os resultados auferidos com o uso do DECK/SCI revelam a eficiência dos mecanismos projetados, e a utilização profícua das características de alto desempenho intrínsecas da rede SCI, haja visto que se obteve uma performance muito próxima dos limites tecnológicos impostos pela arquitetura subjacente. Outrossim, a execução de uma clássica aplicação paralela, para fins de validação, testemunha que as primitivas e abstrações fornecidas pelo DECK/SCI mantêm estritamente a mesma semântica da interface de programação do original DECK.|http://hdl.handle.net/10183/2325
INFIMO : um toolkit para experimentos de intrusão de injetores de falhas|2001|Open Access|Tese|Sistemas : Tempo real;Tolerancia : Falhas;Injecao : Falhas|por|Técnicas de tolerância a falhas visam a aumentar a dependabilidade dos sistemas nos quais são empregadas. Entretanto, há necessidade de garantir a confiança na capacidade do sistema em fornecer o serviço especificado. A validação possui como objetivo propiciar essa garantia. Uma técnica de validação bastante utilizada é a injeção de falhas, que consiste na introdução controlada de falhas no sistema para observar seu comportamento. A técnica de injeção de falhas acelera a ocorrência de falhas em um sistema. Com isso, ao invés de esperar pela ocorrência espontânea das falhas, pode-se introduzi-las intencionalmente, controlando o tipo, a localização, o disparo e a duração das falhas. Injeção de falhas pode ser implementada por hardware, software ou simulação. Neste trabalho são enfocadas técnicas de injeção de falhas por software, desenvolvidas nos níveis da aplicação e do sistema operacional. O trabalho apresenta o problema da validação, através da injeção de falhas, de um protocolo de troca de pacotes. Enfoque especial é dado ao impacto resultante da inclusão de um módulo extra no protocolo, uma vez que o mesmo apresenta restrições temporais. O trabalho investiga alternativas de implementação de injetores de falhas por software que minimizem este impacto. Tais alternativas referem-se a localização do injetor de falhas no sistema, a forma de ativação das atividades do injetor de falhas e a operação de injeção de falhas em si.  Um toolkit para experimentos de intrusão da injeção de falhas é apresentado. O alvo da injeção de falhas é um protocolo com característica tempo real. O toolkit desenvolvido, denominado INFIMO (INtrusiveless Fault Injector MOdule), visa a analisar, de forma experimental, a intrusão do injetor de falhas sobre o protocolo alvo. O INFIMO preocupa-se com protocolos com restrições temporais por esses constituírem um desafio sob o ponto de vista de injeção de falhas. O INFIMO suporta falhas de comunicação, as quais podem ocasionar a omissão de alguns pacotes. O INFIMO apresenta duas ferramentas de injeção de falhas: INFIMO_LIB, implementada no nível da aplicação e INFIMO_DBG implementada com auxílio de recursos do sistema operacional. Destacam-se ainda como contribuições do INFIMO a definição e a implementação do protocolo alvo para experimentos de injeção de falhas, o protocolo INFIMO_TAP. Além disso, o INFIMO apresenta métricas para avaliação da intrusão provocada pelo injetor de falhas no protocolo alvo.|http://hdl.handle.net/10183/2326
Métodos para caracterização de desempenho de CPUs industriais|2002|Open Access|Dissertação|Automação industrial;Avaliacao : Desempenho;Sistemas : Tempo real;Cpu|por|A caracterização de desempenho e uma atividade fundamental na area de controle industrial. Por se tratar, na maior parte das vezes, de aplicações de tempo real, a caracterização de desempenho torna-se ainda mais necessária e importante. Entretanto, atualmente não há nenhuma metodologia estabelecida para realizar esta caracterização. Não há nem mesmo um conjunto de parâmetros que devem ser avaliados em um equipamento de controle utilizado em processos industriais. Para tentar suprir esta carência, este trabalho apresenta uma proposta de métricas e workloads para serem utilizados na avaliação de desempenho de sistemas de controle baseados em CLPs e CPUs Industriais. O processo de avaliação de desempenho e discutido em todas as etapas, desde o estudo da aplicação at e a execução dos passos de caracterização de desempenho. Para ilustrar a aplicação das métricas, técnicas e procedimentos propostos, são avaliadas três CPUs Industriais, e os resultados s~ao apresentados ao nal do trabalho. Espera-se assim estar contribuindo para o estabelecimento de uma metodologia padronizada para avaliação de desempenho de equipamentos de controle industrial.|http://hdl.handle.net/10183/2327
Reconhecimento de caracteres alfanuméricos de placas em imagens de veículos|2001|Open Access|Dissertação|Computação gráfica;Visão computacional;Processamento de imagens;Reconhecimento : Caracteres;Redes neurais|por|Sistemas de visão artificial são cada vez mais usados para auxiliar seres humanos a realizar diferentes tarefas. Estes sistemas são capazes de reconhecer padrões em imagens complexas. Técnicas de visão computacional têm encontrado crescente aplicação em estudos e sistemas de controle e monitoração de tráfego de automóveis. Uma das áreas de pesquisa que tem sido objeto de estudo por diferentes grupos é a leitura automática de placas de matrículas como forma de detectar transgressores, encontrar carros roubados ou efetuar estudos de origem/destino [BAR99]. Com o constante crescimento do volume de tráfego de automóvel e a limitada capacidade dos sensores convencionais, especialistas da área recorrem a técnicas de identificação automática de veículos para obter dados relativos ao escoamento de tráfego. A identificação automática de veículos tem tido essencialmente duas abordagens distintas: a utilização de transponders e a utilização de técnicas de visão computacional [INI85] . Estas são essencialmente úteis em casos em que não é viável obrigar os motoristas a instalar transponders em seus automóveis. No entanto, essas técnicas são mais sensíveis às condições atmosféricas e de iluminação tais como nevoeiros, chuva intensa, luz noturna, reflexos em superfícies, etc. Este trabalho apresenta um estudo de diversas técnicas de processamento de imagem objetivando o aperfeiçoamento de um sistema de identificação automática de placas de veículos. Este aperfeiçoamento está relacionado com a diminuição do tempo de execução necessário à localização e reconhecimento dos caracteres contidos nas placas dos veículos bem como a melhorar a taxa de sucesso no seu reconhecimento.  A primeira versão do sistema de identificação da placas de veículos descrito em [SOU2000], desenvolvido no CPG-EE da UFRGS, denominado SIAV 1.0, localiza e extrai 91,3% das placas corretamente mas apresenta uma taxa de reconhecimento das placas de 37,3%, assim como um tempo de processamento não satisfatório. Neste trabalho, cujo sistema desenvolvido é denominado SIAV 2.0, a imagem é previamente processada através da aplicação de técnicas de realce da imagem. O principal objetivo das técnicas de realce é processar a imagem de modo que o resultado seja mais apropriado para uma aplicação específica do que a imagem original [GON93]. O sistema busca melhorar a qualidade da imagem eliminando ou suavizando sombras e reflexos presentes na cena em virtude da iluminação não controlada. Visando um menor tempo de execução durante o tratamento e análise da imagem um estudo estatístico baseado na distribuição gaussiana foi realizado de maneira a restringir a área de análise a ser processada. O SIAV possui duas redes neurais como ferramentas de reconhecimento de caracteres. A partir da análise dos diferentes modelos de redes neurais empregados na atualidade, foi desenvolvida uma nova arquitetura de rede a ser utilizada pelo SIAV 2.0 que oferece uma taxa de reconhecimento superior a rede neural usada no SIAV 1.0. Visando um melhor tempo de execução, a implementação em hardware dedicado para este modelo é abordado. Os testes foram realizados com três bancos de imagens obtidas por câmeras diferentes, inclusive por dispositivo ""pardal"" comercial. Estes testes foram realizados para verificar a efetividade dos algoritmos aperfeiçoados.|http://hdl.handle.net/10183/2329
A geografia como possibilidade de instrumento interdisciplinar na escola ciclada em uma turma em processo de alfabetização|2002|Open Access|Dissertação|Geografia : Interdisciplinaridade;Geografia : Alfabetizacao;Meio ambiente|por|A experiência apresentada neste estudo atendeu a uma turma de vinte alunos do segundo ano do primeiro ciclo da Escola Municipal de Ensino Fundamental Presidente João B.M. Goulart, da Rede Municipal de Educação de Porto Alegre – RMEPA, localizada na periferia da Zona Norte da cidade. O estudo fundamenta-se na construção do conceito de cidade , surgido a partir da seguinte fala da comunidade: “cada um faz suas coisas, cada um cuida da sua vida.” Tal fala foi retirada da pesquisa sócio-antropológica realizada na comunidade escolar, que participa desta escola. A partir desta fala, criou-se um complexo temático, com o título: “Qualidade de vida.” Este originou o mapa conceitual, que sugeria o estudo do tema central meio ambiente, que foi desmembrado e fez-se a opção de estudar o meio. Qual meio? Que meio? Um meio conhecido mas não percebido como um todo: a cidade. Selecionadas as categorias que compõem o conceito de cidade, foi feita uma relação com os elementos que formam o conceito de identidade. E os dois estudos (cidade e identidade) seguiram em paralelo, onde é possível perceber a trama que há entre os mais diversos conceitos e conteúdos, propostos por todas as áreas de conhecimento. É importante destacar a intenção de proporcionar às crianças um ensino-aprendizagem onde os alunos teriam a oportunidade de pensar, expor estes pensamentos, analisar e sistematizar. Desta forma estariam incluídos, não só no sistema educacional, mas também no meio ao qual pertencem. Estariam, assim, atendendo a um anseio da autora, em oferecer aos seus educandos o mesmo tipo de ensino que procurou para seu filho. Um ensino que percebesse o aluno como um todo, e não em partes distintas. Faz-se necessário esclarecer que este estudo não teve a pretensão de acelerar o processo de alfabetização, mas apresentar a Geografia como possibilidade de instrumentalização interdisciplinar em uma turma de escola ciclada em processo de alfabetização.|http://hdl.handle.net/10183/2336
A dinâmica de uma família de aplicações unidimensionais|2002|Open Access|Dissertação|Sistemas dinâmicos;Evoluções dinâmicas unidimensionais;Teoria do ponto fixo;Famílias caóticas|por|Resumo não disponível.|http://hdl.handle.net/10183/2345
Filmes finos dielétricos para dispositivos microeletrônicos avançados|2003|Open Access|Tese|Filmes finos dieletricos;Silício;Alumínio;Óxidos;Tratamento térmico;Mosfet;Difração de raios X;Feixes de íons;Nitretação;Reações químicas;Recozimento|por|Apresentamos mecanismos de formação e de degradação térmica de filmes fi- nos (espessura da ordem de 10 nm) de diferentes dielétricos sobre substrato de silício monocristalino. Tendo em vista a aplicação dessas estruturas em MOSFETs (transistores de efeito de campo metal-óxido-semicondutor), estudamos o consagrado óxido de silício (SiO2), os atuais substitutos oxinitretos de silício (SiOxNy) e o possível substituto futuro óxido de alumínio (Al2O3). Nossos resultados experimentais baseiam-se em técnicas preparativas de substituição isotópica e de caracterização física com feixes de íons (análise com reações nucleares) ou raios- X (espectroscopia de fotoelétrons). Observamos que: (a) átomos de silício não apresentam difusão de longo alcance (além de ~ 2 nm) durante o crescimento de SiO2 por oxidação térmica do silício em O2; (b) nitretação hipertérmica é capaz de produzir filmes finos de oxinitreto de silício com até dez vezes mais nitrogênio que o resultante do processamento térmico usual, sendo que esse nitrogênio tende a se acumular na interface SiOxNy/Si; e (c) átomos de oxigênio, alumínio e silício migram e promovem reações químicas durante o recozimento térmico de estruturas Al2O3/SiO2/Si em presença de O2. Desenvolvemos um modelo de difusão-reação que poderá vir a permitir o estabelecimento de condições ótimas de processamento térmico para filmes finos de Al2O3 sobre silício a serem empregados na fabricação de MOSFETs.;We present mechanisms of formation and thermal degradation of thin films (thickness about 10 nm) of different dielectrics on monocrystalline silicon substrate. Having in sight the application of such structures in MOSFETs (metal-oxidesemiconductor field effect transistors), we studied the standard silicon oxide (SiO2 ), its current substitutes silicon oxynitrides (SiO xNy) and the possible future substitute aluminum oxide (Al 203 ). The experimental results in this thesis are based on preparation techniques involving isotopic substitution and on physical characterization with ion beams (nuclear reaction analysis) or X-rays (photoelectron spectroscopy). We have observed that: (a) silicon atoms do not present long range diffusion (more than 2 nm) during the growth of SiO 2 by thermal oxidation of silicon in 02 ; (b) hyperthermal nitridation can yield silicon oxynitride thin films with up to ten times more nitrogen than the resulting from conventional thermal processing, and this nitrogen tends to accumulate at the SiO xNy/Si interface (c) oxygen, aluminum, and silicon atoms migrate and promote chemical reactions during thermal annealing of Al203/Si02/Si structures in the presence of 02 . A diffusion-reaction model was developed based on these results. In the future, this model may lead to optimal thermal processing conditions for Al 203 films on silicon to be used in MOSFET fabrication.|http://hdl.handle.net/10183/2350
Otimização de geometrias aerodinâmicas utilizando métodos inversos|2003|Open Access|Dissertação|Otimização;Aerodinâmica;Cálculo numérico;Métodos de integração|por|O objetivo deste trabalho é a obtenção de uma técnica para a modelagem otimizada de corpos submetidos a fluxos de alta velocidade, como aerofólios em escoamentos transônicos e outras geometrias aerodinâmicas. A técnica é desenvolvida através de expansões em séries de Fourier para um conjunto de equações diferenciais com interrelação com as condições de contorno, sendo uma equação para a parte superior e outra para a parte inferior do aerofólio. O método de integração temporal empregado baseia-se no esquema explícito de Runge-Kutta de 5 estágios para as equações da quantidade de movimento e na relação de estado para a pressão. Para a aproximação espacial adota-se um esquema em volumes finitos no arranjo co-localizado em diferenças centrais. Utiliza-se dissipação artificial para amortecer as frequências de alta ordem do erro na solução das equações linearizadas. A obra apresenta a solução de escoamentos bi e tridimensionais de fluidos compressíveis transônicos em torno de perfis aerodinâmicos. Os testes num´ericos são realizados para as geometrias do NACA 0012 e 0009 e asas tridimensionais usando as equações de Euler, para número de Mach igual a 0.8 e ® = 0o. Os resultados encontrados comparam favoravelmente com os dados experimentais e numéricos disponíveis na literatura.|http://hdl.handle.net/10183/2375
Aplicando métodos de solução de problemas em tarefas de interpretação de rochas|2001|Open Access|Dissertação|Inteligencia artificial : Mineracao;Inteligencia artificial : Petrografia;Engenharia : Conhecimento;Sistemas baseados : Conhecimento|por|A Engenharia de Conhecimento (Knowledge Engineering - KE) atual considera o desenvolvimento de Sistemas Baseados em Conhecimento (Knowledge- Based Systems - KBSs) como um processo de modelagem baseado em modelos de conhecimento reusáveis. A noção de Métodos de Solução de Problemas (Problem- Solving Methods - PSMs) desempenha um importante papel neste cenário de pesquisa, pois representa o conhecimento inferencial de KBSs em um formalismo explícito. Não menos importante, PSMs também facilitam a compreensão do processo de raciocínio desenvolvido por humanos. PSMs são descritos em um formalismo abstrato e independente de implementação, facilitando a análise do conhecimento inferencial que muitas vezes é obscurecido em grandes bases de conhecimento. Desta forma, este trabalho discute a noção de PSMs, avaliando os problemas de pesquisa envolvidos no processo de desenvolvimento e especificação de um método, como também analisando as possibilidades de aplicação de PSMs. O trabalho apresenta a descrição e análise de um estudo de caso sobre o processo de desenvolvimento, especificação e aplicação de um PSM Interpretação de Rochas. As tarefas de interpretação de rochas são desenvolvidas por petrógrafos especialistas e correspondem a um importante passo na caracterização de rochasreservatório de petróleo e definição de técnicas de exploração, permitindo que companhias de petróleo reduzam custos de exploração normalmente muito elevados.  Para suportar o desenvolvimento de KBSs neste domínio de aplicação, foram desenvolvidos dois PSMs novos: o PSM Interpretação de Rochas e o PSM Interpretação de Ambientes Diagenéticos. Tais métodos foram especificados a partir de uma análise da perícia em Petrografia Sedimentar, como também a partir de modelos de conhecimento e dados desenvolvidos durante o projeto PetroGrapher. O PSM Interpretação de Rochas e o PSM Interpretação de Ambientes Diagenéticos são especificados conceitualmente em termos de competência, especificação operacional e requisitos/suposições. Tais definições detalham os componentes centrais de um esquema de raciocínio para interpretação de rochas. Este esquema é empregado como um modelo de compreensão e análise do processo de raciocínio requerido para orientar o desenvolvimento de uma arquitetura de raciocínio para interpretação de rochas. Esta arquitetura é descrita em termos de requisitos de armazenamento e manipulação de dados e conhecimento, permitindo projetar e construir um algoritmo de inferência simbólico para uma aplicação de bancos de dados inteligentes denominada PetroGrapher.|http://hdl.handle.net/10183/2399
Um Sistema para aprendizagem de demonstrações dedutivas em geometria euclidiana|2001|Open Access|Dissertação|Informática : Educação;Ensino : Geometria;Ensino : Matematica;Software educacional|por|O objetivo do presente trabalho é realizar a concepção de um sistema para a aprendizagem de demonstrações da Geometria Euclidiana Plana e a implementação de um protótipo deste sistema, denominado LEEG - Learning Environment on Euclidean Geometry, desenvolvido para validar as idéias utilizadas em sua especificação. Nos últimos anos, tem-se observado uma crescente evolução dos sistemas de ensino e aprendizagem informatizados. A preocupação com o desenvolvimento de ambientes cada vez mais eficientes, tanto do ponto de vista computacional quanto pedagógico, tem repercutido em um salto de qualidade dos software educacionais. Tais sistemas visam promover, auxiliar e motivar a aprendizagem das mais diversas áreas do conhecimento, utilizando técnicas de Inteligência Artificial para se aproximarem ao máximo do comportamento de um tutor humano que se adapte e atenda às necessidades de cada aluno. A Geometria pode ser vista sob dois aspectos principais: considerada como uma ciência que estuda as representações do plano e do espaço e considerada como uma estrutura lógica, onde a estrutura matemática é representada e tratada no mais alto nível de rigor e formalismo. Entretanto, o ensino da Geometria, nos últimos anos, abandonou quase que totalmente sua abordagem dedutiva. Demonstrações de teoremas geométricos não são mais trabalhadas na maioria das escolas brasileiras, o que repercute em um ensino falho da Matemática, que não valoriza o desenvolvimento de habilidades e competências relacionadas à experimentação, observação e percepção, realização de conjecturas, desenvolvimento de argumentações convincentes, entre outras.  Levando-se em conta este cenário, desenvolveu-se o LEEG, um sistema para a aprendizagem de demonstrações geométricas que tem como objetivo auxiliar um aprendiz humano na construção de demonstrações da Geometria Euclidiana Plana. O sistema foi modelado sobre uma adaptação do protocolo de aprendizagem MOSCA, desenvolvido para suportar ambientes de ensino informatizados, cuja aprendizagem é baseada na utilização de exemplos e contra-exemplos. Este protocolo propõe um ambiente de aprendizagem composto por cinco agentes, dentre os quais um deles é o aprendiz e os demais assumem papéis distintos e específicos que completam um quadro de ensino-aprendizagem consistente. A base de conhecimento do sistema, que guarda a estrutura lógica-dedutiva de todas as demonstrações que podem ser submetidas ao Aprendiz, foi implementada através do modelo de autômatos finitos com saída. A utilização de autômatos com saída na aplicação de modelagem de demonstrações dedutivas foi extremamente útil por permitir estruturar os diferentes raciocínios que levam da hipótese à tese da proposição de forma lógica, organizada e direta. As demonstrações oferecidas pelo sistema são as mesmas desenvolvidas por Euclides e referem-se aos Fundamentos da Geometria Plana. São demonstrações que priorizam e valorizam a utilização de objetos geométricos no seu desenvolvimento, fugindo das demonstrações que apelam para a simples manipulação algébrica e que não oferecem uma construção significativa do ponto de vista da Geometria. Porém, mesmo sendo consideradas apenas as demonstrações contidas em Elements, todos os diferentes raciocínios para uma mesma demonstração são aceitos pelo sistema, dando liberdade ao aprendiz no processo de construção da demonstração.|http://hdl.handle.net/10183/2414
Extensão de um modelo OO formal com aspectos temporais|2000|Open Access|Dissertação|Engenharia : Software;Especificacao formal;Orientacao : Objetos|por|Na área de Engenharia de Software, há vários modelos formais de especificação orientado a objetos (OO). Um destes é o OO-Method / OASIS. OO-Method se baseia nos seguintes princípios: - dar suporte às noções do modelo conceitual orientado a objetos; - integrar os modelos formais com metodologias de aceitação industrial; - possibilitar a produção de software avançado que inclua a geração completa de código (estática e dinâmica) do desenvolvimento comercial. O processo de desenvolvimento consiste em levantar as propriedades principais do sistema em desenvolvimento (modelo conceitual) por parte do engenheiro de software, e construir de forma automática, em qualquer momento (por um processo de conversão gráfico-textual) a especificação formal orientada a objetos em OASIS (Open and Active Specifications of Information System) que constituirá um repositório de alto nível do sistema. O objetivo de OASIS é expressar os requisitos funcionais de um sistema de informação, em um marco formal, que facilite sua validação e geração automática de programas. OASIS não inclui a especificação de aspectos temporais. A modelagem de aspectos temporais é um importante tópico da modelagem de sistemas de informação, porque através destes são representadas as características dinâmicas das aplicações e a interação temporal entre diferentes processos.  A especificação de requisitos de aplicações através de modelos orientados a objetos permite representar não só os seus estados, mas também, seu comportamento. Modelos temporais representam também a evolução de objetos com o tempo. Como o estado de um objeto pode ser alterado devido à ocorrência de um evento (fato ocorrido em um determinado instante no tempo), é importante que o modelo utilizado permita apresentar a história destes eventos. O presente trabalho tem por finalidade propor uma extensão temporal a um modelo formal de especificação OO. Esta extensão inclui tanto aspectos estáticos quanto dinâmicos. A extensão de aspectos estáticos estende OASIS com atributos temporais. A extensão dos aspectos dinâmicos, contribuição central do trabalho, estende OASIS com eventos temporais.|http://hdl.handle.net/10183/2432
Regras para transformação de esquemas conceituais definidos a partir de um framework de banco de dados geográficos para esquemas lógicos de SIG, com base no padrão SAIF|2001|Open Access|Dissertação|Geoinformática;Sistemas : Informacao geografica;Banco : Dados geograficos;Regras : Mapeamento;SAIF|por|O objetivo dedste trabalho é investigar o projeto de banco de dados aplicado a Sistemas de Informações Geográficas (SIG), mais especificamente, do mapeamento de esquemas conceituais, orientado a objetos, para esquemas lógicos implementados por softwares de SIG comerciais. A transformação dos esquemas conceituais para os lógicos é realizado através da idedntificação de um conjunto de regras genéricas de mapeamento de qualquer esquema concecitual de banco de dados geográficos, baseados em um framework conceitual, para os esquemas lógicos proprietários dos diversos SIG existentes. A concretização desta tarefa de transformação é possível mediante a proposta de um ambiente de suporte. Esse ambiente fornece uma estrutura específica, constituída por uma linguagem formal, definida pelo padrão SAIF (Saptial Archive and Interchange Format), pela ferramenta FME (feature Manipulation Engine) e pela ferramenta CASE Rational Rose v.2000e. O conjunto de regras genéricas elaborado neste trabalho, é composto por dois subconjuntos. O primeiro define regras de correspondência, determinando uma relação entre os conceitos da realidade percebidos pelo Framework conceitual e a linguagem formal apresentada pelo padrão SAIF. O segundo subconjunto define regras de transformação, onde busca-se mapear os conceitos do paradigma da orientação a objetos par aos conceitos relacionais utilizazdos pela maioria dos softwares de SIG, disponíveis no mercado. Com a finalidade de validar a aplicabilidadee deste conjunto de regras de mapeamento e do ambiente de suporte proposto, este trabalho inclui também a implementação de um protótipo, o qual executa a automatização da trasnformação dos esquemas conceituais para os esquemas lógicos de banco de dados geográfico.|http://hdl.handle.net/10183/2433
AvalWeb : sistema interativo para gerência de questões e aplicação de avaliações na web|2001|Open Access|Dissertação|Informática;Educação;Ensino a distância;Avaliacao pedagogica;World Wide Web (WWW);Hiperdocumento;Multimídia|por|Neste trabalho, é realizado um estudo dos processos de avaliação de alunos em ambiente WBT (Web Based Training). Para tanto, foram analisadas várias ferramentas de avaliação disponíveis no mercado. Com base nestas análises, foi proposto um sistema de gerência de questões e aplicação de avaliações, onde o termo “gerência” é utilizado com o objetivo de (i) atender requisições dos professores para elaboração de avaliações, (ii) escolher o nível de dificuldade das questões que comporão a avaliação e (iii) usar estratégias interativas para aplicação de provas, como por exemplo, a utilização de níveis de dificuldade progressivos das questões submetidas aos alunos, dependendo de suas respostas anteriores. Integrando o sistema de gerência de questões foi proposto um módulo de auto-avaliações, com retorno imediato para o aluno sobre qual é a resposta correta juntamente com uma explicação do professor, auxiliando no entendimento do estudante sobre a matéria ao invés de simplesmente atribuir determinada nota. Enfim, foi implementado um protótipo demonstrando a viabilidade das idéias presentes no modelo de avaliação aqui proposto. A proposta conceitual estabelecida para o modelo é bem mais ampla e flexível do que a atual versão da implementação realizada.|http://hdl.handle.net/10183/2468
Utilização de uma ferramenta independente do domínio para diagnóstico do comportamento do aluno em atividades de ensino a distância|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Inteligência artificial;Diagnóstico cognitivo|por|A evolução da Informática na Educação exige ambientes de ensino capazes de se adaptarem ao contexto de acordo com as características individuais do aluno, permitindo interatividade, e que gerem um diagnóstico do comportamento desse aluno. Com base nestes argumentos, o objetivo deste trabalho é propor um sistema de diagnóstico independente do domínio, capaz de analisar o comportamento do aluno em cursos de Ensino a Distância. O professor organiza o material em estruturas de tarefas TÆMS (uma linguagem independente do domínio para descrição de planos de resolução de tarefas), gerando uma biblioteca de planos que deverão ser executados pelo aluno. As informações referentes à navegação do aluno pelo material são gravadas em um log. O processo de diagnóstico ocorre através do confronto entre as informações do log e os planos gerados pelo professor (esta comparação é baseada em um modelo causal geral que pode ser utilizado para diagnosticar diferenças entre quaisquer estruturas TÆMS). Se forem detectadas divergências no processo de diagnóstico, o sistema gerará um arquivo texto contendo os sintomas detectados e as possíveis causas para que estes tenham ocorrido.|http://hdl.handle.net/10183/2471
Condução de experimentos de injeção de falhas em banco de dados distribuídos|2001|Open Access|Dissertação|Banco : Dados;Banco : Dados distribuidos;Injecao : Falhas;Tolerancia : Falhas;Gerencia : Banco : Dados|por|O presente trabalho realiza uma validação experimental, através da técnica de injeção de falhas por software, de sistemas de informações que utilizam gerenciadores de banco de dados distribuídos comerciais. Estes experimentos visam a obtenção de medidas da dependabilidade do SGBD utilizado, levantamento do custo de seus mecanismos de tolerância a falhas e a real aplicabilidade de SGBDs comerciais em sistemas de missão crítica. Procurou-se avaliar e validar as ferramentas de injeção de falhas utilizadas, no caso específico deste trabalho a ComFIRM e o FIDe. Inicialmente são introduzidos e reforçados os conceitos básicos sobre o tema, que serão utilizados no decorrer do trabalho. Em seguida são apresentadas algumas ferramentas de injeção de falhas em sistemas distribuídos, bem como os modelos de falhas em banco de dados distribuídos. São analisados alguns estudos de aplicação de ferramentas de injeção de falhas em bancos de dados distribuídos.  Concluída a revisão bibliográfica é apresentado o modelo de software e hardware que foi implementado, destacando o gerador de cargas de trabalho GerPro-TPC e o gerenciador de injeções e resultados GIR. O GerPro-TPC segue as especificações TPC-c para a simulação de um ambiente transacional comercial padrão e o GIR realiza a integração das ferramentas de injeção de falhas utilizadas, bem como a elaboração do cenário de falhas a injetar e a coleta dos resultados das falhas injetadas. Finalmente são descritos os experimentos realizados sobre o SGBD PROGRESS. São realizados 361 testes de injeções de falhas com aproximadamente 43.000 falhas injetadas em experimentos distintos. Utiliza-se dois modelos de falhas: um focado em falhas de comunicação e outro em falhas de hardware. Os erros resultantes das falhas injetadas foram classificados em erros ignorados/mascarados, erros leves, erros graves e erros catastróficos. Dos modelos de falhas utilizados as que mais comprometeram a dependabilidade do SGBD foram as falhas de hardware. As falhas de comunicação somente comprometeram a disponibilidade do sistema alvo.|http://hdl.handle.net/10183/2474
Filtragem de imagens com preservação das bordas usando a Transformada Wavelet|2002|Open Access|Tese|Computação gráfica;Processamento de imagens;Filtragem : Imagem|por|A filtragem de imagens visando a redução do ruído é uma tarefa muito importante em processamento de imagens, e encontra diversas aplicações. Para que a filtração seja eficiente, ela deve atenuar apenas o ruído na imagem, sem afetar estruturas importantes, como as bordas. Há na literatura uma grande variedade de técnicas propostas para filçtragem de imagens com preservação de bordas, com as mais variadas abordagens, deentrte as quais podem ser citadas a convolução com máscaras, modelos probabilísticos, redes neurais, minimização de funcionais e equações diferenciais parciais. A transformada wavelet é uma ferramenta matemática que permite a decomposição de sinais e imagens em múltiplas resoluções. Essa decomposição é chamada de representação em wavelets, e pode ser calculada atrravés de um algorítmo piramidal baseado em convoluções com filtros passa-bandas e passa-baixas. Com essa transformada, as bordas podem ser calculadas em múltiplas resoluções. Além disso, como filtros passa-baixas são utilizados na decomposição, a atenuação do ruído é um processo intrínseco à transformada. Várias técnicas baseadas na transformada wavelet têm sido propostas nos últimos anos, com resultados promissores. Essas técnicas exploram várias características da transformada wavelet, tais como a magnitude de coeficientes e sua evolução ao longo das escalas. Neste trabalho, essas características da transformada wavelet são exploradas para a obtenção de novas técnicas de filtragem com preservação das bordas.|http://hdl.handle.net/10183/2478
Estratigrafia de seqüências : uso da icnologia, dos argilominerais e da geoquímica em rochas de idade permiana na Bacia do Paraná, Região de Lauro Müller (SC), Brasil|1999|Open Access|Dissertação|Estratigrafia de seqüências : Paraná, Bacia do;Icnologia;Geoquímica;Argilominerais|por|A Icnologia, os Argilominerais e a Geoquímica foram integradas aos estudos de Associação Faciológica com objetivo de testá-las como ferramentas auxiliares na caracterização de Seqüências Deposicionais, segundo os conceitos da Estratigrafia de Seqüências (sentido Exxon). Para isso, foi selecionado o intervalo estratigráfico, compreendido do Sakmariano ao Kunguriano (Permiano), correspondente às formações Rio do Sul, Rio Bonito e base da Formação Palermo. A área de estudo está situada na borda leste da Bacia do Paraná, nos municípios de Orleans e Lauro Müller, região sul do estado de Santa Catarina. Como metodologia de trabalho, foi selecionado um arcabouço estratigráfico considerando uma hierarquia de eventos de 3a e 4a ordem, inseridos em um evento de 2a ordem, correspondente a uma superseqüência. Para isso, foram selecionados dez afloramentos, e os poços PB-18 e PB-20, com testemunhos, totalizando 500 m. Foram interpretadas seis associações faciológicas, representadas, da base para o topo, por rochas glácio-marinha, plataforma marinha dominada por ondas com estrutura “hummocky”, arenito de “shoreface” médio/superior, bioturbados, pelitos marinho/marinhos marginais, flúvio-estuarino e ilha de barreira/laguna. Admitiu-se ainda a formação de vales incisos para as região de Lauro Müller e para a área do poço RL-6, a partir da deposição das rochas flúvio-estuarinas.  O uso da Icnologia limitou-se à interface entre o topo da formação Rio do Sul e a base do Membro Triunfo da Formação Rio Bonito, onde a identificação da Icnofábrica de Glossifungites auxiliou na delimitação de limites de seqüência de alta freqüência. Além disso, foram reconhecidas, o predomínio das Icnofábricas de Thalassinoides e Teichichnus e, secundariamente, Planolites, Ophiomorpha, Arenicolites, Cylindrichnus, Monocraterion, Diplocraterion e Rosselia, permitindo posicionar a interface entre estas formações na Icnofácies Skolithos/Cruziana. Helminthopsis, Condrites e Palaeophycus passam a predominar quando da presença de subambientes mais restritos tipo baías e lagunas. Os argilominerais identificados foram a caolinita, a clorita, a ilita e o interestratificado ilita-esmectita (I-S) do tipo ordenado. O predomínio da clorita e da ilita, na Formação Rio do Sul e o da caolinita no Membro Siderópolis, indicam variações paleoclimáticas, onde, inicialmente, existiram condições mais frias e secas, passando para condições de clima mais quente e úmido. Os pelitos transgressivos do Membro Paraguaçu da Formação Rio Bonito foram caracterizados pelos valores relativos mais elevados do interestratificado I-S, e do SiO2 e Na2O e, os menores valores relativos do Al2O3, K2O, Fe2O3, MgO e TiO2. Interpretou-se uma proveniência detrítica para estes argilominerais sendo associados a minerais micáceos e feldspáticos. As relações V/(V+Ni) e V/Cr indicam condições paleoambientais restritas, de caráter redutor, praticamente para todo o intervalo estudado. A relação Sr/Ba mostrou-se boa indicadora de eventos transgressivos no PB-18 e no poço 1-TV-4-SC, perfurado em posição mais distal na bacia.  Embora o uso da Taphonomia não tenha sido contemplada no objetivo inicial, a utilização da razão pólens/esporos foi satisfatória. Pulsos transgressivos de alta freqüência puderam também ser balizados pelas razões mais elevadas desta relação, havendo uma correlação razoável com as indicações advindas da curva da razão Sr/Ba. Integrando-se os resultados destas várias ferramentas foi possível dividir o intervalo estratigráfico no PB-18, em sete seqüências deposicionais de 4a ordem e, em cinco seqüências deposicionais, no PB-20. As variações encontradas nos estilos de estaqueamento estratigráfico entre as áreas perfuradas por estes poços, deve-se às variações locais no estilo tectono-sedimentar, dentro do modelo de bacias tipo “foreland” – parte distal, modelo este adotado para a sedimentação destas seqüências e que tiveram, na subsidência e no controle glácio-eustático, os agentes moduladores deste padrão estratigráfico.|http://hdl.handle.net/10183/2480
Metalogenese dos depositos hidrotermais de metais-base e au do ciclo brasiliano no bloco São Gabriel, RS|1999|Open Access|Tese|Geoquímica;Metalogenese;Depositos hidrotermais;Geoquimica isotopica;Sulfetos de cobre|por|Os depósitos de metais-base (Camaquã, Santa Maria e na Formação Passo Feio), e Au (Bossoroca) mais importantes do Rio Grande do Sul foram gerados durante o Ciclo Brasiliano ao longo de três eventos distintos, relacionados ao magmatismo e metamorfismo contemporâneos (700, 594 e 562 Ma); os metais foram derivados de fontes relacionadas à crosta juvenil e ao embasamento antigo. O depósito de ouro da Bossoroca (700 Ma) consiste de veios de quartzo com Au e subordinadamente pirita, calcopirita, galena e teluretos, sendo classificado como um depósito orogênico epizonal. Os filões de quartzo aurífero são hospedados por rochas, piroclásticas calcico-alcalinas de composição andesítica, dacítica com basaltos e epiclásticas subordinadas, pertencentes a Formação Campestre. Estudos do zircão através do método U-Pb via SHRIMP mostram que as rochas vulcânicas encaixantes foram geradas a cerca de 760 Ma atrás, no início do Ciclo Brasiliano, e sofreram metamorfismo regional de baixa pressão na transição entre os facies xistos verdes e anfibolito a cerca de 700 Ma. Os depósitos de metais-base do sistema Camaquã Cu (Au, Ag) e Santa Maria Pb-Zn (Cu, Ag) são hidrotermais magmáticos distantes, provavelmente ligados à intrusões graníticas, e foram gerados há cerca de 594 Ma durante o magmatismo pós-colisional do final da Orogenêse Dom Feliciano.  As mineralizações de Cu (Au) e Pb hospedados pela Formação Passo Feio são hidrotermais epigenéticas e foram gerados há 562 Ma durante a intrusão do Granito Caçapava. A composição isotópica do Pb dos sulfetos dos depósitos de Camaquã-Santa Maria indica que os metais foram derivados de uma fonte crustal com Pb muito primitivo no final do Ciclo Brasiliano. A composição isotópica do enxofre dos sulfetos desses depósitos (~ 0‰ CDT) indica uma origem magmática para o enxofre. Os metais dos depósitos hidrotermais epigenéticos da Formação Passo Feio foram também derivados do embasamento antigo, com contribuição importante das rochas meta-vulcanosedimentares da Formação Passo Feio. O enxofre dessas mineralizações possui origem mista, originada pela mistura de fluidos magmáticos (Granito Caçapava) com enxofre derivado da lixiviação das meta-vulcanosedimentares da Formação Passo Feio. Os metais concentrados no depósito de Au da Bossoroca foram mobilizados durante o metamorfismo regional dinamotermal através da interação de fluidos de origem profunda que ascenderam através da pilha vulcanosedimentar extraindo Au e outros metais, e depositando-os em níveis crustais mais rasos em sítios estruturalmente favoráveis. A fonte do Pb determinada para o depósito de Au da Bossoroca é de origem profunda e corresponde a mesma fonte do magma de arco juvenil gerador das rochas vulcânicas do arco. Os isótopos de C-O mostram assinaturas compatíveis com uma fonte profunda para estes fluidos mineralizadores.|http://hdl.handle.net/10183/2486
Estudo da estabilidade termodinâmica de filmes ultrafinos de HfO/sub 2/ sobre Si|2002|Open Access|Dissertação|Filmes finos;Transporte atomico;Termodinâmica;Reacoes quimicas especificas;Argônio;Oxigênio;Deposição de vapor químico;Silício;Feixes de íons;Espectroscopia;Háfnio|por|O presente estudo relata a composição, transporte atômico, estabilidade termodinâmica e as reações químicas durante tratamentos térmicos em atmosfera de argônio e oxigênio, de filmes ultrafinos de HfO2 depositados pelo método de deposição química de organometálicos na fase vapor (MOCVD) sobre Si, contendo uma camada interfacial oxinitretada em NO (óxido nítrico). A caracterização foi realizada utilizando-se técnicas de análise por feixes de íons e espectroscopia de fotoelétrons excitados por raios-X (XPS). Também foram realizadas medidas elétricas sobre os filmes. Os estudos indicaram que esta estrutura é essencialmente estável aos tratamentos térmicos quando é feito um pré-tratamento térmico em atmosfera inerte de argônio, antes de tratamento em atmosfera reativa de oxigênio, exibindo uma maior resistência à incorporação de oxigênio do que quando foi diretamente exposta à atmosfera de oxigênio. Tal estabilidade é atribuída a um sinergismo entre as propriedades do sistema HfO2/Si e a barreira à difusão de oxigênio constituída pela camada interfacial oxinitretada. A composição química dos filmes após os tratamentos térmicos é bastante complexa, indicando que a interface entre o filme e o substrato tem uma composição do tipo HfSixOyNz. Foi observada a migração de Hf para dentro do substrato de Si, podendo esta ser a causa de degradação das características elétricas do filme.|http://hdl.handle.net/10183/2492
O gás ionizado em galáxias ativas|2002|Open Access|Tese|Galáxias ativas;Galaxias seyfert;Gás;Propriedades físicas;Densidade;Massa;Fotoionizacao;Espalhamento;Luminosidade;Radiação;Fontes de infravermelho;Hubble space telescope|por|Foram analisados espectros óticos de fenda longa de 29 galáxias que hospedam núcleos ativos (AGNs), sendo 6 galáxias Seyfert 1, 18 galáxias Seyfert 2, 4 Rádio-galáxias de linhas estreitas (NLRG) e 1 Rádio-galáxia de linhas largas (BLRG). Estas galáxias apresentam emissão por gás de alta excitação que se estende em alguns casos a 10 kpc do núcleo. O objetivo do presente trabalho consiste em estudar e caracterizar as propriedades físicas da região estendida de linhas estreitas (ENLR) destes objetos bem como propriedades da fonte central. A distribuição radial de parâmetros que caracterizam o gás emissor em cada galáxia, tais como brilho superficial das linhas de emissão, densidade do gás, massa, extinção e excitação são obtidos. Estes valores característicos são comparados entre as diferentes classes de atividade nuclear presentes em nossa amostra, bem como às propriedades de galáxias normais com o mesmo tipo de Hubble quando possível. Nós encontramos que a massa de gás ionizado é consistente com a hipótese de que o gás é “originado” na fotoionização pela fonte central das nuvens de HI préexistentes na galáxia hospedeira. Os valores observados das razões entre as linhas estreitas de emissão são comparados com os obtidos através de modelos de fotoionização gerados com o código MAPPINGS Ic, obtendo os parâmetros do modelo – densidade, índice espectral da distribuição de energia e da metalicidade do gás – que melhor reproduzem as observações.  Observamos que a variação da abundância química do gás é necessária para explicar o espalhamento nos valores observados. Adicionalmente, comparamos os valores observados com os obtidos com modelos de choques gerados por Dopita & Sutherland. Investigamos também a influência da emissão proveniente de regiões HII ao espectro observado – a qual concluímos ser importante particularmente nas regiões emissoras mais distantes do que 2 kpc do núcleo. Nós determinamos a luminosidade ionizante da fonte central nos AGNs usando a aproximação de que as nuvens de gás são limitadas por radiação, e obtivemos os correspondentes valores para o fator de cobertura do gás. Esta luminosidade ionizante foi então comparada com a luminosidade observada em raios-X na banda 2 –10 keV, através de aproximações para a distribuição espectral de energia (SED). Para 9 galáxias Seyfert 2 a luminosidade observada está disponível, e verificamos que nosso método recupera a luminosidade do AGN em raios-X – assim como obtida dos dados do satélite ASCA – bem como identifica os 3 casos Compton espessos. Por fim, investigamos a natureza do contínuo infravermelho (IR) médio e distante – comparandose a luminosidade observada no IR, calculada a partir dos fluxos IRAS, com a luminosidade predita para um toro que envolve a fonte central absorvendo a radiação incidente e re-emitindo esta no infravermelho. Encontramos que a luminosidade observada no IR é consistente com a luminosidade predita para o torus.|http://hdl.handle.net/10183/2493
O contínuo e a região estendida de linhas estreitas das galáxias ativas ESO 362-G18 e ESO 362-G8|1997|Open Access|Dissertação|Galáxias;Poeira cosmica;Populacoes estelares;Fotoionizacao;Fotons;Luminosidade;Radiação;Gás;Astronomia infravermelha|por|Foram analisados espectros óticos de fenda longa das galáxias Seyfert 1 ESO362-G18 e Seyfert 2 ESO362-G8. Estas duas galáxias apresentam emissão por gás de alta excitação em forma anisotrópica, possivelmente devido a colimação por um tóro de poeira. O objetivo do presente trabalho consiste em estudar o contínuo e a região estendida de linhas de emissão (ENLR) destes objetos. Este trabalho constitui-se no primeiro estudo detalhado realizado sobre estas galáxias. A população estelar em cada galáxia e caracterizada em função da distância ao núcleo. O contínuo nuclear e estudado em termos de duas componentes: a população estelar e a componente AGN (\Active Galatic Nuclei""). Observa-se a presença desta componente AGN na gal axia Seyfert 1 ESO362-G18. A galáxia Seyfert 2 não permite observar esta componente AGN mesmo depois de subtraída a componente de população estelar. Nas duas galáxias observa-se importante contribuição de população de idade intermediária ( 5 108 anos).  A partir das linhas estreitas de emissão, mapeamos a excitação do gás interestelar ao longo da ENLR. As razões entre estas linhas de emissão são reproduzidas a partir de um modelo de fotoionização de duas componentes - uma componente limitada por matéria (""matter-bounded"") e uma componente limitada por ionização (""ionization-bounded""). Este modelo, construído utilizando o código de fotoionização MAPPINGS Ic, se propõe a resolver os problemas apresentados pelos modelos tradicionais de uma componente apenas. A partir dos resultados obtidos com o modelo de duas componentes, determinamos o fator de preenchimento e o fator de cobertura do gás em função da distância ao núcleo. Utilizamos estes parâmetros para testar a consistência do modelo proposto. Por fim realizase o cálculo de balanço de fótons e estuda-se a natureza do contínuo infravermelho (IR) médio e distante - comparando-se a luminosidade observada no IR, calculada a partir dos fluxos IRAS, com a luminosidade predita para um tóro que envolve a fonte modelada e re-emite a radiação incidente no infravermelho.|http://hdl.handle.net/10183/2494
Fases de stripes nos cupratos : um estudo do modelo t-J anisotrópico|2003|Open Access|Dissertação|Supercondutores de alta temperatura;Modelo t-j;Ondas de spin;Aproximação de Born de onda distorcida;Quase-particulas;Análise numérica;Ressonancia antiferromagnetica;Anisotropia magnética;Fermions|por|Neste trabalho realizamos um estudo de um buraco em um antiferromagneto, como parte de uma revisão de diferentes técnicas de abordagem das fases de “stripes” nos cupratos supercondutores. Estudamos a transição do formalismo de “strings” para um buraco no modelo t - Jz bidimensional, onde existe uma solução analítica, para a solução de pólaron de spin no modelo t - J isotrópico através da aproximação de Born auto-consistente. A forma funcional dos picos de quase-partícula, do peso espectral e do “gap” espectral foi investigada numericamente em detalhe, em função da anisotropia magnética. O movimento de um pólaron de spin na presença de uma parede de domínio antiferromagnética (ADW) em antifase, como uma realização da configuração de “stripes” nos planos CuO dos cupratos de baixa dopagem, também foi analisada.|http://hdl.handle.net/10183/2495
Uma Solução de escalonamento para o DPC++|2002|Open Access|Dissertação|Arquitetura de computadores;Processamento distribuído;Escalonamento : Processos;Dpc++;Objetos distribuidos|por|Este trabalho descreve uma implementação de um modelo de escalonamento para a linguagem de programação DPC++. Esta linguagem, desenvolvida no Instituto de Informática da UFRGS, possibilita que uma aplicação orientada a objetos seja distribuída entre vários processadores através de objetos distribuídos. Muito mais que uma simples biblioteca de comunicação, o DPC ++ torna a troca de mensagens totalmente transparente aos objetos. A integração do DPC++ com o DECK, também em desenvolvimento, trará grandes inovações ao DPC++, principalmente pelo uso de theads. O escalonador proposto para este modelo utiliza estes recursos para implantar os chamados processos espiões, que monitoram a carga de uma máquina, enviando seus resultados ao escalonador. O escalonador implementado possui, desta forma, dois módulos: objetos espiões implementados como um serviço do DECK e o escalonador propriamente dito, incluído no objeto Diretório, parte integrante do DPC++.|http://hdl.handle.net/10183/2501
Processamento de materiais carbonáceos por pulsos intensos de laser em alta pressão|2002|Open Access|Tese|Safira;Filmes finos;Laser;Geometria;Cobre;Evaporação;Condutividade térmica;Espectroscopia Raman;Carbono;Altas temperaturas|por|Neste trabalho foi desenvolvida uma câmara de alta pressão com janela de safira para processamento de filmes finos com pulsos de laser de alta potência, num regime de resfriamento ultra-rápido e geometria confinada. As amostras estudadas consistiram de filmes finos de carbono amorfo depositados sobre substratos de cobre. Os processamentos foram realizados com um laser pulsado Nd:YAG com energia de até 500 mJ por pulso, com duração de 8 ns, focalizada numa região de cerca de 1,5mm2, gerando uma região de elevada temperatura na superfície da amostra durante um intervalo de tempo bastante curto, da ordem do tempo de duração do pulso do laser. Para evitar a evaporação do filme de carbono, aplicava-se através da câmara, uma pressão de 0,5 a 1,0 GPa, confinando a amostra e eliminando o efeito da ablação. Este sistema tornou possível produzir taxas de resfriamento extremamente elevadas, com supressão da formação de uma pluma durante a incidência do laser, sendo o calor dissipado rapidamente pelo contato com os substratos de cobre e safira, ambos com elevada condutividade térmica. As amostras processadas foram analisadas por micro-espectroscopia Raman e os resultados revelaram a formação de estruturas com cadeias lineares de carbono, “carbynes”, caracterizadas pela presença de um pico Raman intenso na região de 2150 cm-1.  Outro conjunto de picos Raman foi observado em 996 cm-1, 1116 cm-1 e 1498 cm-1 quando o filme fino de carbono amorfo foi processado dentro da câmara, com uma seqüência de mais de três pulsos consecutivos de laser. Várias tentativas foram feitas para investigar a natureza da fase que origina estes picos. Apesar da similaridade com o espectro Raman correspondente ao poliacetileno (CnHn), não foi possível constatar evidências experimentais sobre a presença de hidrogênio nos filmes de carbono processados. Estes picos Raman não foram observados quando o filme de carbono era depositado sobre outros substratos metálicos, a não ser em cobre. O conjunto de resultados experimentais obtidos indica que estes picos estariam relacionados a pequenos aglomerados lineares de átomos de carbono, diluídos numa matriz de átomos de cobre, formados durante os pulsos subseqüentes de laser e retidos durante o resfriamento ultra-rápido da amostra. A comparação dos resultados experimentais com a simulação do espectro Raman para diferentes configurações, permite propor que estes aglomerados seriam pequenas cadeias lineares, com poucos átomos, estabilizadas frente à formação de grafenos pela presença de átomos de cobre em abundância. Foram também realizados processamentos de materiais carbonáceos por pulsos de laser em meios líquidos, através de câmaras especialmente construídas para este fim. Os resultados, em diversos materiais e configurações, mostraram apenas a formação de estruturas grafíticas, sem evidência de outras fases.|http://hdl.handle.net/10183/2515
Solução de sistemas de equações algébrico-diferenciais ordinárias de índice superior|2002|Open Access|Dissertação|Equações algébrico-diferenciais;Redução de índices;Modelagem matemática;Integração numérica;Código DASSL|por|A modelagem matemática de problemas importantes e significativos da engenharia, física e ciências sociais pode ser formulada por um conjunto misto de equações diferenciais e algébricas (EADs). Este conjunto misto de equações deve ser previamente caracterizado quanto a resolubilidade, índice diferencial e condições iniciais, para que seja possível utilizar um código computacional para resolvê-lo numericamente. Sabendo-se que o índice diferencial é o parâmetro mais importante para caracterizar um sistema de EADs, neste trabalho aplica-se a redução de índice através da teoria de grafos, proposta por Pantelides (1988). Este processo de redução de índice é realizado numericamente através do algoritmo DAGRAFO, que transforma um sistema de índice superior para um sistema reduzido de índice 0 ou 1. Após esta etapa é necessário fornecer um conjunto de condições inicias consistentes para iniciar o código numérico de integração, DASSLC. No presente trabalho discute-se três técnicas para a inicialização consistente e integração numérica de sistemas de EADs de índice superior. A primeira técnica trabalha exclusivamente com o sistema reduzido, a segunda com o sistema reduzido e as restrições adicionais que surgem após a redução do índice introduzindo variáveis de restrição, e a terceira técnica trabalha com o sistema reduzido e as derivadas das variáveis de restrição.  Após vários testes, conclui-se que a primeira e terceira técnica podem gerar um conjunto solução mesmo quando recebem condições iniciais inconsistentes. Para a primeira técnica, esta característica decorre do fato que no sistema reduzido algumas restrições, muitas vezes com significado físico importante, podem ser perdidas quando as equações algébricas são diferenciadas. Trabalhando com o sistema reduzido e as derivadas das variáveis de restrição, o erro da inicialização é absorvido pelas variáveis de restrição, mascarando a precisão do código numérico. A segunda técnica adotada não tem como absorver os erros da inicialização pelas variáveis de restrição, desta forma, quando as restrições adicionais não são satisfeitas, não é gerada solução alguma. Entretanto, ao aplicar condições iniciais consistentes para todas as técnicas, conclui-se que o sistema reduzido com as derivadas das variáveis restrição é o método mais conveniente, pois apresenta melhor desempenho computacional, inclusive quando a matriz jacobiana do sistema apresenta problema de mau condicionamento, e garante que todas as restrições que compõem o sistema original estejam presentes no sistema reduzido.|http://hdl.handle.net/10183/2526
Modos em vigas com secção transversal de variação linear|2002|Open Access|Dissertação|Método espectral;Vigas;Modelagem Euler-Bernoulli;Equações de Bessel;Software simbólico MAPLEV5|por|O objetivo principal deste trabalho é a obtenção dos modos e as freqüências naturais de vigas de variação linear e em forma de cunha, com condições de contorno clássicas e não-clássicas, descritas pelo modelo estrutural de Euler-Bernoulli. A forma dos modos foi determinado com o uso das funções cilíndricas. No caso forçado se considera uma força harmônica e se resolve o problema pelo método espectral, utuilizando o software simbólico Maple V5. Realiza-se uma análise comparativa dos resultados obtidos com os resultados existentes na literatura para vigas uniformes.|http://hdl.handle.net/10183/2532
Derivações de ordem superior em anéis primos e semiprimos|2000|Open Access|Tese|Anéis não comutativos : Derivação de ordem superior : Algebra não comutativa|por|Nesta tese estudamos as derivações de ordem superior (DOS) em anéis não-comutativos. Inicialmente, mostramos que toda derivação tripla de Jordan de ordem superior em um anel semiprimo livre de 2-torção é uma DOS. Em particular, toda derivação de Jordan de ordem superior (DJOS) num anel deste tipo é uma DOS. Estendemos também o resultado a ideais de Lie U, provando que se R é um anel primo livre de 2-torção e D é uma DJOS de U em R onde U ct Z(R) é tal que U2E U para todo u E U, então D é uma DOS de U em R. Nestas condições, se U C Z(R), então o resultado não é válido. Estudamos ainda as DOS cujas componentes satisfazem relações de dependência linear sobre R ou Q (o anel de quocientes à direita de M artindale de R). Caracterizamos tais DOS, e mostramos que as relações de dependência linear são preservadas ao estendermos uma DOS de R a Q.;In this thesis we study the higher order derivations (sho rtly, DOS) in noncommutative rings. Initially, we show that every higher order Jordan triple derivation on a 2-torsion free semiprime ring is a DOS. In particular, every higher order Jordan derivation (DJOS) in a ring of this type is a DOS. We also extend the result to Lie ideais U, proving that if R is a 2-torsion free prime ring and D is a DJOS of U into R where U ct Z(R) (the center of R) is such that U2E U for all u E U, then D is a DOS of U into R. With these conditions, if U C Z(R), then the result is no more true. We also study the DOS whose components satisfy relationships of linear dependence on R or Q (the Martindale ring of right quocients of R). We characterize such DOS and we show that the relationships of linear dependence are preserved if we extend a DOS of R to Q.|http://hdl.handle.net/10183/2562
Distribuição de extinção na Pequena Nuvem de Magalhães|1997|Open Access|Dissertação|Astrofisica extragalatica;Pequena Nuvem de Magalhães;Espectroscopia;Catalogos astronomicos|por|O estudo da distribuição da extinção na direção da Pequena Nuvem de Magalhães (PNM) é feito atravéss da contagem de galáxias de ”fundo”, e espectroscopicamente pela comparação de espectros nucleares de uma amostra das mesmas com os de galáxias de referência de similar população estelar. O método de contagens é baseado em um novo catálogo realizado no presente trabalho, contendo 3037 galáxias estendendo-se até a magnitude limite B¼ 20 em 6 placas do ESO/SERC na região da PNM e seus arredores. O método espectroscópico foi aplicado a uma amostra de 16 galáxias na mesma região, assim como numa outra de 27 galáxias na direção da Grande Nuvem de Magalhães, para comparação. A deficiência de galáxias indicada pelo método de contagens sugere E(B-V)=0.35 nas partes centrais da PNM, e E(B-V)=0.15 a 6º do centro. Por outro lado o método espectroscópico indica que a PNM é basicamente transparente. Sugerem-se as seguintes explicações para esta diferença: (i) a deficiência de galáxias nas regiões centrais detectadas pelo método de contagens é signifivativamente afetada pela alta concentração de estrelas e objetos extendidos pertencentes à PNM; (ii) a amostra espectroscópica conteria tipicamente galáxias em zonas menos avermelhadas, o que indicaria que a absorção ocorre em nuvens de poeira com uma distribuição preferencialmente discreta. A aplicação do m´etodo espectroscópico na GNM também sugere a presença de nuvens de poeira discretas nas suas regiões centrais.|http://hdl.handle.net/10183/2567
Síntese, análise e caracterização do filme polimérico da eletrorredução do furfural sobre platina em acetonitrila|2003|Open Access|Tese|Eletrodo de platina;Furfural : Polimerização eletroquímica;Voltametria ciclica|por|O presente trabalho apresenta um estudo sistemático para obter um filme polimérico a partir da eletrooxidação e eletrorredução do furfural. O filme foi crescido sobre a superfície do eletrodo usando-se três técnicas eletroquímicas: voltametria cíclica (VC); a cronoamperometria e a cronopotenciometria. Estas técnicas foram usadas em ambos os processos. Inicialmente foi testada a eletrooxidação e polimerização do furfural sobre o eletrodo de platina brilhante em solução aquosa de ácido sulfúrico 0,50 mol L-1. A influência de algumas variáveis como a concentração de furfural e o tempo de polarização foram estudadas. Os resultados confirmam a formação de um filme fino sobre a superfície do eletrodo, entretanto com uma aderência ruim. Este efeito foi atribuído à solubilidade do filme em meio aquoso. Devido a isto, realizou-se experimentos em meio não aquoso, usando acetonitrila como solvente. O eletrodo de trabalho usado neste caso foi a platina platinizada, este foi usado com o objetivo de se aumentar a superfície ativa do eletrodo e o eletrólito suporte usado foi o cloreto de tetrametilamônio 0,1 mol L-1. Neste caso a polarização anódica foi realizada pelas três técnicas com objetivo de melhorar a aderência do filme sobre a superfície do eletrodo. Entretanto a qualidade do filme diminui com o aumento da concentração de água presente no meio.  Uma nova estratégia foi adotada, entretanto, envolvendo a formação de um filme por eletrorredução do furfural, sobre a superfície do eletrodo de platina, em acetonitrila e cloreto de lítio como eletrólito suporte. Os resultados confirmam um filme polimérico poroso e espesso, sobre a superfície do eletrodo usando as técnicas eletroquímicas. A aderência deste filme foi melhor se comparada com a do filme obtido anteriormente. O filme foi caracterizado por medidas eletroquímicas, por Ressonância Magnética Nuclear, por espectroscopia de Ultra-violeta, por espectroscopia de Infra-vermelho, Cromatografia de permeação em gel e, por DSC. Os dados obtidos permitiram estabelecer uma proposta de mecanismo que inclui a presença da acetonitrila na formação do filme polimérico obtido sobre a superfície do eletrodo de platina.|http://hdl.handle.net/10183/2568
Síntese e análise estrutural de novos ciclopaladatos e síntese de N-heterociclos a partir de ciclopaladatos e alenos|2002|Open Access|Tese|Complexos ciclopaladatos;N-heterociclos : Síntese|por|Os compostos de paládio vêm apresentado uma vasta linha de aplicação, tanto como catalisadores como precursores em reações de síntese orgânica. Dentre esses compostos, os ciclopaladatos, que são compostos cíclicos com uma ligação Pd-heteroátomo, permite a formação de novas estruturas cíclicas contendo algum heteroátomo, como nitrogênio, oxigênio ou enxofre. Neste trabalho foram sintetizadas aminas propargílicas capazes de se coordenar a sais de paládio, formando novos ciclopaladatos através da reação de cloropaladação. Esses compostos se encontram na forma de dímeros e podem apresentar-se como diferentes isômeros. Estudos espectroscópicos, tais como RMN de 1H, 13C e raios-X de monocristais foram realizados para a elucidação estrutural desses novos compostos. Além dos isômeros geométricos clássicos (cisóide e transóide) foram observados pela primeira vez a formação de atropoisômeros. Esses ciclopaladatos, contendo nitrogênio ligado ao paládio, foram testados frente a alenos diferentemente substituídos, mostrando que ocorre a inserção do aleno na ligação Pd-C e, seguido da depaladação, ocorre a formação de novos compostos heterocíclicos a seis membros.  Alguns ciclopaladatos, quando em solução, podem apresentar certa instabilidade, ocorrendo a decomposição do ciclopaladato com a regeneração do alcino precursor do respectivo ciclopaladato. Assim, estudou-se a reação de decomposição de diferentes ciclopaladatos, chamada de retrocloropaladação, utilizando a técnica de RMN de 1H em diferentes intervalos de tempo.;Palladium compounds have application in different fields, such as catalysis or as precursors in organic synthesis reactions. Among these compounds, the palladacycles, which are cyclic compounds with a Pdheteroatom bond, allow the formation of new cyclic structures with at least one heteroatom such as nitrogen, oxygen or sulfur. In this work, propargylic amines were synthesized, which are capable to coordinate with palladium salts, affording new palladacycles by chloropalladation reactions. These compounds are dimers and can be present as different isomers. Spectroscopic analyses, such as 1H, 13C NMR and X-Ray of monocrystals were performed in order to elucidate the structure of these new compounds. Besides the classic geometric isomers (cisoid and transoid) for the first time the formation of atropisomers was observed. This palladacycles, with nitrogen bonded to palladium, were tested with different substituted allenes and showed the insertion of allene into the Pd-C bond. Subsequent depalladation resulted in the formation of new six member heterocycles. Some palladacycles are not stable in solution and undergo decomposition reactions yielding the alkyne precursors of palladacycle.  We have studied these decomposition reactions of different palladacycles, called retro-chloropalladation, using 1H NMR analysis at different time intervals.|http://hdl.handle.net/10183/2570
Líquidos iônicos em catálise bifásica: hidroformilação de alfa-olefinas superiores|2003|Open Access|Tese|Líquidos iônicos;Catálise bifásica;Olefinas : Hidroformilação|por|O uso estratégico de sistemas bifásicos para preservação das vantagens da catálise homogênea tem se mostrado uma alternativa tecnológica interessante, já que se aliam altas atividades e seletividades, com a possibilidade de recuperação/re-utilização do sistema catalítico. O sistema catalítico Rh(acac)(CO)2/Sulfoxantphos imobilizado no líquido iônico hidrofóbico 1-n-butil-3-metilimidazólio (BMI.PF6) promove a reação de hidroformilação de olefinas pesadas com seletividades em aldeídos superiores a 98%. A regiosseletividade deste sistema é fortemente dependente da natureza do líquido iônico e da fase móvel envolvidos na reação. Estudos sobre transferência de massa gás/líquido também foram realizados e demonstraram que CO é mais solúvel do que H2 no BMI.PF6, na ordem de duas vezes mais, nas condições reacionais utilizadas. A formação do complexo de catalítico em BMI.PF6 foi monitorada por infravermelho (HPIR) e ressonância magnética nuclear (HPNMR) in situ. As mesmas espécies catalíticas ee e ea-(difosfina)Rh(CO)2H observadas em solventes orgânicos foram formadas no BMI.PF6. O uso de HPIR sob condições de hidroformilação mostrou que o equilíbrio dinâmico ee:ea segue um comportamento semelhante ao observado em meio homogêneo onde solventes orgânicos clássicos, como o tolueno, são utilizados.|http://hdl.handle.net/10183/2571
Novos modelos para cálculo de energia livre de solvatação em simulações de dinâmica molecular|2003|Open Access|Tese|Dinâmica molecular : Líquidos;Solvatação;Simulação computacional|por|Combinando métodos computacionais da eletrostática e dinâmica molecular, este trabalho teve como objetivo descrever processos de solvatação em sistemas quimicamente importantes. Foram obtidas propriedades termodinâmicas necessárias para o entendimento do processo de solvatação. Estão descritos e testados modelos para descrever a interação soluto-solvente, possibilitando, assim, aprimorar a descrição físico-química dos processos de solvatação. Utilizaram-se programas desenvolvidos em nosso grupo e programas comerciais que permitem os cálculos de dinâmica molecular e química quântica. Uma nova abordagem para o cálculo de energia livre de solvatação foi desenvolvida proporcionando a obtenção acurada e eficiente dessa propriedade, dentro do enfoque da dinâmica molecular. Nessa nova abordagem, novas metodologias para a geração de cavidades moleculares foram propostas e avaliadas. As energias livres de solvatação obtidas estão em boa concordância com os valores experimentais.|http://hdl.handle.net/10183/2572
Simulação numérica de esteiras em transição utilizando o método dos contornos virtuais|2002|Open Access|Dissertação|Simulação numérica;Métodos de contornos virtuais;Geometrias complexas;Simulações de escoamento;Método espectral|por|Simulações Numéricas são executadas em um código numérico de alta precisão resolvendo as equações de Navier-Stokes e da continuidade para regimes de escoamento incompressíveis num contexto da turbulência bidimensional. Este código utiliza um esquema compacto de diferenças finitas de sexta ordem na aproximação das derivadas espaciais. As derivadas temporais são calculadas usando o esquema de Runge-Kuta de terceeira ordem com baixo armazenamento. Tal código numérico fornece uma representação melhorada para uma grande faixa de escalas de comprimento e de tempo. As técnicas dos contornos imersos acopladas ao método dos contornos virtuais permitem modelar escoamentos não-estacionários sobre geometrrias complexas, usando simplesmente uma malha Cartesiana uniforme. Por meio de procedimentos de aproximação/interpolação, as técnicas dos contornos imersos (aproximação Gaussiana, interpolação bilinear e redistribuição Gaussiana), permitem a representação do corpo sólido no interior do campo de escoamento, com a superfície não coincidindo com a malha computacional. O método dos contornos virtuais, proposto originalmente por Peskin, consiste, basicamente, na imposição na superfície e/ou no interior do corpo, de um termo de força temporal acrescentando às equações do momento. A aplicação deste campo de força local leva o fluido ao repouso na superfície do corpo, permitindo obter as condições de contorno de não-deslizamento e de não penetração de fluido na parede. A análise das oscilações induzidas no escoamento-contorno pelo processo de desprendimento de vórtices na esteira do cilindro circular e de geometria retangulares na incidência, para números de Reybolds variando de 40 a 400, confirma a eficiência computacional e a aplicabilidade das técncias implementadas.|http://hdl.handle.net/10183/2587
Estudo da perícia em petrografia sedimentar e sua importância para a engenharia de conhecimento|2001|Open Access|Tese|Geoinformática;Aquisicao : Conhecimento;Engenharia : Conhecimento;Modelagem : Conhecimento|por|Perícia é a capacidade de aplicar habilidades intelectuais para resolver problemas em domínios estratégicos, com um desempenho e qualidade de solução superior à média dos profissionais da área. Ampliar a compreensão do que é a perícia fornece suporte e justificativas para a proposição de novos recursos para aquisição e modelagem de conhecimento na área da Engenharia de Conhecimento. Esta tese apresenta os resultados de um estudo sobre a perícia em Geologia, em especial numa aplicação em Petrografia Sedimentar. A tarefa em questão é especialmente significativa porque, ao contrário das tarefas típicas, cujo estudo tem levado ao desenvolvimento de diversas metodologias de aquisição de conhecimento, essa tarefa aplica primariamente raciocínio baseado na análise de imagens e, secundariamente, busca e métodos analíticos para interpretar os objetos da perícia (no caso, rochas-reservatório de petróleo). O objetivo deste projeto de tese é a identificação dos recursos cognitivos aplicados por especialistas na solução de problemas, que são essencialmente de reconhecimento visual e a representação do que é reconhecido. A interpretação dessas habilidades fornece fundamentos para a proposta de novos recursos para aquisição e modelagem, e posterior desenvolvimento de sistemas especialistas para interpretação de rochas. Também contribuem para o tratamento da perícia em outros campos que possuam o mesmo caráter de reconhecimento visual tal como a interpretação de rochas.  O estudo foi desenvolvido em duas fases. Na primeira, o conhecimento foi eliciado de um especialista em Petrografia Sedimentar e estruturado, utilizando técnicas tradicionais de aquisição de conhecimento. A segunda fase envolveu o desenvolvimento de um experimento com dezenove geólogos com diferentes níveis de perícia, para identificar os tipos de conhecimentos que suportam a perícia e quais os métodos de solução que são aplicados nos altos níveis da perícia. O estudo das habilidades cognitivas demonstrou que especialistas em Petrografia sedimentar desenvolvem uma grande variedade de formas mentais e hierarquias que diferem daquelas normalmente descritas na literatura da área. Especialistas retém ainda um grande conjunto de abstrações simbólicas de imagens, denominados aqui de pacotes visuais. Os pacotes visuais possuem importante papel na indexação das estruturas mentais e na condução do processo de inferência. As representações são tipicamente associadas com seus próprios métodos de solução de problemas adequados à complexidade da tarefa de caracterização de reservatórios. A aplicação desses recursos faz parte do conjunto de conhecimentos tácitos dos especialistas.  A associação de grafos de conhecimento e a análise de casos mostrou-se, neste trabalho, um método adequado para a externalizar e adquirir o conhecimento declarativo e as relações causais, as quais não são evidenciadas com as técnicas de aquisição de conhecimento tradicionais. Métodos de solução de problemas, por sua vez, foram eliciados com o auxílio das bibliotecas de solução de problemas disponíveis na literatura e grafos de conhecimento. O modelo de representação, aqui proposto, expressa o conhecimento em dois níveis: o nível da externalização, compatível com o conhecimento de um intermediário em Petrografia Sedimentar, e o nível da inferência, que modela o conhecimento tácito do especialista. Esta tese apresenta de forma inédita o conceito de pacote visual como uma primitiva de representação e um conjunto de métodos de solução de problemas adequados à interpretação de rochas.|http://hdl.handle.net/10183/2588
Protocolo de recuperação por retorno, coordenado, não determinístico|2002|Open Access|Tese|Confiabilidade : Computadores;Tolerancia : Falhas;Sistemas distribuídos;Recuperacao : Processos|por|O uso da recuperação de processos para obter sistemas computacionais tolerantes a falhas não é um assunto novo. Entretanto, a discussão de algoritmos para a recuperação em sistemas distribuídos, notadamente aqueles que se enquadram na categoria assíncrona, ainda encontra pontos em aberto. Este é o contexto do presente trabalho. Este trabalho apresenta um novo algoritmo de recuperação por retorno, em sistemas distribuídos. O algoritmo proposto é do tipo coordenado, e seus mecanismos componentes determinam que seja classificado como um algoritmo baseado em índices (index-based coordinated). Desta forma, a tolerância a falhas é obtida através do estabelecimento de linhas de recuperação, o que possibilita um retorno consideravelmente rápido, em caso de falha. Seu desenvolvimento foi feito com o objetivo de minimizar o impacto ao desempenho do sistema, tanto quando este estiver operando livre de falhas como quando ocorrerem as falhas. Além disso, os mecanismos componentes do algoritmo foram escolhidos visando facilitar a futura tarefa de implementação. A satisfação dos objetivos decorre principalmente de uma importante característica assegurada pelos mecanismos propostos no algoritmo: o não bloqueio da aplicação, enquanto é estabelecida uma nova linha de recuperação. Esta característica, associada ao rápido retorno, oferece uma solução promissora, em termos de eficiência, para a recuperação, um vez que o impacto no desempenho tende a ser reduzido, quando o sistema encontra-se operando em ambas condições: livre de erros ou sob falha.  Diferentemente da maioria dos algoritmos coordenados encontrados na literatura, o algoritmo proposto neste trabalho trata as mensagens perdidas. A partir da análise das características das aplicações, bem como dos canais de comunicação, quando estes interagem com o algoritmo de recuperação, concluiu-se que os procedimentos usados para recuperação de processos devem prever o tratamento desta categoria de mensagens. Assim, o algoritmo proposto foi incrementado com um mecanismo para tratamento das mensagens que têm o potencial de tornarem-se perdidas, em caso de retorno, ou seja, evita a existência de mensagens perdidas. Uma das decisões tomadas durante o desenvolvimento do algoritmo foi a de permitir um processamento não determinístico. Na realidade, esta escolha visou o aumento do espectro das falhas que poderiam ser tratadas pela recuperação. Tradicionalmente, a recuperação por retorno é empregada para tolerar falhas temporárias. Entretanto, a diversidade de ambiente, freqüente nos SDs, também pode ser usada para tolerar algumas falhas permanentes. Para verificar a correção do algoritmo, decidiu-se empregar um formalismo existente. Assim, a lógica temporal de Lamport (TLA) foi usada na especificação dos mecanismos do algoritmo bem como em sua demonstração de correção. O tratamento referente às mensagens perdidas, atrav´es do uso de mensagens de resposta, associado com o uso de uma lógica temporal, levou à necessidade de rever os critérios de consistência. Esta revisão gerou um conjunto de fórmulas de consistência ajustadas à existência de mensagens de diferentes classes: mensagens da aplicação e mensagens de resposta.|http://hdl.handle.net/10183/2590
Um assistente de feedback para o serviço de filtragem do software direto|2002|Open Access|Dissertação|Armazenamento : Dados;Recuperacao : Informacao;Software livre;Serviço : Filtragem;Perfil : Usuario|por|Este trabalho descreve a especificação e implementação do protótipo Assistente de Feedback que ajuda os usuários a ajustarem os parâmetros do serviço de filtragem de mensagens vindas do correio eletrônico de sistemas como o Direto. O Assistente de Feedback é instalado no computador do usuário do Direto para monitorar suas preferências representadas pelas ações aplicadas nas mensagens do correio eletrônico. O trabalho apresenta, ainda, uma revisão bibliográfica sobre os conceitos gerais de probabilidades, redes Bayesianas e classificadores. Procura-se descrever as características gerais dos classificadores, em especial o Naive Bayes, sua lógica e seu desempenho comparado a outros classificadores. São abordados, também, conceitos relacionados ao modelo de perfil de usuário e o ambiente Direto. O Naive Bayes torna-se atraente para ser utilizado no Assistente de Feedback por apresentar bom desempenho sobre os demais classificadores e por ser eficiente na predição, quando os atributos são independentes entre si.  O Assistente de Feedback utiliza um classificador Naive Bayes para predizer as preferências por intermédio das ações do usuário. Utiliza, também, pesos que representarão a satisfação do usuário para os termos extraídos do corpo da mensagem. Esses pesos são associados às ações do usuário para estimar os termos mais interessantes e menos interessantes, pelo valor de suas médias finais. Quando o usuário desejar alterar os filtros de mensagens do Direto, ele solicita ao Assistente de Feedback sugestões para possíveis exclusões dos termos menos interessantes e as possíveis inclusões dos termos mais interessantes. O protótipo é testado utilizando dois métodos de avaliação para medir o grau de precisão e o desempenho do Assistente de Feedback. Os resultados obtidos na avaliação de precisão apresentam valores satisfatórios, considerando o uso de cinco classes pelo classificador do Assistente de Feedback. Os resultados dos testes de desempenho permitem observar que, se forem utilizadas máquinas com configurações mais atualizadas, os usuários conseguirão receber sugestões com tempo de respostas mais toleráveis.|http://hdl.handle.net/10183/2596
Operadores aritméticos de baixo consumo para arquiteturas de circuitos DSP|2002|Open Access|Tese|Microeletrônica;Circuitos digitais;Consumo : Potencia|por|Este trabalho tem como foco a aplicação de técnicas de otimização de potência no alto nível de abstração para circuitos CMOS, e em particular no nível arquitetural e de transferência de registrados (Register Transfer Leve - RTL). Diferentes arquiteturas para projetos especificos de algorítmos de filtros FIR e transformada rápida de Fourier (FFT) são implementadas e comparadas. O objetivo é estabelecer uma metodologia de projeto para baixa potência neste nível de abstração. As técnicas de redução de potência abordadas tem por obetivo a redução da atividade de chaveamento através das técnicas de exploração arquitetural e codificação de dados. Um dos métodos de baixa potência que tem sido largamente utilizado é a codificação de dados para a redução da atividade de chaveamento em barramentos. Em nosso trabalho, é investigado o processo de codificação dos sinais para a obtenção de módulos aritméticos eficientes em termos de potência que operam diretamente com esses códigos. O objetivo não consiste somente na redução da atividade de chavemanto nos barramentos de dados mas também a minimização da complexidade da lógica combinacional dos módulos.  Nos algorítmos de filtros FIR e FFT, a representação dos números em complemento de 2 é a forma mais utilizada para codificação de operandos com sinal. Neste trabalho, apresenta-se uma nova arquitetura para operações com sinal que mantém a mesma regularidade um multiplicador array convencional. Essa arquitetura pode operar com números na base 2m, o que permite a redução do número de linhas de produtos parciais, tendo-se desta forma, ganhos significativos em desempenho e redução de potência. A estratégia proposta apresenta resultados significativamente melhores em relação ao estado da arte. A flexibilidade da arquitetura proposta permite a construção de multiplicadores com diferentes valores de m. Dada a natureza dos algoritmos de filtro FIR e FFT, que envolvem o produto de dados por apropriados coeficientes, procura-se explorar o ordenamento ótimo destes coeficientes nos sentido de minimizar o consumo de potência das arquiteturas implementadas.|http://hdl.handle.net/10183/2597
Algoritmos para o posicionamento de células em circuitos VLSI|2002|Open Access|Dissertação|Microeletrônica;Cad : Microeletronica;Algoritmos : Posicionamento;Sintese fisica|por|Este trabalho faz uma análise ampla sobre os algoritmos de posicionamento. Diversos são extraídos da literatura e de publicações recentes de posicionamento. Eles foram implementados para uma comparação mais precisa. Novos métodos são propostos, com resultados promissores. A maior parte dos algoritmos, ao contrário do que costuma encontrar-se na literatura, é explicada com detalhes de implementação, de forma que não fiquem questões em aberto. Isto só possível pela forte base de implementação por trás deste texto. O algorítmo de Fidduccia Mateyeses, por exemplo, é um algorítmo complexo e por isto foi explicado com detalhes de implementação. Assim como uma revisão de técnicas conhecidas e publicadas, este trabalho oferece algumas inovações no fluxo de posicionamento. Propõe-se um novo algorítimo para posicionamento inicial, bem como uma variação inédita do Cluster Growth que mostrta ótimos resultados. É apresentada uma série de evoluções ao algorítmo de Simulated Annealling: cálculo automático de temperatura inicial, funções de perturbação gulosas (direcionadas a força), combinação de funções de perturbação atingindo melhores resultados (em torno de 20%), otimização no cálculo de tamanho dos fios (avaliação das redes modificadas e aproveitamento de cálculos anteriores, com ganhos em torno de 45%). Todas estas modificações propiciam uma maior velocidade e convergência do método de Simulated Annealling.  É mostrado que os algorítmos construtivos (incluindo o posicionador do Tropic, baseado em quadratura com Terminal Propagation) apresentam um resultado pior que o Simulated Annealling em termos de qualidade de posicionamento às custas de um longo tempo de CPD. Porém, o uso de técnicas propostas neste trabalho, em conjunto com outras técnicas propostas em outros trabalhos (como o trabalho de Lixin Su) podem acelerar o SA, de forma que a relação qualidade/tempo aumente.|http://hdl.handle.net/10183/2598
Estudo e implementação da programação genética para síntese de fala|2002|Open Access|Dissertação|Linguística computacional;Síntese : Fala;Programacao genetica;Sistemas evolutivos|por|Este trabalho descreve a aplicação da Programação Genética, uma técnica de Computação Evolucionária, ao problema da Síntese de Fala automática. A Programação Genética utiliza as técnicas da evolução humana para descobrir programas bem adaptados a um problema específico. Estes programas, compostos de instruções, variáveis, constantes e outros elementos que compõe uma linguagem de programação, são evoluídos ao longo de um conjunto de gerações. A Síntese de Fala, consiste na geração automática das formas de ondas sonoras a partir de um texto escrito. Uma das atividades mais importantes, é realizada através da conversão de palavras e letras para os sons da fala elementares (fonemas). Muitos sistemas de síntese são implementados através de regras fixas, escritas por programadores humanos. Um dos mais conhecidos sistemas de síntese é o FESTIVAL, desenvolvido pela Universidade de Edimburgh, usando a linguagem de programação funcional LISP e um número fixo de regras.  Neste trabalho, nós exploramos a possibilidade da aplicação do paradigma da Programação Genética, para evoluir automaticamente regras que serão adotadas para implementação do idioma Português na ferramenta FESTIVAL, desenvolvido no projeto SPOLTECH (CNPq – NSF cooperação entre UFRGS e Universidade do Colorado). A modelagem do problema, consiste na definição das regras de pronúncia do Português Brasileiro, que a implementação do sistema FESTIVAL pronuncia erradamente, já que o mesmo foi implementado primariamente para o idioma Inglês. A partir destas regras, o sistema de Programação Genética, desenvolvido neste trabalho, evolui programas que constituem boas soluções para a conversão de letras para fonemas. A descrição dos resultados obtidos, cobre detalhes sobre a evolução das soluções, complexidade e regras implementadas, representadas pelas soluções mais bem adaptadas; mostrando que a Programação Genética, apesar de ser complexa, é bastante promissora.|http://hdl.handle.net/10183/2599
SADA sistema virtual de ensino com análise de desempenho e aplicação de avaliação na Web|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Internet;Avaliacao pedagogica|por|Este trabalho apresenta um sistema virtual para suporte ao ensino, denominado SADA. O sistema se baseia na análise do desempenho durante a aprendizagem para implementar técnicas de adaptação para apresentação dos links. Tendo como objetivo principal auxiliar o ensino presencial, e caracterizar as possibilidades para se adaptar a navegação no conteúdo do curso através da forma de apresentação dos links. O SADA utiliza ferramentas computacionais de domínio público na Web, e possui um conjunto de opções para criação e divulgação de cursos com conteúdos didáticos, e questões de múltipla escolha para avaliação do desempenho do aluno. Um sistema de ensino como o SADA, mediado por computador e baseado na Internet (Web), pode ser uma ferramenta complementar ao trabalho do professor, por acrescentar maior dinâmica aos cursos ou disciplinas devido às novas formas de interação com o aluno.|http://hdl.handle.net/10183/2600
Aplicação de XML para estruturação de ambientes de controle acadêmico baseado em ontologias|2002|Open Access|Dissertação|Armazenamento : Dados;Internet;Recuperacao : Informacao;XML (Linguagem de marcação);Informatica : Controle academico|por|Nos últimos anos, um grande esforço tem sido despendido no estudo de formas de representar documentos textuais, chamados semi-estruturados, objetivando extrair informações destes documentos com a mesma eficiência com que essas são extraídas de bancos de dados relacionais e orientados a objetos. A pesquisa, em dados semi-estruturados, tornou-se fundamental com o crescimento da Web como fonte e repositório de dados, uma vez que os métodos de pesquisa existentes, baseados em navegação e busca por palavra-chave, mostraram-se insuficientes para satisfazer as necessidades de consulta em escala cada vez maior. Com o surgimento da XML, e a tendência de se tornar a linguagem padrão na Web, fez com que a representação de informações fosse dirigida para este novo padrão, porque disponibiliza um bom intercâmbio de informações e a produção de documentos eletrônicos. Existe a necessidade de se disponibilizar os documentos e as informações na rede em formato Web, HTML ou XML. Sendo assim, consultar documentos XML representa um desafio para a comunidade de pesquisa em banco de dados, pois implica em disponibilizar os grandes volumes de dados já existentes em formato XML, surgindo a necessidade de ferramentas de consulta que sejam ao mesmo tempo, flexíveis o suficiente para compreender a heterogeneidade dos documentos e, poderosas ao ponto de extraírem informações com rapidez e correção.  Este trabalho apresenta uma abordagem sobre a linguagem XML, sua importância, vantagens, principais aplicações e as linguagens de consulta para dados XML. Após, é detalhada uma aplicação para Web utilizando a tecnologia XML, baseado em Ontologias, e a sua disponibilização na Web. A aplicação desenvolvida utilizou XML e linguagens de consulta para XML e com suporte a XML, baseando-se em Ontologias, com o objetivo de permitir consultas e o armazenamento de informações referentes aos alunos concluintes de determinados cursos da Universidade da Região da Campanha - Urcamp/Bagé, colocando à disposição da Universidade uma nova ferramenta, que disponibiliza informações referentes aos cursos em questão, utilizando uma nova tecnologia, que tende a tornar-se padrão na Web.|http://hdl.handle.net/10183/2601
Mecanismos para interoperação de backbones MPLS e redes que utilizem outras arquiteturas de QoS|2002|Open Access|Dissertação|Redes : Computadores;Engenharia : Trafego;MPLS;Qualidade : Servico|por|Acredita-se que no futuro as redes de telecomunicação e dados serão integradas em uma só rede, baseada na comutação de pacotes IP. Esta rede deverá oferecer serviços com qualidade (QoS) para as aplicações atuais e futuras. Uma das tecnologias que deverá ser adotada no núcleo desta nova rede é MPLS. MPLS introduz o conceito de switching (comutação) no ambiente IP e também permite que seja implementada a Engenharia de Tráfego, otimizando sua utilização através do roteamento baseado em restrições. Junto com MPLS outras arquiteturas para fornecimento de QoS, como Serviços Integrados e Serviços Diferenciados, serão utilizadas. Entretanto, como nenhuma delas atende a todos os requisitos para garantia de QoS fim a fim e levando-se em consideração o fato de a Internet ser uma rede heterogênea, surge a necessidade de um framework que permita a interoperabilidade das diferentes arquiteturas existentes. Neste trabalho é proposto um modelo de integração que fornece garantias de QoS fim a fim para redes que utilizam tanto Serviços Integrados como Serviços Diferenciados através do emprego de uma infra-estrutura baseada em MPLS e Serviços Diferenciados. A aplicabilidade do modelo foi testada no simulador ns2 e os resultados são apresentados neste trabalho.|http://hdl.handle.net/10183/2620
Um modelo de videoconferência para computador pessoal orientado ao perfil de aplicação|2002|Open Access|Dissertação|Redes;Computadores;Videoconferência;Multimídia|por|A informática vem adquirindo papéis cada vez mais importantes na vida cotidiana. Um dos papéis mais significativos, hoje, é o suporte a comunicações; atualmente, é muito difícil pensar em comunicações – mesmo interpessoais – sem fazer associação às áreas de informática e redes. Dentre as aplicações que utilizam informática e redes como suporte, a tecnologia de videoconferência tem recebido papel de destaque. Os avanços na tecnologia de redes e conectividade, aliados à padronização e à crescente oferta de produtos de videoconferência, têm aumentado a aplicabilidade e a popularidade destes produtos, sobretudo utilizados sobre arquitetura de redes TCP/IP. Trata-se de uma tecnologia atraente em termos de resultado, por agregar, além do áudio – recurso comum há muito tempo como suporte à comunicação – os recursos de vídeo e aplicações integradas (como quadro-branco compartilhado, Chat, troca de arquivos e outros). Contudo, essas aplicações são bastante exigentes, tanto em termos de banda quanto de qualidade de serviço (QoS) da rede. O primeiro item se justifica pelo volume de dados gerados pelas aplicações de videoconferência; o segundo, pela significativa influência que os problemas de qualidade da infraestrutura de rede (como elevada latência, jitter e descartes) podem exercer sobre tais aplicações.  A busca para as soluções destes problemas não é tarefa simples, pois muitas vezes envolve investimentos que desencorajam a adoção da tecnologia de videoconferência – principalmente para uso pessoal ou por empresas pequenas. Este trabalho propõe uma solução aos problemas mencionados, visando proporcionar uma melhor aceitação e maior disseminação da tecnologia de videoconferência, valendo-se de recursos com pouca demanda de investimento. A estratégia abordada é a adaptação de tráfego, com um enfoque diferenciado: o de levar em conta, para cada aplicação, o comportamento que o processo de adaptação apresentasse. A partir dessa orientação, é proposto um modelo de adaptação de tráfego orientado ao perfil da aplicação, voltado ao interesse do usuário, e que disponibilize uma forma ao mesmo tempo simples e eficiente para que o usuário realize a adequação do mecanismo de adaptação do sistema às suas necessidades e expectativas. A partir desta proposta, foi implementado um protótipo de aplicação, com o objetivo de verificar a funcionalidade do modelo em termos práticos. As observações dos resultados dos testes, bem como as conclusões geradas, serviram como validação da proposta.|http://hdl.handle.net/10183/2621
Uma proposta de arquitetura de linha de produto para sistemas de gerenciamento de workflow|2002|Open Access|Dissertação|Engenharia : Software;Reutilizacao : Software;Sistemas : Workflow|por|A tecnologia de workflow vem apresentando um grande crescimento nos últimos anos. Os Workflow Management Systems (WfMS) ou Sistemas de Gerenciamento de Workflow oferecem uma abordagem sistemática para uniformizar, automatizar e gerenciar os processos de negócios. Esta tecnologia requer técnicas de engenharia de software que facilitem a construção desse tipo de sistema. Há muito vem se formando uma consciência em engenharia de software de que para a obtenção de produtos com alta qualidade e que sejam economicamente viáveis torna-se necessário um conjunto sistemático de processos, técnicas e ferramentas. A reutilização está entre as técnicas mais relevantes desse conjunto. Parte-se do princípio que, reutilizando partes bem especificadas, desenvolvidas e testadas, pode-se construir software em menor tempo e com maior confiabilidade. Muitas técnicas que favorecem a reutilização têm sido propostas ao longo dos últimos anos. Entre estas técnicas estão: engenharia de domínio, frameworks, padrões, arquitetura de software e desenvolvimento baseado em componentes. Porém, o que falta nesse contexto é uma maneira sistemática e previsível de realizar a reutilização. Assim, o enfoque de linha de produto de software surge como uma proposta sistemática de desenvolvimento de software, baseada em uma família de produtos que compartilham um conjunto gerenciado de características entre seus principais artefatos. Estes artefatos incluem uma arquitetura base e um conjunto de componentes comuns para preencher esta arquitetura. O projeto de uma arquitetura para uma família de produtos deve considerar as semelhanças e variabilidades entre os produtos desta família.  Esta dissertação apresenta uma proposta de arquitetura de linha de produto para sistemas de gerenciamento de workflow. Esta arquitetura pode ser usada para facilitar o processo de produção de diferentes sistemas de gerenciamento de workflow que possuem características comuns, mas que também possuam aspectos diferentes de acordo com as necessidades da indústria. O desenvolvimento da arquitetura proposta tomou como base a arquitetura genérica e o modelo de referência da Workflow Management Coalition (WfMC) e o padrão de arquitetura Process Manager desenvolvido no contexto do projeto ExPSEE1. O processo de desenvolvimento da arquitetura seguiu o processo sugerido pelo Catalysis com algumas modificações para representar variabilidade. A arquitetura proposta foi descrita e simulada através da ADL (Architecture Description Language) Rapide. A principal contribuição deste trabalho é uma arquitetura de linha de produto para sistemas de gerenciamento de workflow. Pode-se destacar também contribuições para uma proposta de sistematização de um processo de desenvolvimento de arquitetura de linha de produto e também um melhor entendimento dos conceitos e abordagens relacionados à prática de linha de produto, uma vez que esta tecnologia é recente e vem sendo largamente aplicada nas empresas.|http://hdl.handle.net/10183/2626
Uma proposta de arquitetura de um ambiente de desenvolvimento de software distribuído baseada em agentes|2002|Open Access|Dissertação|Engenharia : Software;Sistemas distribuídos;Desenvolvimento : Software;Agentes : Software|por|A crescente complexidade das aplicações, a contínua evolução tecnológica e o uso cada vez mais disseminado de redes de computadores têm impulsionado os estudos referentes ao desenvolvimento de sistemas distribuídos. Como estes sistemas não podem ser facilmente desenvolvidos com tecnologias de software tradicionais por causa dos limites destas em lidar com aspectos relacionados, por exemplo, à distribuição e interoperabilidade, a tecnologia baseada em agentes parece ser uma resposta promissora para facilitar o desenvolvimento desses sistemas, pois ela foi planejada para suportar estes aspectos, dentre outros. Portanto, é necessário também que a arquitetura dos ambientes de desenvolvimento de software (ADS) evolua para suportar novas metodologias de desenvolvimento que ofereçam o suporte necessário à construção de softwares complexos, podendo também estar integrada a outras tecnologias como a de agentes. Baseada nesse contexto, essa dissertação tem por objetivo apresentar a especificação de uma arquitetura de um ADS distribuído baseada em agentes (DiSEN – Distributed Software Engineering Environment). Esse ambiente deverá fornecer suporte ao desenvolvimento de software distribuído, podendo estar em locais geograficamente distintos e também os desenvolvedores envolvidos poderão estar trabalhando de forma cooperativa.  Na arquitetura proposta podem ser identificadas as seguintes camadas: dinâmica, que será responsável pelo gerenciamento da (re)configuração do ambiente em tempo de execução; aplicação, que terá, entre os elementos constituintes, a MDSODI (Metodologia para Desenvolvimento de Software Distribuído), que leva em consideração algumas características identificadas em sistemas distribuídos, já nas fases iniciais do projeto e o repositório para armazenamento dos dados necessários ao ambiente; e, infra-estrutura, que proverá suporte às tarefas de nomeação, persistência e concorrência e incorporará o canal de comunicação. Para validar o ambiente será realizada uma simulação da comunicação que pode ser necessária entre as partes constituintes do DiSEN, por meio da elaboração de diagramas de use case e de seqüência, conforme a notação MDSODI. Assim, as principais contribuições desse trabalho são: (i) especificação da arquitetura de um ADS distribuído que poderá estar distribuído geograficamente; incorporará a MDSODI; proporcionará desenvolvimento distribuído; possuirá atividades executadas por agentes; (ii) os agentes identificados para o DiSEN deverão ser desenvolvidos obedecendo ao padrão FIPA (Foundation for Intelligent Physical Agents); (iii) a identificação de um elemento que irá oferecer apoio ao trabalho cooperativo, permitindo a integração de profissionais, agentes e artefatos.|http://hdl.handle.net/10183/2627
Um monitor de transações de serviços internet|2002|Open Access|Dissertação|Redes : Computadores;Seguranca : Redes : Computadores|por|Monitorar significa, de forma genérica, acompanhar e avaliar dados fornecidos por aparelhagem técnica. Quando se fala em monitoramento de uma rede, não se está fugindo desta idéia. Para monitorar a rede são utilizados mecanismos para coletar dados da mesma, sendo estes dados posteriormente avaliados. O monitoramento da rede é, sob o ponto de vista da administração da mesma, uma atividade indispensável. Através desta operação é possível obter conclusões sobre a “saúde” da rede. A busca e análise dos dados da rede podem ser feitas com vários enfoques, cada um buscando cercar uma situação específica, onde entre outros, destacam-se a segurança e a carga da rede. A proposta de fazer uso de algum recurso que permita monitorar a rede fica cada vez mais importante, à medida que as redes têm crescido em tamanho e importância para as organizações. Atualmente, é comum se falar em redes locais com centenas e até milhares de computadores conectados. Associada a esta realidade existe ainda a conexão com a Internet, que faz com que o número de máquinas em contato, suba para valores gigantescos. Os usuários de computador que estão conectados a uma rede, podem estar, fisicamente, muito longe dos olhos do administrador da mesma. Com isso, este sente-se obrigado a utilizar ferramentas que permita monitorar a rede, uma vez que não tem controle sobre os usuários.  Sob o ponto de vista da segurança, a preocupação está em verificar a possível ocorrência de ataques ou detectar problemas nas configurações dos mecanismos de segurança implementados. Já quanto à carga da rede, o enfoque é monitorar os tipos de acessos e serviços utilizados, a fim de identificar atividades supérfluas que possam estar sobrecarregando a rede. O presente trabalho tem por objetivo estudar meios para construir uma ferramenta que permita verificar, de forma on-line, as conexões TCP/IP que estão ativas na rede local, seja uma conexão entre duas máquinas da rede local, ou com a Internet, possibilitando visualizar os serviços que estão sendo acessados e a quantidade de tráfego gerada pelos computadores. Ao final será construído um protótipo a fim de validar o estudo feito. O estudo parte da análise do padrão de rede Ethernet, que é ambiente a ser utilizado neste estudo. Na seqüência serão estudadas as características dos principais protocolos da família TCP/IP, que é o conjunto de protocolo utilizado pela grande maioria das redes, inclusive pela maior delas, que é a Internet. Em uma fase posterior, serão estudadas as formas de se fazer o monitoramento em uma rede Ethernet e as ferramentas de monitoramento existentes. Na seqüência, os detalhes do protótipo para monitorar conexões TCP/IP são apresentados bem como os resultados dos testes de validação do mesmo.|http://hdl.handle.net/10183/2628
Modelagem de articulações para humanos virtuais baseada em anatomia|2001|Open Access|Dissertação|Computação gráfica;Corpo humano;Processamento de imagens|por|Técnicas de Processamento de Imagens e de Computação Gráfica vêm sendo empregadas há bastante tempo para o diagnóstico por imagens em Medicina. Mais recentemente, aplicações baseadas em modelos anatômicos, tanto extraídos de volumes de imagens como criados com base em estudos de anatomia, despontam com força. Tais modelos visam suportar simulação de movimento e de fisiologia. Porém, para que isso se torne realidade, modelos anatômicos do corpo humano precisam ser construídos e aperfeiçoados. Entre outras funcionalidades, esses modelos devem ser capazes de representar o movimento articulado do corpo humano. O problema de modelagem das articulações já foi considerado em diversos trabalhos, principalmente em Robótica e Animação. Entretanto, esses trabalhos não levaram em conta fidelidade anatômica com profundidade suficiente para que pudessem ser utilizados em aplicações de Medicina. O principal objetivo deste trabalho, portanto, é a criação de uma estratégia de representação de articulações embasada em características anatômicas para modelagem de esqueletos humanos virtuais.  Um estudo da anatomia do esqueleto humano é apresentado, destacando os tipos de articulações humanas e aspectos do seu movimento. Também é apresentado um estudo dos modelos de articulações encontrados na literatura de Computação Gráfica, e são comentados alguns sistemas de software comercial que implementam corpos articulados. Com base nesses dois estudos, procurou-se identificar as deficiências dos modelos existentes em termos de fidelidade anatômica e, a partir disso, propor uma estratégia de representação para articulações humanas que permitisse a construção de corpos humanos virtuais anatomicamente realísticos. O modelo de articulações proposto foi projetado com o auxílio de técnicas de projeto orientado a objetos e implementado no âmbito do projeto Virtual Patients. Usando as classes do modelo, foi construído um simulador de movimentos, que recebe a descrição de um corpo articulado através de um arquivo em formato XML e apresenta uma animação desse corpo. A descrição do movimento também é especificada no mesmo arquivo. Esse simulador foi utilizado para gerar resultados para verificar a correção e fidelidade do modelo articular. Para isso, um joelho virtual foi construído, seus movimentos foram simulados e comparados com outros joelhos: o modelo de outro simulador, um modelo plástico anatômico e o joelho real.|http://hdl.handle.net/10183/2629
FlexGroup: um ambiente flexível para comunicação em grupo|1999|Open Access|Dissertação|Sistemas operacionais;Sistemas distribuídos;Tolerancia : Falhas;Difusao confiavel|por|Mecanismos de comunicação entre processos são fundamentais no desenvolvimento de sistemas distribuídos, já que constituem o único meio de compartilhar dados entre processos que não dispõem de memória comum. Um dos principais mecanismos de comunicação utilizados é a troca de mensagens entre os processos componentes do sistema. Existem muitas aplicações que são compostas por um conjunto de processos que cooperam para realizar uma determinada tarefa e que são mais facilmente construídas se o sistema operacional oferecer a possibilidade de se enviar uma mensagem a diversos destinos. Neste caso são necessários mecanismos que permitam a difusão confiável de uma mensagem para um grupo de processos em uma única operação. Tendo em vista esta necessidade, diversos protocolos têm sido apresentados na literatura para permitir a comunicação entre um grupo de processos com diferentes graus de complexidade e de desempenho.  Este trabalho apresenta um ambiente para desenvolvimento e utilização de protocolos de comunicação em grupo, denominado FlexGroup. O ambiente divide os protocolos em suas características fundamentais, permitindo que estas características possam ser desenvolvidas separadamente como subprotocolos. Os subprotocolo são interligados através de uma interface comum e gerenciados pelo núcleo do ambiente. A comunicação entre as diversas máquinas da rede é gerenciada pelo FlexGroup, permitindo que o desenvolvedor de um novo subprotocolo possa somente se focar nas características específicas do seu protocolo. Esta modularidade permite, ainda, que apenas as partes de interesse de um novo protocolo precisem ser implementadas, além de também viabilizar a criação de um protocolo baseado nos já existentes no ambiente. Além disso, o ambiente permite que as aplicações de comunicação em grupo possam definir, através de uma biblioteca, o conjunto de subprotocolos que desejam utilizar, em tempo de execução, sem necessidade de conhecer a implementação interna dos subprotocolos.. Da mesma forma, alguém que se proponha a realizar comparações com os protocolos existentes, pode utilizar os diversos subprotocolos e as aplicações existentes, bastando alterar os protocolos utilizados em tempo de execução e avaliando somente as características que deseje analisar.|http://hdl.handle.net/10183/2631
Desenvolvimento de arquitetura para sistemas de reconhecimento automático de voz baseados em modelos ocultos de Markov|2001|Open Access|Tese|Reconhecimento : Padroes;Reconhecimento : Voz;FPGA;Projeto : Circuitos integrados;Processamento : Sinais;Voz computacional|por|Este trabalho foi realizado dentro da área de reconhecimento automático de voz (RAV). Atualmente, a maioria dos sistemas de RAV é baseada nos modelos ocultos de Markov (HMMs) [GOM 99] [GOM 99b], quer utilizando-os exclusivamente, quer utilizando-os em conjunto com outras técnicas e constituindo sistemas híbridos. A abordagem estatística dos HMMs tem mostrado ser uma das mais poderosas ferramentas disponíveis para a modelagem acústica e temporal do sinal de voz. A melhora da taxa de reconhecimento exige algoritmos mais complexos [RAV 96]. O aumento do tamanho do vocabulário ou do número de locutores exige um processamento computacional adicional. Certas aplicações, como a verificação de locutor ou o reconhecimento de diálogo podem exigir processamento em tempo real [DOD 85] [MAM 96]. Outras aplicações tais como brinquedos ou máquinas portáveis ainda podem agregar o requisito de portabilidade, e de baixo consumo, além de um sistema fisicamente compacto. Tais necessidades exigem uma solução em hardware. O presente trabalho propõe a implementação de um sistema de RAV utilizando hardware baseado em FPGAs (Field Programmable Gate Arrays) e otimizando os algoritmos que se utilizam no RAV. Foi feito um estudo dos sistemas de RAV e das técnicas que a maioria dos sistemas utiliza em cada etapa que os conforma. Deu-se especial ênfase aos Modelos Ocultos de Markov, seus algoritmos de cálculo de probabilidades, de treinamento e de decodificação de estados, e sua aplicação nos sistemas de RAV.  Foi realizado um estudo comparativo dos sistemas em hardware, produzidos por outros centros de pesquisa, identificando algumas das suas características mais relevantes. Foi implementado um modelo de software, descrito neste trabalho, utilizado para validar os algoritmos de RAV e auxiliar na especificação em hardware. Um conjunto de funções digitais implementadas em FPGA, necessárias para o desenvolvimento de sistemas de RAV é descrito. Foram realizadas algumas modificações nos algoritmos de RAV para facilitar a implementação digital dos mesmos. A conexão, entre as funções digitais projetadas, para a implementação de um sistema de reconhecimento de palavras isoladas é aqui apresentado. A implementação em FPGA da etapa de pré-processamento, que inclui a pré-ênfase, janelamento e extração de características, e a implementação da etapa de reconhecimento são apresentadas finalmente neste trabalho.|http://hdl.handle.net/10183/2633
Remoção de íons Cd2+ de soluções aquosas por eletrodeposição em eletrodos de carbono vítreo reticulado|2003|Open Access|Tese|Deposição eletroquímica;Metais : Remoção;Cádmio;Carbono vítreo reticulado|por|As soluções aquosas obtidas após o tratamento final de efluentes de processos de eletrodeposição de Cd contém baixas concentrações de íons Cd2+. Neste trabalho determinaram-se as melhores condições para a utilização de eletrodos de carbono vítreo reticulado (CVR) no polimento destas soluções. A eletrodeposição do íon Cd2+ sobre eletrodo de carbono vítreo reticulado de porosidades distintas, 30, 60 e 100 ppi, com e sem recobrimento com polipirrol, foi investigada em soluções aquosas aeradas de ácido sulfúrico e sulfato de potássio em pH 4,8. Sob condições potenciostáticas, uma elevada eficiência de remoção foi obtida para soluções contendo 5 e 10 mg L-1 de íon Cd2+, na faixa de potenciais entre –0,9 e –1,1 V para CVR e em –3,0 V para CVR recoberto com polipirrol (CVR-PPy0). Após cada experimento de eletrodeposição, a diminuição da concentração do íon Cd2+ no eletrólito foi monitorada por voltametria de redissolução anódica. Neste experimenteo, empregando um eletrodo de gota pendente de mercúrio sendo estes resultados comparados com medidas por espectrometria de emissão atômica (ICP).  Para o eletrodo de CVR, neste intervalo de potenciais, -0,9 e –1,1 V, a eletrodeposição do íon cádmio é controlada por transporte de massa e a concentração de íons cádmio varia exponencialmente com o tempo, seguindo uma cinética de pseudo primeira ordem. Para a concentração 10 mg L-1 e usando eletrodo de CVR 30 ppi, as eficiências de corrente e de remoção determinadas a -1,1 V após 30 minutos de eletrólise foram, 38 % e 97% , respectivamente. Para eletrodo de CVR 60 ppi foram encontrados 30 % e 99 %, respectivamente. Para o eletrodo de CVR-PPy0 a maior eficiência de remoção encontrada foi de 84% após 90 minutos de eletrólise em –3,0 V, sendo a eficiência de corrente menor do que 2%. A presença de Cd metálico depositado na superfície do eletrodo de CVR e CVR-PPy0 depois da redução em –1,1 V e –3,0 V, respectivamente, foi confirmada por análise de Microscopia Eeletrônica de Varredura (MEV) e espectrometria de energia dispersiva (EDS).|http://hdl.handle.net/10183/2634
Estudo da agregação de surfactantes aniônicos em presença de (hidroxipropil)celulose|2002|Open Access|Dissertação|Surfactantes aniônicos;Espalhamento de luz;Celulose|por|As técnicas de espectrofluorimetria, viscosimetria e espalhamento de luz têm sido utilizadas no estudo da agregação de diferentes surfactantes aniônicos em presença de 0,5% (m/v) de (hidroxipropil)celulose, no regime diluído (HPC Mw = 173000 g/mol), e em moderada força iônica (NaCl 0,1 M). Admitindo-se uma faixa geral de concentração, entre 10-5 e 10-2 mol.L-1, foram empregados neste estudo os surfactantes colato de sódio (CS), deoxicolato de sódio (DC), derivados dos sais bilares, e o alquilsintético dodecil sulfato de sódio (SDS). O polímero HPC contribui diferentemente no processo de agregação de cada surfactante, evidenciado pela mudança dos valores da concentração de agregação crítica, C1, em relação à concentração micelar crítica (cmc), obtidos pela técnica de espectrofluorimetria. Ambos os valores de C1 diminuem com respeito à cmc para SDS, bem como para DC, enquanto um ligeiro aumento é observado para CS. Os dados de viscosidade relativa, ηrel, indicam um aumento substancial da viscosidade dos sistemas HPC/sais biliares a altas concentrações de surfactante.  Diferentemente, para o sistema HPC/SDS, os valores de ηrel passam por um máximo a 3 mmol.L-1 em agregados e decresce a valores abaixo da viscosidade da solução polimérica livre de surfactante. As medidas da temperatura de turbidez (Tturb), por espalhamento de luz, para os sistemas HPC/sais biliares, mostraram um crescimento gradual de Tturb em função do aumento da concentração de surfactante, de 37 oC (0,5% HPC/NaCl 0,1 M) até estabilizar-se em torno de 50o C, para concentrações mais elevadas. Por outro lado, para HPC/SDS, a Tturb cresce acentuadamente, superando a temperatura de 100 oC a contrações maiores do que 20 mmol.L-1. Através do espalhamento de luz dinâmico, verificou-se a existência de dois modos difusivos (rápido e lento) para todos os sistemas estudados. A principal contribuição é proveniente do modo rápido, exceto na faixa de concentração entre C1 e a concentração de saturação, C2, na qual ambos os modos contribuem igualmente. Esses modos estão relacionados inicialmente a agregados HPC/surfactante e a “clusters” do polímero e, posteriormente, a agregados HPC/surfactante e micelas livres. Os resultados indicam uma interação HPC/SDS mais efetiva do que HPC/DC ou HPC/CS, fato este relacionado à estrutura dos agregados formados. Comparativamente, os agregados HPC/sais biliares são menores, possuem menor densidade de carga e são mais rígidos do que os agregados de HPC/SDS.|http://hdl.handle.net/10183/2635
Análise estratigráfica dos sedimentos eo/mesodevonianos da porção ocidental da Bacia do Amazonas sob a ótica da estratigrafia de seqüências no interior cratônico|2000|Open Access|Dissertação|Cronoestratigrafia;Estratigrafia de sequencias;Cicloestratigrafia;Análise faciológica;Amazonas, Rio, Bacia|por|O trabalho foi desenvolvido com a finalidade de verificação da aplicabilidade dos conceitos da moderna Estratigrafia de Seqüências no interior cratônico e a possibilidade do refinamento cronoestratigráfico da seção eo/mesodevoniana da Bacia do Amazonas. O autor utilizou as seguintes ferramentas disponíveis para a interpretação e elaboração de um modelo geológico para a seção sedimentar estudada: a) a análise da Paleogeografia, do Paleomagnetismo e da Paleoecologia, através do estudo da Tectônica e do Clima atuantes no Eo/mesodevoniano, no Supercontinente Gondwana, retratadas em reconstituições do mundo devoniano e apoiada em extensiva consulta bibliográfica e em correlação com os conteúdos faunístico, icnológico e litológico da seção que compõe o intervalo pesquisado, com o auxílio de testemunhos, amostras de calha e afloramentos, b) o estabelecimento de superfícies-chave da Estratigrafia de Seqüências, definidas com o apoio de perfis elétrico-radioativos, notadamente o perfil de raios-gama, c) a Cicloestratigrafia química com a utilização dos teores de carbono orgânico e o índice de hidrogênio, d) a Cicloestratigrafia orbital e climática, mediante a análise espectral do perfil de raios-gama da seção estudada e a definição do controle da sedimentação, influenciada pela excentricidade curta da órbita terrestre, em ciclos dentro da banda de freqüências de Milankovitch.  O encadeamento dessas análises levou o autor a montar um arcabouço cronoestratigráfico para o Eo/mesodevoniano da porção ocidental da Bacia do Amazonas. Para tanto, foram consideradas a hierarquização das unidades, a definição dos tratos de sistemas deposicionais, limites de seqüências e outras superfícies-chave estratigráficas, e a duração temporal dos eventos. Além disso, são discutidas as possíveis causas principais da evolução tectono-estratigráfica da seção estudada e sua associação com a curva de variação do nível do mar devoniano. Realizou-se ainda, tentativamente, a correlação com outras bacias intracratônicas gondwânicas através da comparação do conteúdo faunístico, da xvii paleoclimatologia, da tectônica, da análise da variação da curva do nível do mar devoniano, da posição geográfica e da relação com o Pólo Sul devoniano, que serviram de base para a compreensão do evento relacionado à passagem do Givetiano ao Frasniano, nessas bacias. Como contribuições principais o autor aponta a aplicabilidade da Estratigrafia de Seqüências no interior cratônico e a possibilidade de refinamento cronoestratigráfico em pelo menos uma ordem de grandeza (10 6 anos para 10 5 anos) com a utilização da metodologia da Cicloestratigrafia orbital.|http://hdl.handle.net/10183/2657
Derivação e solução de equações modelo da dinâmica de gases rarefeitos|2003|Open Access|Dissertação|Equação linearizada de Boltzman;Métodos de ordenadas discretas;Gases rarefeitos|por|Neste trabalho, duas equações modedlo na área da dinâmica de gases rarefeitos, são derivadas a partir de algumas soluções exatas da equação linearizada de Boltzmann homogênea e não homogênea. Em adição, uma versão analítca do método de ordenadas discretas é usado para resolver problemas clássicos nesta área, descritos pelo ""Modelo S"". Resultados numéricos são apresentados para os problemas de fluxo de Couette, fluxo de Poiseuille, ""Creep"" Térmico, Deslizamento Térmico e problema de Kramers.|http://hdl.handle.net/10183/2658
Crescimento térmico de filmes dielétricos sobre SiC e caracterização das estruturas formadas|2003|Open Access|Tese|Filmes finos dieletricos;Silício;Óxidos;Nitretos;Tratamento térmico;Reacoes nucleares|por|Na presente tese, foi investigado o crescimento térmico de filmes dielétricos (óxido de silício e oxinitreto de silício) sobre carbeto de silício. Além disso, os efeitos da irradiação iônica do SiC antes de sua oxidação, com o intuito de acelerar a taxa de crescimento do filme de SiO2, também foram investigados. A tese foi dividida em quatro diferentes etapas. Na primeira, foram investigados os estágios iniciais da oxidação térmica do SiC: mudanças no ambiente químico dos átomos de Si foram observadas após sucessivas etapas de oxidação térmica de uma lâmina de SiC. A partir desses resultados, em conjunto com a análise composicional da primeira camada atômica da amostra, acompanhou-se a evolução do processo de oxidação nesses estágios iniciais. Na etapa seguinte, foram utilizadas as técnicas de traçagem isotópica e análise por reação nuclear com ressonância estreita na curva de seção de choque na investigação do mecanismo e etapa limitante do crescimento térmico de filmes de SiO2 sobre SiC. Nesse estudo, compararam-se os resultados obtidos de amostras de óxidos termicamente crescidos sobre Si e sobre SiC. Na terceira etapa, foram investigados os efeitos da irradiação iônica do SiC antes de sua oxidação nas características finais da estrutura formada. Finalmente, comparam-se os resultados da oxinitretação térmica de estruturas SiO2/SiC e SiO2/Si utilizando dois diferentes gases: NO e N2O.|http://hdl.handle.net/10183/2661
Introdução ao estudo dos espaços pré-homogêneos|2003|Open Access|Dissertação|Espaços pré-homogêneos regulares|por|Resumo não disponível.|http://hdl.handle.net/10183/2665
Vibrações livres e forçadas no modelo de Timoshenko|2002|Open Access|Dissertação|Modelo de Timoshenko;Vibracoes;Cálculo modal;Bases dinamicas;Simulação numérica;Software MAPLE|por|O objetivo deste trabalho é o cálculo modal da resposta impulso distribu ída de uma viga descrita pela equação de Timoshenko e das vibrações forçadas, devidas a influência de cargas externas. Os modos vibratórios foram obtidos com o uso da base dinâmica, gerada por uma resposta livre e suas derivadas. Esta resposta é caracterizada por condições iniciais impulsivas. Simulações foram realizadas para os modos, a resposta impulso distribuída e vibrações forçadas em vigas apoiadas em uma extremidade e na outra livre, fixa, deslizante ou apoiada, sujeitas a cargas oscilatórias espacialmente concentradas ou distribuídas através de pulsos.|http://hdl.handle.net/10183/2681
Inspeção de aplicações Java através da identificação de padrões de projeto|2003|Open Access|Tese|Engenharia : Software;Software orientado : Objetos;Engenharia reversa;Java (Linguagem de programação);Reflexao computacional|por|Para reutilização, manutenção e refatoração, projetistas de sistemas de software, freqüentemente, precisam examinar o código fonte da aplicação para entender os detalhes dos sistemas desenvolvidos. As aplicações orientadas a objetos em geral, tornam-se coleções nebulosas de classes e implementações de métodos. Sem dúvida a habilidade de entender sistemas de software é largamente aumentada visualizando-se esses produtos em níveis mais altos de abstração. Os padrões de projeto demonstram um alto índice de abstração e são considerados uma ferramenta efetiva para o entendimento de sistemas de software orientados a objetos. Aplicações orientadas a objetos visualizadas como um sistema de interação de padrões requerem a descoberta, identificação e classificação de grupos de classes relacionadas. Estas visualizações podem representar qualquer padrão conhecido ou agrupamentos que executam uma tarefa abstrata e necessariamente não são uma solução de padrão conhecida. Os padrões de projeto descrevem, portanto, microarquiteturas que resolvem problemas arquitetônicos em sistemas de software orientados a objetos. É importante identificar estas microarquiteturas durante a fase de manutenção de aplicações orientadas a objetos. Faz-se necessário salientar que estas microarquiteturas aparecem freqüentemente distorcidas na aplicação fonte.  O objeto deste trabalho é demonstrar a viabilidade de construir uma ferramenta para descobrir a utilização de padrões de projeto em aplicações Java. Assim, esta tese examina as características de alguns padrões, determinando a natureza do que faz um padrão ser detectável por intermédio de meios automatizados, e propõe algumas regras pelas quais um conjunto de padrões possa ser identificado. As regras são baseadas nos relacionamentos entre classes e objetos mediante observação dos modelos estático e dinâmico. Este trabalho também documenta o desenvolvimento do protótipo da ferramenta de inspeção, que tem por objetivo aplicar os processos de engenharia reversa e reflexão computacional sobre código Java, utilizando as informações adquiridas para detectar padrões de projeto. Finalmente, esta tese demonstra a utilização dessa ferramenta em um exemplo pequeno de aplicação Java e forma a base para trabalhos adicionais que investiguem a existência de diferentes padrões de projeto em sistemas de software construídos em Java.|http://hdl.handle.net/10183/2686
Um sistema de valores de troca para suporte às interações em sociedades artificiais|2003|Open Access|Dissertação|Inteligência artificial;Sistemas multiagentes;Sociedades artificiais|por|Este trabalho propõe a definição de um Sistema de Valores de Troca para modelar as trocas sociais entre agentes em sociedade artificiais. Esse sistema é baseado na Teoria das Trocas de Valores de Jean Piaget e é composto por uma algebrá de valores de troca, que indica como esses valores devem ser representados e manipulados, por um mecanismo de raciocínio social baseado em vaores de troca e por estruturas capazes de armazenar e manipular tais valores. Nesse sistema, os valores de troca são vistos tanto como elementos motivadores das interações quanto como elementos reguladores responsáveis pelo equilíbrio e continuidade das trocas sociais. Acredita-se que o istema proposto é capaz de melhorar a modelagem das interações. É mostrado, também, como o sistema de valores proposto pode ser integrado com modelos de interação existentes na literatura de sistemas multiagente; Para isso, foram escolhidos dosi modelos práticos de organização dinâmica - o Redes de Contrato e o Modelo de Coalizões Baseadas em Dependências. Para demonstrar comomo o sistema de valores pode ser aplicado na modelagem e na simuulação de situações reais, é descrito um cenário para experimentação, no qual o sistema proposto é utilizado para modelar, de forma simplificada, o processo de lobby atrtavés de contribuições para campanhas políticas.  Com este cenário pretende-se observar, além da dinâmica dos valores de troca, a capacidade do sistema em modelar caraterísticas mais subjetivas das interações (normalmente observadas nas relações humanas), e, ao tempo tempo, prover elementos reguladores, instrurmentos para a continuidade das interações e trocas sociais.|http://hdl.handle.net/10183/2687
Uma ferramenta para automação da geração do leiaute de circuitos analógicos sobre uma matriz de transistores MOS pré-difundidos|2003|Open Access|Dissertação|Microeletrônica;Cad : Microeletronica;Projeto : Circuitos integrados|por|Este trabalho apresenta o LIT, uma ferramenta de auxílio ao projeto de circuitos integrados analógicos que utiliza a técnica da associação trapezoidal de transistores (TAT) sobre uma matriz digital pré-difundida. A principal característica é a conversão de cada transistor simples de um circuito analógico em uma associação TAT equivalente, seguido da síntese automática do leiaute da associação séria-paralela de transistores. A ferramenta é baseada na matriz SOT (sea-of-transistors), cuja arquitetura é voltada para o projeto de circuitos digitais. A matriz é formada somente por transistores unitários de canal curto de dimensões fixas. Através da técnica TAT, entretanto, é possível criar associações série-paralelas cujo comportamento DC aproxima-se dos transistores de dimensões diferentes dos unitários. O LIT é capaz de gerar automaticamente o leiaute da matriz SOT e dos TATs, além de células analógicas básicas, como par diferencial e espelho de corrente, respeitando as regras de casamento de transistores. O cálculo dos TATs equivalentes também é realizado pela ferramenta. Ela permite a interação com o usuário no momento da escolha da melhor associação. Uma lista de possíveis associações é fornecida, cabendo ao projetista escolher a melhor. Além disso, foi incluído na ferramenta um ambiente gráfico para posicionamento das células sobre a matriz e um roteador global automático. Com isso, é possível realizar todo o fluxo de projeto de um circuito analógico com TATs dentro do mesmo ambiente, sem a necessidade de migração para outras ferramentas.  Foi realizado também um estudo sobre o cálculo do TAT equivalente, sendo que dois métodos foram implementados: aproximação por resistores lineares (válida para transistores unitários de canal longo) e aproximação pelo modelo analítico da corrente de dreno através do modelo BSIM3. Três diferentes critérios para a escolha da melhor associação foram abordados e discutidos: menor diferença de corrente entre o TAT e o transistor simples, menor número de transistores unitários e menor condutância de saída. Como circuito de teste, foi realizado o projeto com TATs de um amplificador operacional de dois estágios (amplificador Miller) e a sua comparação com o mesmo projeto utilizando transistores full-custom. Os resultados demonstram que se pode obter bons resultados usando esta técnica, principalmente em termos de desempenho em freqüência. A contribuição da ferramenta LIT ao projeto de circuitos analógicos reside na redução do tempo de projeto, sendo que as tarefas mais suscetíveis a erro são automatizadas, como a geração do leiaute da matriz e das células e o roteamento global. O ambiente de projeto, totalmente gráfico, permite que mesmo projetistas analógicos menos experientes realizem projetos com rapidez e qualidade. Além disso, a ferramenta também pode ser usada para fins educacionais, já que as facilidades proporcionadas ajudam na compreensão da metodologia de projeto.|http://hdl.handle.net/10183/2693
ORPIS: um modelo de consistência de conteúdo replicado em servidores Web distribuídos|2003|Open Access|Dissertação|Sistemas operacionais;Redes : Computadores;Servidor : Www;Replicação : Servidores;Processamento distribuído|por|O surgimento de novas aplicações que utilizam o protocolo HTTP nas suas transações e a crescente popularidade da World Wide Web (WWW) provocaram pesquisas pelo aumento do desempenho de servidores Web. Para tal, uma das alternativas propostas neste trabalho é utilizar um conjunto de servidores Web distribuídos que espalham a carga de requisições entre vários computadores, atuando como um só associado a uma estratégia de replicação de conteúdo. Um dos problemas centrais a ser resolvido em servidores Web distribuídos é como manter a consistência das réplicas de conteúdo entre os equipamentos envolvidos. Esta dissertação apresenta conceitos fundamentais envolvendo o tema replicação de conteúdo em servidores Web distribuídos. São mostrados detalhes sobre arquitetura de servidores Web distribuídos, manutenção da consistência em ambientes de servidores Web distribuídos, uso de replicação e formas de replicação. Além disso, são citados alguns trabalhos correlatos ao propósito de manter réplicas consistentes em ambientes de servidores Web distribuídos. Este trabalho tem por objetivo propor um modelo de manutenção da consistência de conteúdo em servidores Web distribuídos com características de transparência e autonomia.  O modelo, denominado One Replication Protocol for Internet Servers (ORPIS), adota uma estratégia de propagação otimista porque não existe sincronismo no envio das atualizações para as réplicas. Este trabalho apresenta os principais componentes tecnológicos empregados na Web, além dos problemas causados pela escalabilidade e distribuição inerentes a esse ambiente. São descritas as principais técnicas de aumento de desempenho de servidores Web que atualmente vêm sendo utilizadas. O modelo ORPIS é descrito, sendo apresentados seus pressupostos, elencados seus componentes e detalhados os seus algoritmos de funcionamento. Este trabalho dá uma visão geral sobre a implementação e os testes realizados em alguns módulos do protótipo do modelo, caracterizando o ambiente de desenvolvimento do protótipo e detalhes da implementação. São enumerados os atributos e métodos das classes do protótipo e definidas as estruturas de dados utilizadas. Além disso, apresentam-se os resultados obtidos da avaliação funcional dos módulos implementados no protótipo. Um ponto a ser salientado é a compatibilidade do modelo ORPIS aos servidores Web existentes, sem a necessidade de modificação em suas configurações. O modelo ORPIS é baseado na filosofia de código aberto. Durante o desenvolvimento do protótipo, o uso de software de código aberto proporcionou um rápido acesso às ferramentas necessárias (sistema operacional, linguagens e gerenciador de banco de dados), com possibilidade de alteração nos códigos fonte como uma alternativa de customização.|http://hdl.handle.net/10183/2694
Estendendo quadtress para suporte ao armazenamento e recuperação de dados espaço-temporais|2000|Open Access|Dissertação|Geoinformática;Sistemas : Informacao geografica;Recuperacao : Informacao|por|Nos Sistemas de Informação Geográfica (SIG), os aspectos temporais são importantes, principalmente, para representar o histórico de dados georreferenciados. Vários modelos conceituais de dados para SIG propõem classes e operações que permitem representar os aspectos espaciais e temporais das aplicações. Porém, ao nível do modelo interno dos sistemas atuais, as estruturas de dados armazenam e manipulam somente os aspectos espaciais dos dados geográficos, não contemplando os aspectos espaço-temporais propostos nos modelos conceituais. O objetivo desse trabalho é estender estruturas de dados do tipo quadtree para suporte ao armazenamento e à recuperação de dados espaço-temporais.|http://hdl.handle.net/10183/2695
Smart visible sets para ambientes de rede|2003|Open Access|Dissertação|Computação gráfica;Realidade virtual;Jogos : Estrategia|por|A visualização em tempo real de cenas complexas através de ambientes de rede é um dos desafios na computação gráfica. O uso da visibilidade pré-computada associada a regiões do espaço, tal como a abordagem dos Potentially Visible Sets (PVS), pode reduzir a quantidade de dados enviados através da rede. Entretanto, o PVS para algumas regiões pode ainda ser bastante complexo, e portanto uma estratégia diferente para diminuir a quantidade de informações é necessária. Neste trabalho é introduzido o conceito de Smart Visible Set (SVS), que corresponde a uma partição das informações contidas no PVS segundo o ângulo de visão do observador e as distâncias entre as regiões. Dessa forma, o conceito de “visível” ou de “não-visível” encontrado nos PVS é estendido. A informação referente ao conjunto “visível” é ampliada para “dentro do campo de visão” ou “fora do campo de visão” e “longe” ou “perto”. Desta forma a informação referente ao conjunto “visível” é subdividida, permitindo um maior controle sobre cortes ou ajustes nos dados que devem ser feitos para adequar a quantidade de dados a ser transmitida aos limites impostos pela rede. O armazenamento dos SVS como matrizes de bits permite ainda uma interação entre diferentes SVS. Outros SVS podem ser adicionados ou subtraídos entre si com um custo computacional muito pequeno permitindo uma rápida alteração no resultado final. Transmitir apenas a informação dentro de campo de visão do usuário ou não transmitir a informação muito distante são exemplos dos tipos de ajustes que podem ser realizados para se diminuir a quantidade de informações enviadas.  Como o cálculo do SVS depende da existência de informação de visibilidade entre regiões foi implementado o algoritmo conhecido como “Dual Ray Space”, que por sua vez depende do particionamento da cena em regiões. Para o particionamento da cena em uma BSP-Tree, foi modificada a aplicação QBSP3. Depois de calculada, a visibilidade é particionada em diferentes conjuntos através da aplicação SVS. Finalmente, diferentes tipos de SVS puderam ser testados em uma aplicação de navegação por um cenário 3D chamada BSPViewer. Essa aplicação também permite comparações entre diferentes tipos de SVS e PVS. Os resultados obtidos apontam o SVS como uma forma de redução da quantidade de polígonos que devem ser renderizados em uma cena, diminuindo a quantidade de informação que deve ser enviada aos usuários. O SVS particionado pela distância entre as regiões permite um corte rápido na informação muito distante do usuário. Outra vantagem do uso dos SVS é que pode ser realizado um ordenamento das informações segundo sua importância para o usuário, desde que uma métrica de importância visual tenha sido definida previamente.|http://hdl.handle.net/10183/2696
Paralelizações de métodos numéricos em clusters empregando as bibliotecas MPICH, DECK e Pthread|2003|Open Access|Dissertação|Análise numérica;Cluster;Mpi;Paralelismo|por|Este trabalho tem como objetivo desenvolver e empregar técnicas e estruturas de dados agrupadas visando paralelizar os métodos do subespaço de Krylov, fazendo-se uso de diversas ferramentas e abordagens. A partir dos resultados é feita uma análise comparativa de desemvpenho destas ferramentas e abordagens. As paralelizações aqui desenvolvidas foram projetadas para serem executadas em um arquitetura formada por um agregado de máquinas indepentes e multiprocessadas (Cluster), ou seja , são considerados o paralelismo e intra-nodos. Para auxiliar a programação paralela em clusters foram, e estão sendo, desenvolvidas diferentes ferramentas (bibliotecas) que visam a exploração dos dois níveis de paralelismo existentes neste tipo de arquitetura. Neste trabalho emprega-se diferentes bibliotecas de troca de mensagens e de criação de threads para a exploração do paralelismo inter-nodos e intra-nodos. As bibliotecas adotadas são o DECK e o MPICH e a Pthread. Um dos itens a serem analisados nestes trabalho é acomparação do desempenho obtido com essas bibliotecas.O outro item é a análise da influência no desemepnho quando quando tulizadas múltiplas threads no paralelismo em clusters multiprocessados. Os métodos paralelizados nesse trabalho são o Gradiente Conjugação (GC) e o Resíduo Mínmo Generalizado (GMRES), quepodem ser adotados, respectivamente, para solução de sistemas de equações lineares sintéticos positivos e definidos e não simétricas. Tais sistemas surgem da discretização, por exemplo, dos modelos da hidrodinâmica e do transporte de massa que estão sendo desenvolvidos no GMCPAD.  A utilização desses métodos é justificada pelo fato de serem métodos iterativos, o que os torna adequados à solução de sistemas de equações esparsas e de grande porte. Na solução desses sistemas através desses métodos iterativos paralelizados faz-se necessário o particionamento do domínio do problema, o qual deve ser feito visando um bom balanceamento de carga e minimização das fronteiras entre os sub-domínios. A estrutura de dados desenvolvida para os métodos paralelizados nesse trabalho permite que eles sejam adotados para solução de sistemas de equações gerados a partir de qualquer tipo de particionamento, pois o formato de armazenamento de dados adotado supre qualquer tipo de dependência de dados. Além disso, nesse trabalho são adotadas duas estratégias de ordenação para as comunicações, estratégias essas que podem ser importantes quando se considera a portabilidade das paralelizações para máquinas interligadas por redes de interconexão com buffer de tamanho insuficiente para evitar a ocorrência de dealock. Os resultados obtidos nessa dissertação contribuem nos trabalhos do GMCPAD, pois as paralelizações são adotadas em aplicações que estão sendo desenvolvidas no grupo.|http://hdl.handle.net/10183/2700
Pré-processamento no processo de descoberta de conhecimento em banco de dados|2003|Open Access|Dissertação|Banco : Dados;Descoberta : Conhecimento;Mineracao : Dados;Inteligência artificial|por|A Descoberta de Conhecimento em Banco de Dados (DCBD) é uma nova área de pesquisa que envolve o processo de extração de conhecimento útil implícito em grandes bases de dados. Existem várias metodologias para a realização de um processo de DCBD cuja essência consiste basicamente nas fases de entendimento do domínio do problema, pré-processamento, mineração de dados e pós-processamento. Na literatura sobre o assunto existem muitos trabalhos a respeito de mineração de dados, porém pouco se encontra sobre o processo de pré-processamento. Assim, o objetivo deste trabalho consiste no estudo do pré-processamento, já que é a fase que consome a maior parte do tempo e esforço de todo o processo de DCBD pois envolve operações de entendimento, seleção, limpeza e transformação de dados. Muitas vezes, essas operações precisam ser repetidas de modo a aprimorar a qualidade dos dados e, conseqüentemente, melhorar também a acurácia e eficiência do processo de mineração.  A estrutura do trabalho abrange cinco capítulos. Inicialmente, apresenta-se a introdução e motivação para trabalho, juntamente com os objetivos e a metodologia utilizada. No segundo capítulo são abordadas metodologias para o processo de DCBD destacando-se CRISP-DM e a proposta por Fayyad, Piatetsky-Shapiro e Smyth. No terceiro capítulo são apresentadas as sub-fases da fase de pré-processamento contemplando-se entendimento, seleção, limpeza e transformação de dados, bem como os principais métodos e técnicas relacionados às mesmas. Já no quarto capítulo são descritos os experimentos realizados sobre uma base de dados real. Finalmente, no quinto capítulo são apresentadas as considerações finais sobre pré-processamento no processo de DCBD, apontando as dificuldades encontradas na prática, contribuições do presente trabalho e pretensões da continuidade do mesmo. Considera-se como principais contribuições deste trabalho a apresentação de métodos e técnicas de pré-processamento existentes, a comprovação da importância da interatividade com o especialista do domínio ao longo de todo o processo de DCBD, mas principalmente nas tomadas de decisões da fase de pré-processamento, bem como as sugestões de como realizar um pré-processamento sobre uma base de dados real.|http://hdl.handle.net/10183/2701
Mineração de dados utilizando aprendizado não-supervisionado: um estudo de caso para bancos de dados da saúde|2003|Open Access|Dissertação|Banco : Dados;Mineracao : Dados;Descoberta : Conhecimento;Inteligência artificial;Informática médica|por|A mineração de dados constitui o processo de descoberta de conhecimento interessante, com a utilização de métodos e técnicas que permitem analisar grandes conjuntos de dados para a extração de informação previamente desconhecida, válida e que gera ações úteis, de grande ajuda para a tomada de decisões estratégicas. Dentre as tarefas de mineração de dados, existem aquelas que realizam aprendizado não-supervisionado, o qual é aplicado em bases de dados não-classificados, em que o algoritmo extrai as características dos dados fornecidos e os agrupa em classes. Geralmente, o aprendizado não-supervisionado é aplicado em tarefas de agrupamento, que consistem em agrupar os dados de bancos de dados volumosos, com diferentes tipos de dados em classes ou grupos de objetos que são similares dentro de um mesmo grupo e dissimilares em diferentes grupos desses bancos de dados, de acordo com alguma medida de similaridade. Os agrupamentos são usados como ponto de partida para futuras investigações. Este trabalho explora, mediante a realização de um estudo de caso, o uso de agrupamento como tarefa de mineração de dados que realiza aprendizado nãosupervisionado, para avaliar a adequação desta tecnologia em uma base de dados real da área de saúde. Agrupamento é um tema ativo em pesquisas da área pelo seu potencial de aplicação em problemas práticos.  O cenário da aplicação é o Sistema de Informações Hospitalares do SUS, sob a gestão da Secretaria Estadual de Saúde do Rio Grande do Sul. Mensalmente, o pagamento de um certo número de internações é bloqueado, uma vez que a cobrança de internações hospitalares é submetida a normas do SUS e a critérios técnicos de bloqueio estabelecidos pela Auditoria Médica da SES para verificar a ocorrência de algum tipo de impropriedade na cobrança dos procedimentos realizados nessas internações hospitalares. A análise de agrupamento foi utilizada para identificar perfis de comportamentos ou tendências nas internações hospitalares e avaliar desvios ou outliers em relação a essas tendências e, com isso, descobrir padrões interessantes que auxiliassem na otimização do trabalho dos auditores médicos da SES. Buscou-se ainda compreender as diferentes configurações de parâmetros oferecidos pela ferramenta escolhida para a mineração de dados, o IBM Intelligent Miner, e o mapeamento de uma metodologia de mineração de dados, o CRISP-DM, para o contexto específico deste estudo de caso. Os resultados deste estudo demonstram possibilidades de criação e melhora dos critérios técnicos de bloqueio das internações hospitalares que permitem a otimização do trabalho de auditores médicos da SES. Houve ainda ganhos na compreensão da tecnologia de mineração de dados com a utilização de agrupamento no que se refere ao uso de uma ferramenta e de uma metodologia de mineração de dados, em que erros e acertos evidenciam os cuidados que devem ser tomados em aplicações dessa tecnologia, além de contribuírem para o seu aperfeiçoamento.|http://hdl.handle.net/10183/2702
AFRODITE: ambiente de simulação baseado em agentes com emoções|2003|Open Access|Dissertação|Inteligência artificial;Sistemas multiagentes;Simulação;Emoções|por|Este trabalho está relacionado às áreas de Sistemas Multiagentes, Simulação Computacional e Emoções. A partir do estudo destas áreas de pesquisa, foi proposto e desenvolvido um protótipo para um ambiente de simulação baseado em agentes com emoções. Os sistemas multiagentes têm sido utilizados nas mais diversas áreas de pesquisa, não apenas para a área acadêmica, mas também para fins comerciais. Isso ocorre devido a características importantes que estes possuem, como flexibilidade e cooperação. Estas características são úteis para um grande número de aplicações, como para simulação de situações reais, pois os modelos de simulação desenvolvidos utilizando a tecnologia de agentes são muito eficazes e versáteis no estudo dos mais diferentes problemas. Emoções vêm sendo estudadas há algum tempo, pois elas influenciam a tomada de decisão de todas as suas atividades. A tentativa de expressar emoções é algo complexo, dependendo de diversos fatores, tanto sociais como fisiológicos. Objetivando a abrangência das pesquisas na área de sistemas multiagentes, este trabalho propõe o desenvolvimento de um protótipo para um ambiente de simulação baseado em agentes com emoções, utilizando como base para a estruturação das emoções o modelo OCC.  Este novo ambiente é chamado AFRODITE. De forma a melhor definir como o AFRODITE seria implementado, foram estudados quatro ambientes de simulação baseados em agentes existentes - SIEME, SWARM, SeSAm e SIMULA, e alguns aspectos destes foram utilizados na construção do novo ambiente. Para demonstrar como o AFRODITE é utilizado, três exemplos de aplicações de áreas de conhecimentos diferentes foram modelados: o IPD (Iterated Prisoner’s Dilemma), da área de Teoria dos Jogos; Simulação de Multidões, da área de Engenharia de Segurança; e Venda de aparelhos celulares com serviço WAP, da área de Telecomunicações. Através dos três exemplos modelados foi possível demonstrar que o ambiente proposto é de fácil utilização e que a tarefa de inserção de emoções nas regras de comportamento pode ser realizada pelo usuário de forma transparente.|http://hdl.handle.net/10183/2706
Ferramenta visual para especificação de hiperdocumentos, segundo o método OOHDM|2001|Open Access|Dissertação|Automação : Escritórios;Hiperdocumento;OOHDM|por|O desenvolvimento de artefatos de software é um processo de engenharia, como todo processo de engenharia, envolve uma série de etapas que devem ser conduzidas através de uma metodologia apropriada. Para que um determinado software alcance seus objetivos, as características conceituais e arquiteturais devem ser bem definidas antes da implementação. Aplicações baseadas em hiperdocumentos possuem uma característica específica que é a definição de seus aspectos navegacionais. A navegação é uma etapa crítica no processo de definição de softwares baseados em hiperdocumentos, pois ela conduz o usuário durante uma sessão de visita ao conteúdo de um site. Uma falha no processo de especificação da navegação causa uma perda de contexto, desorientando o usuário no espaço da aplicação. Existem diversas metodologias para o tratamento das características de navegação de aplicações baseadas em hiperdocumentos. As principais metodologias encontradas na literatura foram estudadas e analisadas neste trabalho. Foi realizada uma análise comparativa entre as metodologias, traçando suas abordagens e etapas. O estudo das abordagens de especificação de hiperdocumentos foi uma etapa preliminar servindo como base de estudo para o objetivo deste trabalho.  O foco é a construção de uma ferramenta gráfica de especificação conceitual de hiperdocumentos, segundo uma metodologia de modelagem de software baseado em hiperdocumentos. O método adotado foi o OOHDM (Object-Oriented Hypermedia Design Model), por cercar todas as etapas de um processo de desenvolvimento de aplicações, com uma atenção particular à navegação. A ferramenta implementa uma interface gráfica onde o usuário poderá modelar a aplicação através da criação de modelos. O processo de especificação compreende três modelos: modelagem conceitual, modelagem navegacional e de interface. As características da aplicação são definidas em um processo incremental, que começa na definição conceitual e finaliza nas características de interface. A ferramenta gera um protótipo da aplicação em XML. Para a apresentação das páginas em um navegador Web, utilizou-se XSLT para a conversão das informações no formato XML para HTML. Os modelos criados através das etapas de especificação abstrata da aplicação são exportados em OOHDM-ML. Um estudo de caso foi implementado para validação da ferramenta. Como principal contribuição deste trabalho, pode-se citar a construção de um ambiente gráfico de especificação abstrata de hiperdocumentos e um ambiente de implementação de protótipos e exportação de modelos. Com isso, pretende-se orientar, conduzir e disciplinar o trabalho do usuário durante o processo de especificação de aplicações.|http://hdl.handle.net/10183/2707
Reação de estado sólido em multicamadas de Fe-Zr|1993|Open Access|Tese|Filmes finos;Ferro;Zircônio;Multicamadas;Temperatura;Materia condensada;Retroespalhamento rutherford;Difratometria de raios-x;Espectroscopia mossbauer;Tratamento térmico;Energia livre;Transformações de fase|por|Filmes finos de Fe-Zr na geometria de multicamadas foram tratados termicamente variando-se a temperatura (350ºC e 500ºC) e o tempo de tratamento (10min a 72h). As fases formadas por Reação de Estado Sólido (RES) em multicamadas com três composições totais diferentes (Fe0.67Zr0.33, Fe0.50Zr0.50, Fe0.33Zr0.67) foram analisadas por espectroscopia de retroespalhamento de Rutherford, difratometria de raios-X e espectroscopia Mössbauer. Verificou-se que a primeira fase formada é sempre uma fase amorfa de composição aproximada 50%at Fe. A fração Mössbauer desta fase, em função do tempo de tratamento térmico a 350ºC, foi traçada para as três composições. Também foram identificadas as fases cristalinas Fe2Zr e FeZr3. A seqüência de fases formadas revelou depender da composição da multicamada como-depositada e também da temperatura de tratamento. O diagrama esquemático de Energia Livre de Gibbs foi proposto de forma a comportar os resultados experimentais. Um modelo de crescimento planar de fases reagidas na interface de pares de reação foi aplicado às condições do crescimento multifásico observadas no presente trabalho.|http://hdl.handle.net/10183/2712
Escadas de spin integráveis|2003|Open Access|Tese|Sistemas de spin;Simetrias;Equacao de bethe ansatz;Energia;Propriedades físicas;Hamiltonianos de spin;Suscetibilidade magnética;Meio anisotrópico;Temperatura;Estados de impureza;Transformações de fase;Cobre|por|Neste trabalho definimos três modelos de escadas de spin integráveis novos que correspondem a variações de um modelo de escada de spin baseado na simetria SU(4). Os modelos são exatamente solúveis através do método do ansatz de Bethe e as equações do ansatz de Bethe, os autovalores de energia e o gap de spin são derivados e propriedades físicas interessantes são discutidas. Inicialmente apresentamos um modelo de escada de spin integrável que possui um parâmetro livre além do acomplamento ao longo dos degraus. Determinamos a dependência do parâmetro anisotrópico na transição de fase entre uma região com gap e outra sem gap. Nós também mostramos que o modelo é um caso especial de uma Hamiltoniana mais geral que possui três parâmetros livres. A susceptibilidade magnética em função da temperatura é obtida numericamente e sua dependência no parâmetro anisotrópico é determinada explicitamente. Uma comparação entre o gap de spin obtido através da curva de susceptibilidade magnética e aquele obtido das equações do ansatz de Bethe é feita e uma boa concordância encontrada. A conexão com alguns compostos é apresentada e mostramos que os nossos resultados ajustam bem a curva da susceptibilidade magnética dos compostos KCuCI3, CU2(C5H12N2hC14e (C5H12NhCuBr4. A seguir nós propomos dois tipos diferentes de modelos integráveis com impurezas. Mostramos em ambos os casos que uma transição de fase entre uma região com gap e outra sem gap ocorre para um valor crítico do acoplamento ao longo dos degraus. Além disso, a dependência das impurezas na transição de fase é determinada explicitamente. Em um dos modelos o gap diminui com o aumento da intensidade da impureza A. E, fixando a intensidade de impureza A, é observada uma redução do gap com o aumento da concentração de impurezas. Este resultado está qualitativamente de acordo com resultados experimentais.|http://hdl.handle.net/10183/2715
Implementação de bancos de dados temporais com versionamento de esquemas: um estudo comparativo|2003|Open Access|Dissertação|Banco : Dados;Banco : Dados temporais;Esquema : Banco : Dados;Versoes : Banco : Dados|por|Nas aplicações do mundo real, os dados mudam com o passar do tempo. Devido à característica dinâmica das aplicações, o esquema conceitual também pode mudar para se adaptar às mudanças que freqüentemente ocorrem na realidade. Para representar esta evolução, uma nova versão do esquema é definida e os dados armazenados são adaptados à nova versão. Entretanto, existem aplicações que precisam acessar também o esquema sob os diversos aspectos de suas mudanças, requerendo o uso de versionamento de esquemas. Durante a evolução do esquema, o versionamento preserva todas as versões deste esquema e seus dados associados, possibilitando a recuperação dos dados através da versão com a qual foram definidos. Ultimamente muitas pesquisas têm sido realizadas envolvendo as áreas de versionamento de esquemas e bancos de dados temporais. Estes bancos de dados provêm suporte ao versionamento de esquemas, pois permitem armazenar e recuperar todos os estados dos dados, registrando sua evolução ao longo do tempo. Apesar de muitos esforços, ainda não existem SGBDs temporais comercialmente disponíveis. A utilização de um modelo de dados temporal para a especificação de uma aplicação não implica, necessariamente, na utilização de um SGBD específico para o modelo. Bancos de dados convencionais podem ser utilizados desde que exista um mapeamento adequado entre o modelo temporal e o SGBD utilizado.  Este trabalho apresenta uma abordagem para a implementação de um banco de dados temporal permitindo o versionamento de esquemas, usando um banco de dados relacional, tendo como base o modelo temporal TRM (Temporal Relational Model). Como forma de ilustrar apresenta-se um exemplo de implementação utilizando o SGBD DB2. O principal objetivo é avaliar diferentes técnicas de implementar e gerenciar o versionamento de esquemas em bancos de dados temporais. Para atingir esse objetivo, um protótipo foi desenvolvido para automatizar os mapeamentos do TRM para o DB2 e gerenciar o versionamento de esquemas e dados. Duas experiências de implementação foram realizadas utilizando formas diferentes de armazenar os dados - um repositório e vários repositórios - com o objetivo de comparar os resultados obtidos, considerando um determinado volume de dados e alterações. Um estudo de caso também é apresentado para validar as implementações realizadas.|http://hdl.handle.net/10183/2720
Preparação de sílica organofuncionalizada à base de zircônia e estudos de adsorção de hidrocarbonetos policíclicos aromáticos e pesticidas organoclorados|2003|Open Access|Dissertação|Silica organofuncionalizada;Pesticidas : Organoclorados;Hidrocarbonetos poliaromáticos : Adsorção|por|O trabalho realizado conjuga informações de preparação e caracterização de materiais à base de sílica, com avaliação de possível aplicação potencial na área de química analítica ambiental. As sílicas organofuncionalizadas foram preparadas segundo 2 métodos: grafting e sol-gel. Os 4 sólidos obtidos por grafting diferem essencialmente na natureza da esfera de coordenação em torno do centro metálico (átomo de zircônio), enquanto que os 2 sólidos obtidos pelo processo sol-gel são sólidos híbridos, contendo ligantes indenil, grupos silanóis e etóxidos na superfície, diferindo entre si pela presença de centro metálico. Os teores de metal, determinados por RBS, nas sílicas organofuncionalizadas ficaram em torno de 0,3 % para os sólidos preparados por grafting e 4,5 %,no caso, do sólido preparado por sol-gel. A análise por DRIFTS confirmou a presença dos ligantes orgânicos e, ainda, grupos silanóis residuais nos adsorventes preparados por grafting.  A capacidade de adsorção das sílicas organofuncionalizadas foi avaliada frente a duas famílias de compostos: HPAs e pesticidas organoclorados. A identificação e quantificação dos HPAs foi conduzida através de cromatografia gasosa com detector seletivo de massas. Os resultados da capacidade de adsorção para os 16 HPAs prioritários não foram quantitativamente considerados satisfatórios, em parte devido à dificuldade de solubilidade em água. A determinação quantitativa da eficiência dos adsorventes sólidos, na pré-concentração/extração dos pesticidas organoclorados, foi realizada através da cromatografia gasosa com detector de captura de elétrons. Os resultados de recuperação, para os compostos organoclorados: heptacloro epóxido, dieldrin e endrin foram considerados satisfatórios e em concordância com valores encontrados com o adsorvente comercial LC-18. O lindano apresentou boa recuperação especificadamente nos adsorventes preparados por sol-gel. De uma forma geral, os resultados indicam a possibilidade de utilização futura dos adsorventes sólidos preparados em protocolos de pré-concentração/extração de organoclorados a nível de traços.|http://hdl.handle.net/10183/2724
Um microprocessador com capacidades analógicas|2002|Open Access|Dissertação|Microeletrônica;Testes : Circuitos integrados;Microprocessadores;Testes : Hardware|por|Este trabalho apresenta um estudo, implementação e simulação de geradores de sinais analógicos usando-se circuitos digitais, em forma de CORE, integrando-se este com o microprocessador Risco. As principais características procuradas no gerador de sinais são: facilidade de implementação em silício, programabilidade tanto em freqüência quanto em amplitude, qualidade do sinal e facilidade de integração com um microprocessador genérico. Foi feito um estudo sobre a geração convencional de sinais analógicos, dando-se ênfase em alguns tipos específicos de circuitos como circuitos osciladores sintonizados, multivibradores, geradores de sinais triangulares e síntese de freqüência digital direta. Foi feito também um estudo sobre conversão digital-analógica, onde foram mostrados alguns tipos básicos de conversores D/A. Além disso foram abordadas questões como a precisão desses conversores, tipos digitais de conversores digitalanalógico, circuitos geradores de sinais e as fontes mais comuns de erros na conversão D/A. Dando-se ênfase a um tipo específico de conversor D/A, o qual foi utilizado nesse trabalho, abordou-se a questão da conversão sigma-delta, concentrando-se principalmente no ciclo de formatação de ruído. Dentro desse assunto foram abordados o laço sigma-delta, as estruturas de realimentação do erro, estruturas em cascata, e também o laço quantizador.  Foram abordados vários circuitos digitais capazes de gerar sinais analógicos, principalmente senóides. Além de geradores de senóides simples, também se abordou a geração de sinais multi-tom, geração de outros tipos de sinais baseando-se no gerador de senóides e também foi apresentado um gerador de funções. Foram mostradas implementações e resultados dessas. Iniciando-se pelo microprocessador Risco, depois o gerador de sinais, o teste deste, a integração do microprocessador com o gerador de sinais e finalmente a implementação standard-cell do leiaute desse sistema. Por fim foram apresentadas conclusões, comentários e sugestões de trabalhos futuros baseando-se no que foi visto e implementado nesse trabalho.|http://hdl.handle.net/10183/2731
Geração de sistemas supervisórios a partir de modelos orientados a objetos|2000|Open Access|Dissertação|Administração : Produção;Sistemas : Tempo real;Orientacao : Objetos|por|Este trabalho aborda o tema da geração de sistemas supervisórios a partir de modelos orientados a objetos. A motivação para realização do trabalho surgiu com o estudo de sistemas supervisórios e de ferramentas de suporte à modelagem de sistemas usando orientação a objetos. Notou-se que nos primeiros, apesar de possuírem como principal objetivo a visualização de estados e grandezas físicas relacionadas a componentes de plantas industriais (nível de um tanque, temperatura de um gás, por exemplo), os modelos computacionais utilizados baseiam-se em estruturas de dados não hierárquicas, nas quais variáveis de contexto global e não encapsuladas, as chamadas “tags”, são associadas às grandezas físicas a serem visualizadas. Modelos orientados a objeto, por outro lado, constituem uma excelente proposta para a criação de modelos computacionais nos quais a estrutura e semântica dos elementos de modelagem é bastante próxima a de sistemas físicos reais, facilitando a construção e compreensão dos modelos. Assim sendo, a proposta desenvolvida neste trabalho busca agregar as vantagens do uso de orientação a objetos, com conceitos existentes em sistemas supervisórios, a fim de obter-se ferramentas que melhor auxiliem o desenvolvimento de aplicações complexas. Classes e suas instâncias são usadas para modelagem de componentes da planta industrial a ser analisada. Seus atributos e estados são associados às grandezas físicas a serem visualizadas. Diferentes formas de visualização são associadas às classes, aumentando assim o reuso e facilitando o desenvolvimento de sistemas supervisórios de aplicações complexas.  A proposta conceitual desenvolvida foi implementada experimentalmente como uma extensão à ferramenta SIMOO-RT, tendo sido denominada de “Supervisory Designer”. A ferramenta desenvolvida estende o modelo de objetos e classes de SIMOO-RT, permitindo a adição de informações específicas para supervisão – tais como as definições de limites para os atributos. A ferramenta foi validada através do desenvolvimento de estudos de casos de aplicações industriais reais, tendo demonstrado diversas vantagens quando comparada com o uso de ferramentas para construção de sistemas supervisórios disponíveis comercialmente).|http://hdl.handle.net/10183/2732
Consultando fontes de dados XML heterogêneas através de modelos conceituais|2003|Open Access|Dissertação|Programação;XML (Linguagem de marcação);Linguagens : Consulta|por|XML é um padrão da W3C largamente utilizado por vários tipos de aplicações para representação de informação semi-estruturada e troca de dados pela Internet. Com o crescimento do uso de XML e do intercâmbio de informações pela Internet é muito provável que determinadas aplicações necessitem buscar uma mesma informação sobre várias fontes XML relativas a um mesmo domínio de problema. No intuito de representar a informação dessas várias fontes XML, o programador é obrigado a escolher entre muitas estruturas hierárquicas possíveis na criação dos esquemas de seus documentos XML. Um mesmo domínio de informação, desta maneira, pode então ser representado de diferentes formas nas diversas fontes de dados XML existentes. Por outro lado, verifica-se que as linguagens de consulta existentes são fortemente baseadas no conhecimento da estrutura de navegação do documento. Assim, ao consultar uma determinada informação semanticamente equivalente em várias fontes é necessário conhecer todos os esquemas envolvidos e construir consultas individuais para cada uma dessas fontes.  Em um ambiente de integração, entretanto, é possível gerar um modelo global que representa essas fontes. Ao construir consultas sobre um modelo global, temos consultas integradas sobre estas fontes. Para se atingir esse objetivo, contudo, devem ser resolvidos os problemas da heterogeneidade de representação dos esquemas XML. Dessa forma, com base em uma abordagem de integração de esquemas, o presente trabalho define a linguagem de consulta CXQuery (Conceptual XQuery) que possibilita a construção de consultas sobre um modelo conceitual. Para possibilitar o retorno dos dados das fontes nas consultas feitas em CXQuery, foi desenvolvido um mecanismo de tradução da linguagem CXQuery para a linguagem de consulta XQuery 1.0. A linguagem XQuery 1.0 é umas das linguagens mais utilizadas para o acesso as fontes XML e permite que os dados possam ser retornados ao usuário. Para possibilitar essa tradução, foi definida uma metodologia de representação da informação de mapeamento através de visões XPath. Essa metodologia é relativamente eficaz no mapeamento das diferentes representações das fontes XML.|http://hdl.handle.net/10183/2733
Um metamodelo da linguagem de modelagem real time UML, de suporte à criação de dicionário de dados para ferramentas de modelagem de sistema tempo real, visando a verificação de consistência dos modelos|2000|Open Access|Dissertação|Engenharia : Software;Sistemas : Tempo real;Orientacao : Objetos;Uml|por|A computação de tempo real é uma das áreas mais desafiadoras e de maior demanda tecnológica da atualidade. Está diretamente ligada a aplicações que envolvem índices críticos de confiabilidade e segurança. Estas características, inerentes a esta área da computação, vêm contribuindo para o aumento da complexidade dos sistemas tempo real e seu conseqüente desenvolvimento. Isto fez com que mecanismos para facilitar especificação, delimitação e solução de problemas passem a ser itens importantes para tais aplicações. Este trabalho propõe mecanismos para atuarem no desenvolvimento de sistemas de tempo real, com o objetivo de serem empregados como ferramenta de apoio no problema da verificação de presença de inconsistências, que podem vir a ocorrer nos vários modelos gerados partir da notação da linguagem de modelagem gráfica para sistemas de tempo real - UML-RT(Unified Modeling Language for Real Time). Estes mecanismos foram projetados através da construção de um metamodelo dos conceitos presentes nos diagramas de classe, de objetos, de seqüência, de colaboração e de estados.  Para construir o metamodelo, utiliza-se a notação do diagrama de classes da UML (Unified Modeling Language). Contudo, por intermédio das representações gráficas do diagrama de classes não é possível descrever toda a semântica presente em tais diagramas. Assim, regras descritas em linguagem de modelagem OCL (Object Constraint Language) são utilizadas como um formalismo adicional ao metamodelo. Com estas descrições em OCL será possível a diminuição das possíveis ambigüidades e inconsistências, além de complementar as limitações impostas pelo caráter gráfico da UML. O metamodelo projetado é mapeado para um modelo Entidade&Relacionamento. A partir deste modelo, são gerados os scripts DDL (Data Definition Language) que serão usados na criação do dicionário de dados, no banco de dados Oracle. As descrições semânticas escritas através de regras em OCL são mapeadas para triggers, que disparam no momento em que o dicionário de dados é manipulado. O MET Editor do SiMOO-RT é a ferramenta diagramática que faz o povoamento dos dados no dicionário de dados. SiMOO-RT é uma ferramenta orientada a objetos para a modelagem, simulação e geração automática de código para sistemas de tempo real.|http://hdl.handle.net/10183/2736
Particionamento de domínio e balanceamento de carga no modelo HIDRA|2003|Open Access|Tese|Simulação;Processamento paralelo;Análise numérica;Mecanica : Fluidos;Balanceamento : Carga|por|A paralelização de aplicaçõpes envolvendo a solução de problemas definidos sob o escopo da Dinâmica dos Fluidos Computacional normalmente é obtida via paralelismo de dados, onde o domínio da aplicação é dividido entre os diversos processadores, bem como a manutenção do balancecamento durante a execução é um problema complexo e diversas heurísticas têm sido desenvolvidas. Aplicações onde a simulação é dividida em diversas fases sobre partes diferentes do domínio acrescentam uma dificuldade maior ao particionamento, ao se buscar a distirbuição equlibrada das cargas em todas as fases. este trabalho descreve a implementação de mecanismos de particionamento e balanceamento de carga em problemas multi-fase sobre clusters de PCs. Inicialmente é apresentada a aplicação desenvolvida, um modelo de circulação e transporte de susbtâncias sobre corpos hídricos 2D e 3 D, que pode ser utilizado para modelar qualquer corpo hídrico a partir da descrição de sua geometria, batimetria e condições de contorno. Todo o desenvolvimento e testes do modelo foi feito utilizando como caso de estudo o domínio do Lago Guaíba, em Porto Alegre. Após, são descritas as principais heurísticas de particionamento de domínio de aplicações multi-fase em clusters, bem como mecanismos para balanceamento de carga para este tipo de aplicação. Ao final, é apresentada a solução proposta e desenvolvida, bem como os resultados obtidos com a mesma.|http://hdl.handle.net/10183/2739
VEBit: um algorítmo para codificação de vídeo com escalabilidade|2003|Open Access|Dissertação|Redes;Computadores;Multimídia;Codificação;Vídeo digital;Mpeg|por|A codificação de vídeo de modo a permitir a escalabilidade durante a transmissão tem se tornado um tópico de grande nos últimos anos. Em conjunto com um algorítmo de controle de congestionamento, é possível a criação de um ambiente de transmissão multimída mais apropriado. Esta dissertação apresente um algoritimo de codificação de vídeo escalável baseado em bit-planes. O modelo de codificação do video utiliza 3D-DCT para codificação espacial e temporal e um quantizador escalar semelehante ao empregado no MPEG 4. A técnica de escalabilidade em bit-planes implementada permite a divisão da saída de codificador em taxas complementares e com granularidade fina. Este algorítmo e parte integrante doprojeto SAM (Sistema Adaptativo Multimídia), que busca criar um ambiente de transmissão multimídia adaptativo em tempo real. Este projeto está em desenvolvimento na tese de doutorado de Valter Roesler. O algorítmo proposto e implementado é capaz de transmitir de forma unidirecional vídeos de baixa movimentação. Os resultados dos testes realizados com a implementação feita mostram que a solução proposta é flexível em realação a tecnologia disponível de transmissão através do ajuste no númerp de camadas e suas respectivas especificações de banda.  Os testes realizados apresentaram um desempenho aceitável para codificação e decodificação de vídeo em tempo real. A taxa de compressão apresentou resultados satisfatórios na transmissao em ambientes de baixa velocidade para as camadas inferiores, bem como taxas de transmissão para ambientes em ADSL, cable modem e rede local para as camadas superiores. Com relação a qualidade do vídeo, esta varia de acordo com o grau de movimentação do mesmo. Por exemplo, no modelo ""talking-head"", comum em videoconferências, a qualidade se mostrou viável para ambientes de baixa velocidade (56 kbits/s).|http://hdl.handle.net/10183/2742
Hidrogenação de catalítica de acetileno sobre catalisadores de paládio e catalisadores bimetálicos de paládio e prata|2002|Open Access|Dissertação|Catalisadores : Paládio;Hidrogenação catalítica;Acetileno|por|Nesse estudo, prepararam-se catalisadores de paládio e catalisadores de paládio e prata que foram avaliados, juntamente com o catalisador comercial G58H para a reação de hidrogenação seletiva de acetileno. Foram preparados materiais com 0,03% e 0,1% de Pd depositados sobre α-Al2O3 e α-Al2O3 impregnada com 0,2% de Ag. A superfície dos catalisadores foi analisada por quimissorção de CO, TPR de H2 e medidas de área superficial pelo método BET. Os catalisadores de paládio e paládio/prata mostraram-se bastante ativos e seletivos para a reação de hidrogenação de acetileno. Os resultados dos ensaios de quimissorção de CO indicaram que os catalisadores monometálicos preparados a partir do precursor Pd(NO3)2 possuem maior dispersão e que esta diminui com o aumento do conteúdo metálico. A adição da prata diminui a dispersão do paládio nos catalisadores bimetálicos. As análises de redução a temperatura programada (TPR) indicaram que os catalisadores monometálicos de paládio preparados a partir do precursor Pd(NO3)2 apresentam um número maior de espécies redutíveis. No caso dos catalisadores bimetálicos, as análises de TPR permitiram a identificação de fases metálicas onde o paládio interage com a prata. prata. As medidas de área superficial sugeriram que a adição da fase metálica reduz a área superficial dos catalisadores. Os ensaios de atividade catalítica mostraram que os catalisadores mono e bimetálicos preparados a partir do precursor Pd(NO3)2 são mais ativos que os preparados a partir do complexo [Pd(acac)2]. A atividade aumenta com percentual de paládio, porém, as menores seletividades foram obtidas para os catalisadores com maior percentual de paládio. Os catalisadores bimetálicos de Pd/Ag mostraram-se menos ativos que os monometálicos de paládio, entretanto foram mais seletivos na formação de etileno.|http://hdl.handle.net/10183/2744
Um Estudo dos efeitos organizacionais e sociais da utilização da tecnologia Groupware Lotus Notes na administração pública do Estado do Paraná|2000|Open Access|Dissertação|Sistemas : Informação;Trabalho cooperativo;Internet;Intranet;Lotus notes;Informatica : Administracao publica|por|Em 1995, o Estado do Paraná escolheu o software Lotus Notes como solução de correio eletrônico distribuído e automação de escritórios para suas estruturas organizacionais. Após 5 anos do início do processo de sua implantação, a previsão inicial de 3.600 licenças de uso dessa tecnologia groupware já foi amplamente superada. Existem atualmente mais de 9.000 usuários do Lotus Notes, distribuídos em praticamente todos os níveis da administração pública. Essa dissertação é o primeiro estudo formal realizado para avaliar os impactos organizacionais e sociais produzidos pela utilização de tecnologias de informação colaborativas na administração pública do Paraná. Os capítulos que fazem parte dessa dissertação focalizam em tecnologias de informação colaborativas, com ênfase no Lotus Notes e a sua utilização na administração pública. Inicialmente são introduzidos conceitos básicos sobre Groupware e CSCW (Computer-Supported Cooperative Work), em seguida é apresentado o Lotus Notes, com um breve histórico de sua criação e uma visão resumida de pesquisas acadêmicas sobre implementações dessa tecnologia. Na seqüência, o ambiente de informática da administração pública do Estado do Paraná e a questão da seleção de uma plataforma tecnológica para resolver o problema de comunicação nesse ambiente são descritos, assim como o processo de implantação do Lotus Notes no Estado.  A parte principal desse trabalho é apresentada em seguida: uma ampla pesquisa realizada nos órgãos da administração pública sobre a utilização do Lotus Notes, operacionalizada através de questionários virtuais e entrevistas. O modelo referencial utilizado na pesquisa é mostrado, assim como seu embasamento conceitual e as variáveis consideradas nas questões. Na conclusão são comentados os principais resultados do trabalho realizado, as contribuições e benefícios de sua realização, assim como sugestões de continuidade para futuras pesquisas sobre o assunto.|http://hdl.handle.net/10183/2750
Ferramenta de injeção de falhas para avaliação de segurança|2003|Open Access|Dissertação|Redes : Computadores;Seguranca : Redes : Computadores;Injecao : Falhas|por|Nos ultimos anos, com a crescente popularização das redes de computadores baseadas no protocolo IP, desde pequenas redes até metropolitanas começaram a se agrupar e a fazer do que hoje se conhece como a rede mundial de computadores.Apesar dos benefícios de comunicação e troca de informação da Internet, esse feômeno global também trouxe problemas de segurança, pois a origem e estrutura dos protocolos utilizados na comunicação entre as diversas máquinas limitam as possibilidades de prevenir, identificar ou detectar possíveis ataques ou intrusos. Assim, várias ferramentas surgiram para prevenir e auxiliar na tarefa de identicar problemas de segurança nas redes como firewalls, sniffers e sistemas de detecção de intrusão. Apesar dos benefícios trazidos por essas novas tecnologias, surgiram muitas dúvidas referentes a segurança que esses recursos proporcionam. Afinal, o desenvolvimento e validação desses sistemas são procedimentos bastante complexos e, freqüentemente, esses sitemas têm se tornado o alvo primário de um atacante. O resultado disso, não raramente, é uma falsa noção de segurança devido à utilização inadequada desses mecanismos, o que é, normalmente, mais prejudicial do que a simples inexistência de segurança em uma organização, mas cujas falhas são conhecidas por seus administradores.  A realização de testes para verificação da segurança de uma organização é uma atividade fundamental a fim de alcançar um ambiente operacional seguro e de verificar a correta aplicação dos requisitos de segurança de uma organização.O uso de testes permite a uma empresa verificar com precisão a postura de segurança utilizada em seus sistemas ao mesmo tempo em que permite visualizar a sua rede da mesma maneira que um atacante a visualizaria. Ao visualizar a rede como atacante, pode-se verificar com que facilidade obtém-se informações da rede, quais suas fragilidades e a dificuldade que se tem para invadí-la. Assim, obtém-se uma visão mais realista da segurança de uma organização. Além de técnicas para a avaliação, é muito importante que se possua ferramentas para a realização desses testes. Assim, é possível automotizar a realização de testes e verificar com maior facilidade a existência de problemas em uma rede. A existência de ferramentas que testem sistemas de segurnaça é extremamente importante e necessária, pois, afinal, a segurança de toda uma rede pode depender fortemente de algum desses sistemas. Este trabalho apresenta as técncias existentes para a injecção de falhas visando verificar as que são mais eficientes para a valiação de sistemas de segurança de rede.  Adicionalmente são apresentadas algumas técnicas para o teste de mecanismos de segurança e algumas ferramentas existentes para a realizão de tais testes. A partir desses estudos, é apresentado um modelo de ferramenta adequando as funções de um sistema de injeção de falhas ao teste de mecanismos de segurança em rede. Um protótipo essa ferramenta foi desenvolvido e é apresentado neste trabalho. Esse protótipo permite o envio e o recebimento de pacotes da pilha TCP/IP, podendo testar problemas em protocolos e sistemas utilizados na rede. E, através da utilização de plug-ins, permite que diversos tipos de ataque mais sofisticados possam ser realizados.|http://hdl.handle.net/10183/2752
Estratégia de migração de aplicações legadas visuais (tipo WIMP) para o ambiente Web|2003|Open Access|Dissertação|Engenharia : Software;Engenharia reversa;Reengenharia;Metamodelos;World Wide Web (WWW)|por|O sucesso da Internet como plataforma de distribuição de sistemas de informação encoraja organizações a disponibilizar serviços presentes em seus sistemas legados nesse ambiente. Uma parte desses sistemas foi desenvolvida na fase inicial do desenvolvimento das aplicações cliente/servidor para banco de dados, usando ambientes visuais com interfaces gráficas tipo WIMP, implementadas sob o paradigma procedimental/estruturado, baseado em objetos e eventos. Como conseqüência, produziu-se sistemas legados difíceis de manter, evoluir e adaptar a novas tecnologias e arquiteturas, pois os projetos desenvolvidos não seguiam, na maioria das vezes, os bons preceitos e práticas modernas defendidas na Engenharia de Software. O objetivo deste trabalho é propor uma metodologia para migrar sistemas legados com as características citadas acima para a plataforma Web. O processo de migração proposto destaca duas estratégias: a elaboração de modelos de classes conceituais da aplicação e o tratamento dado à interface do usuário, para serem utilizados na reconstrução de uma nova aplicação. O processo é baseado em técnicas e métodos de engenharia reversa, que visa obter abstrações por meio de análise estática e dinâmica da aplicação. Na análise dinâmica, destaca-se o mecanismo para recuperar aspectos dos requisitos funcionais do sistema legado e representá-los na ferramenta denominada UC/Re (Use Case para Reengenharia).  Todos os artefatos gerados durante o processo podem ser armazenados em um repositório, representando os metamodelos construídos na metodologia. Para delimitar e exemplificar o processo, escolheu-se como domínio de linguagem de programação do software legado, o ambiente Delphi (sob a linguagem Object Pascal). É proposto também um ambiente CASE, no qual é descrito o funcionamento de um protótipo que automatiza grande parte das funcionalidades discutidas nas etapas do processo. Algumas ferramentas desenvolvidas por terceiros são empregadas na redocumentação do sistema legado e na elaboração dos modelos UML do novo sistema. Um estudo de caso, apresentando uma funcionalidade específica de um sistema desenvolvido em Delphi, no paradigma procedimental, é usado para demonstrar o protótipo e serve de exemplo para a validação do processo. Como resultado do processo usando o protótipo, obtém-se o modelo de classes conceituais da nova aplicação no formato XMI (formato padrão para exportação de modelos UML), e gabaritos de páginas em HTML, representando os componentes visuais da interface original na plataforma Web.|http://hdl.handle.net/10183/2753
Um Algoritmo para indução de árvores e regras de decisão|2002|Open Access|Dissertação|Banco : Dados;Descoberta : Conhecimento;Mineracao : Dados;Arvore : Decisao|por|A classificação é uma das tarefas da Mineração de Dados. Esta consiste na aplicação de algoritmos específicos para produzir uma enumeração particular de padrões. Já a classificação é o processo de gerar uma descrição, ou um modelo, para cada classe a partir de um conjunto de exemplos dados. Os métodos adequados e mais utilizados para induzir estes modelos, ou classificadores, são as árvores de decisão e as regras de classificação. As regras e árvores de decisão são populares, principalmente, por sua simplicidade, flexibilidade e interpretabilidade. Entretanto, como a maioria dos algoritmos de indução particionam recursivamente os dados, o processamento pode tornar-se demorado, e a árvore construída pode ser muito grande e complexa, propensa ao overfitting dos dados, que ocorre quando o modelo aprende detalhadamente ao invés de generalizar. Os conjuntos de dados reais para aplicação em Mineração de Dados são, atualmente, muito grandes, e envolvem vários milhares de registros, sendo necessária, também, uma forma de generalizar estes dados.  Este trabalho apresenta um novo modelo de indução de classificadores, em que o principal diferencial do algoritmo proposto é a única passada pelo conjunto de treinamento durante o processo de indução, bem como a sua inspiração proveniente de um Sistema Multiagente. Foi desenvolvido um protótipo, o Midas, que foi validado e avaliado com dados de repositórios. O protótipo também foi aplicado em bases de dados reais, com o objetivo de generalizar as mesmas. Inicialmente, foi estudado e revisado o tema de Descoberta de Conhecimento em Bases de Dados, com ênfase nas técnicas e métodos de Mineração de Dados. Neste trabalho, também são apresentadas, com detalhes, as árvores e regras de decisão, com suas técnicas e algoritmos mais conhecidos. Finalizando, o algoritmo proposto e o protótipo desenvolvido são apresentados, bem como os resultados provenientes da validação e aplicação do mesmo.|http://hdl.handle.net/10183/2755
Um verificador seguro de integridade de arquivos|2002|Open Access|Dissertação|Sistemas operacionais;Integridade : Arquivos;Linux|por|Este trabalho apresenta um modelo de mecanismo para o controle de integridade de arquivos em sistemas operacionais, com a capacidade de bloquear o acesso à arquivos inválidos de forma a conter os danos causados por possíveis ataques. O modelo foi inicialmente formulado a partir do estudo detalhado do processo de controle de integridade, revelando diversos pontos críticos de segurança nele existentes, e da avaliação dos mecanismos atualmente implementados nos sistemas operacionais que oferecem, mesmo que indiretamente, algum tipo de garantia de integridade dos arquivos do sistema. Durante o seu desenvolvimento, a segurança do próprio modelo foi detalhadamente analisada de forma a enumerar os seus pontos críticos e possíveis soluções a serem empregadas, resultando na definição dos requisitos mínimos de segurança que devem ser encontrados nesse tipo de mecanismo. Em seguida, visando a validação do modelo especificado e decorrente disponibilização do mecanismo para uso prático e em estudos futuros, um protótipo foi implementado no sistema operacional GNU/Linux. Procurando confirmar a sua viabilidade, foram realizadas análises do impacto causado sobre o desempenho do sistema. Por fim, foi confirmada a importância e viabilidade do emprego do modelo apresentado como mecanismo adicional de segurança em sistemas operacionais. Além disso, foi colocado em evidência um campo de pesquisa ainda pouco explorado e portanto atrativo para a realização de novos estudos.|http://hdl.handle.net/10183/2756
Evolução de esquemas de workflow representados em XML|2003|Open Access|Dissertação|Sistemas : Informação;Sistemas : Workflow;XML (Linguagem de marcação);Modelagem : Workflow|por|Sistemas de gerência de workflow estão sendo amplamente utilizados para a modelagem e a execução dos processos de negócios das organizações. Tipicamente, esses sistemas interpretam um workflow e atribuem atividades a participantes, os quais podem utilizar ferramentas e aplicativos para sua realização. Recentemente, XML começou a ser utilizada como linguagem para representação dos processos bem como para a interoperação entre várias máquinas de workflow. Os processos de negócio são, na grande maioria, dinâmicos, podendo ser modi- ficados devido a inúmeros fatores, que vão desde a correção de erros até a adaptação a novas leis externas à organização. Conseqüentemente, os workflows correspondentes devem também evoluir, para se adequar às novas especificações do processo. Algumas propostas para o tratamento deste problema já foram definidas, enfocando principalmente as alterações sobre o fluxo de controle. Entretanto, para workflows representados em XML, ainda não foram definidos mecanismos apropriados para que a evolução possa ser realizada.  Este trabalho apresenta uma estratégia para a evolução de esquemas de workflow representados em XML. Esta estratégia é construída a partir do conceito de versionamento, que permite o armazenamento de diversas versões de esquemas e a consulta ao hist´orico de versões e instâncias. As versões são representadas de acordo com uma linguagem que considera os aspectos de evolução. As instâncias, responsáveis pelas execuções particulares das versões, também são adequadamente modeladas. Além disso, é definido um método para a migração de instâncias entre versões de esquema, no caso de uma evolução. A aplicabilidade da estratégia proposta é verificada por meio de um estudo de caso.|http://hdl.handle.net/10183/2761
WTROPIC : um gerador automático de macro células CMOS acessível via WWW|2001|Open Access|Dissertação|Microeletrônica;Layout : Circuitos integrados|por|Este trabalho apresenta a pesquisa e o desenvolvimento da ferramenta para geração automática de leiautes WTROPIC. O WTROPIC é uma ferramenta para a geração remota, acessível via WWW, de leiautes para circuitos CMOS adequada ao projeto FUCAS e ao ambiente CAVE. O WTROPIC foi concebido a partir de otimizações realizadas na versão 3 da ferramenta TROPIC. É mostrado também, como as otimizações no leiaute do TROPIC foram implementadas e como essas otimizações permitem ao WTROPIC cerca de 10% de redução da largura dos circuitos gerados em comparação ao TROPIC. Como o TROPIC, o WTROPIC é um gerador de macro células CMOS independente de biblioteca. Apresenta-se também, como a ferramenta WTROPIC foi integrada ao ambiente de concepção de circuitos CAVE, as mudanças propostas para metodologia de integração de ferramentas do CAVE que conduzem a uma melhora na qualidade de integração e a padronização das interfaces de usuário e como a síntese física de um leiaute pode ser então realizada remotamente. Dessa maneira, obteve-se uma ferramenta para a concepção de leiautes disponível a qualquer usuário com acesso a internet, mesmo que esse usuário não disponha de uma máquina com elevada capacidade de processamento, normalmente exigido por ferramentas de CAD.|http://hdl.handle.net/10183/2778
Materialização de visões XML|2001|Open Access|Dissertação|Banco : Dados;Banco : Dados relacionais;XML (Linguagem de marcação);Visoes : Banco : Dados;Dados semi-estruturados|por|A grande quantidade de dados eletrônicos disponível atualmente nem sempre pode ser representada com modelos tradicionais, principalmente devido à ausência de esquema no momento da criação destes dados. Neste sentido, modelos semi-estruturados têm sido propostos; uma das abordagens utilizadas é XML, uma linguagem para troca e representação deste tipo de informação. Entretanto, consultar dados semi-estruturados pode demandar processos de extração com alto custo. Uma das alternativas para solucionar este problema é a definição de visões sobre estes dados, e a posterior materialização destas informações. O uso de visões materializadas para dados XML ainda é pouco explorado. Uma das abordagens que podem ser utilizadas é o uso de sistemas de gerenciamento de bancos de dados relacionais para o armazenamento das visões. Desse modo, informação semanticamente relacionada (informação acerca de um mesmo domínio, possivelmente representada em formatos diferentes) pode ser agrupada em uma única unidade lógica, facilitando o acesso a estes dados por parte do usuário, e introduzindo alguma estrutura nos dados semiestruturados. Dessa maneira, o usuário final submete consultas diretamente sobre a visão materializada, evitando extrações contínuas de dados nas fontes XML. A materialização de dados XML exige a definição de um repositório de dados para o armazenamento destas instâncias. Utilizando-se a abordagem relacional, é necessário definir um mecanismo para a geração do esquema lógico do banco de dados. Consultar os dados nas fontes XML exige a integração destas instâncias.  Neste contexto, integrá-las significa identificar quais instâncias de dados representam o mesmo objeto do mundo real, bem como resolver ambigüidades de representação deste objeto. O problema de identificação de entidades em XML é mais complexo que em bases de dados estruturadas. Dados XML, como propostos originalmente, não carregam necessariamente a noção de chave primária ou identificador de objeto. Assim, é necessária a adoção de um mecanismo que faça a identificação das instâncias na integração destes dados. Além disso, à medida que as fontes de dados XML sofrem alterações, a visão materializada deve ser atualizada, a fim de manter-se consistente com as fontes de dados. A manutenção deve propagar as alterações feitas nos dados XML para a visão materializada. Reprocessar todo o conteúdo da visão materializada é, na maioria das vezes, muito caro. Assim, é desejável propagar as mudanças incrementalmente, ou seja, processar apenas as alterações necessárias. Neste sentido, o presente trabalho apresenta uma proposta de técnica para armazenamento de dados XML em um banco de dados relacional. A proposta utiliza ontologias para a geração do esquema lógico do banco de dados. O problema de integração de dados é mostrado. O foco principal do trabalho está na proposta de uma técnica de atribuição de identificadores a instâncias XML, baseada no uso de funções Skolem e no padrão XPath, proposto pelo W3C. Também é proposto um mecanismo para manutenção incremental deste banco, à medida que as fontes XML sofrem atualizações.|http://hdl.handle.net/10183/2779
Proposta de um framework conceitual para apoiar a criação de técnicas de indexação para banco de dados temporais|2002|Open Access|Dissertação|Banco : Dados;Banco : Dados temporais;Framework;Indexação|por|Bancos de Dados Temporais (BDTs) surgiram para tentar suprir a necessidade de se obter um melhor aproveitamento das informações que circulam atualmente. Porém, ao mesmo tempo em que é benéfico o seu uso, uma vez que armazenam o histórico das informações, existe um problema neste tipo de banco de dados, que é o desempenho. Além do grande volume de dados armazenados, este problema se agrava ainda mais devido à complexidade nas operações que governam os BDTs, como por exemplo, inclusão, remoção, alteração e consulta. Portanto, focalizando o problema, existe a necessidade de melhorar o desempenho dos BDTs frente às operações de manipulação de dados. Técnicas de indexação apropriadas para dados temporais podem amenizar este problema de desempenho. Técnicas consagradas de indexação são largamente usadas, amparadas no seu alto grau de desempenho e portabilidade. São exemplos B-Tree, B+-Tree e R-Tree, entre outras. Estas técnicas não suportam indexar os complexos BDTs, mas são fundamentais para que sirvam de base para novas estruturas que suportem esses tipos de dados. As técnicas de indexação para dados temporais existentes não conseguem suprir a semântica temporal na sua totalidade. Existem ainda algumas deficiências do tipo: poucas técnicas que abrangem ao mesmo tempo tempo de validade e tempo de transação; não existe uma técnica que oferece informações do seu desempenho; a maioria não distingue ponto no tempo de intervalo de tempo; entre outras.  Entretanto, possuem características relevantes em cada uma delas. Assim, um estudo das características mais importantes se tornou um fator importante para que possa ser desenvolvido um modelo capaz de auxiliar na criação de novas técnicas de indexação para dados temporais, a fim de contemplar melhor estes tipos de dados. O objetivo deste trabalho é, com base nas características das técnicas estudadas, desenvolver um framework conceitual capaz de auxiliar na criação de novas técnicas de indexação para dados temporais. Esta estrutura apresenta as características mais relevantes das técnicas existentes, agregando novas idéias e conceitos para contemplar os dados temporais. O framework conceitual desenvolvido agrega características de diferentes técnicas de indexação, possibilitando de variar a arquitetura de um índice para dados temporais, ajustando-os para um melhor desempenho em diferentes sistemas. Para validar o framework proposto é apresentada uma especificação de índices para o modelo de dados TF-ORM (Temporal Functionality in Objects With Roles Model).|http://hdl.handle.net/10183/2780
Gerenciamento distribuído e flexível de protocolos de alto nível, serviços e aplicações em redes de computadores|2002|Open Access|Tese|Redes : Computadores;Gerencia : Redes : Computadores;Protocolos : Redes : Computadores|por|As redes de computadores experimentam um grande crescimento não apenas em tamanho, mas também no número de serviços oferecidos e no número de protocolos de alto nível e aplicações que são executados sobre elas. Boa parte desses software (ex.: ICQ e Napster), em geral, não está diretamente ligada a aplicações críticas, mas o seu uso não controlado pode degradar o desempenho da rede. Para que se possa medir o impacto dos mesmos sobre a infra-estrutura, mecanismos de gerência ligados à contabilização e caracterização de tráfego são desejáveis. Por outro lado, alguns protocolos, serviços e aplicações (ex. servidores DNS e Web) suportam aplicações e precisam ser monitorados e gerenciados com maior atenção. Para essa classe de software de rede, a simples contabilização e caracterização de tráfego não é suficiente; tarefas de gerência como teste de serviços, detecção e manipulação de falhas, medição de desempenho e detecção de intrusão são importantes para garantir alta disponibilidade e eficiência da rede e aplicações. As ferramentas existentes para essa finalidade são, entre aspectos, (a) não integradas (necessidade de uma ferramenta para monitorar cada aplicação), (b) centralizadas (não oferecem suporte à distribuição de tarefas de gerenciamento) e (c) pouco flexíveis (dificuldade em gerenciar novos protocolos, serviços e aplicações). Nesse contexto, a tese propõe uma arquitetura, centrada na monitoração passiva em temporal do tráfego de rede, para gerenciamento distribuído de protocolos de alto nível, serviços e aplicações em rede.  Baseada da MIB (Management Information Base) Script do IETF (Internet Engineering Task Force), a arquitetura Trace oferece mecanismos para a delegação de tarefas de gerenciamento a gerentes intermediários, que interagem com agentes de monitoração e agentes de ação para executá-las. A tese propõe também PTSL (Protocol Trace Specification Language), uma linguagem gráfica/textual criada para permitir que gerentes de rede especificam as interações de protocolos (traços) que lhes interessam monitorar. As especificações são usadas pelso gerentes intermediários para programar os agentes de monitoração. Uma vez programadas, esses agentes passam a monitorar a ocorrência dos traços.As informações obtidas são analisadas pelos agentes intermediários, que podem requisitar de ação a execução de procedimentos (ex: scripts Perl), possibilitando a automação de diversas tarefas de gerenciamento. A arquitetura proposta é validada por um protótipo: a plataforma de gerenciamento Trace.|http://hdl.handle.net/10183/2782
Uma Metodologia para o desenvolvimento de aplicações de visão computacional utilizando um projeto conjunto de hardware e software|2001|Open Access|Tese|Computação gráfica;Visão computacional;Redes neurais;Processamento de imagens;FPGA|por|As tarefas de visão computacional incentivam uma significativa parte da pesquisa em todas as áreas científicas e industriais, entre as quais, cita-se a área voltada para o desenvolvimento de arquiteturas de computadores. A visão computacional é considerada um dos problemas mais desafiadores para a computação de alto desempenho, pois esta requer um grande desempenho, bem como um alto grau de flexibilidade. A flexibilidade é necessária pois a visão computacional abrange aplicações em que há diferentes tarefas a serem realizadas com diferentes necessidades de desempenho. Esta flexibilidade é particularmente importante em sistemas destinados a atuar como ambientes experimentais para novas técnicas de processamento visual ou para a prototipação de novas aplicações. Computação configurável tem demonstrado, por meio de exemplos implementados pela comunidade científica, fornecer uma boa relação entre alto desempenho e flexibilidade necessária para a implementação de diferentes técnicas utilizadas na área de visão computacional. Contudo, poucos esforços de pesquisa têm sido realizados na concepção de sistemas completos visando a solução de um problema de visão computacional, incluindo ambos os requisitos de software e de hardware.  O principal objetivo deste trabalho é mostrar que as técnicas e tecnologias disponíveis na área de computação configurável podem ser empregadas para a concepção de um sistema capaz de implementar um grande número de aplicações da área de visão computacional na pesquisa e no ambiente industrial. Entretanto, não é escopo deste trabalho implementar um sistema de computação que seja suficiente para abordar os requerimentos necessários para todas as aplicações em visão computacional, mas os métodos aqui introduzidos podem ser utilizados como uma base geral de implementação de várias tarefas de visão computacional. Este trabalho utiliza ambientes que permitem implementações conjuntas de hardware e software, pois os mesmos facilitam a validação das técnicas aqui apresentadas, por meio da implementação de um estudo de caso, sendo parte deste estudo de caso implementado em software e outra parte em hardware.|http://hdl.handle.net/10183/2783
Intercâmbio de dados entre aplicativos utilizando XML/XSLT|2001|Open Access|Dissertação|Engenharia : Software;Framework;Intercâmbio : Dados;XML (Linguagem de marcação);XSLT|por|A integração de aplicações heterogêneas é uma tarefa constante entre empresas do mundo moderno. A grande quantidade de fornecedores de software, aliada à extensa variedade de técnicas e linguagens computacionais utilizadas, fazem desta integração uma tarefa trabalhosa e cara para as organizações. As alternativas existentes para a integração de sistemas de diferentes fornecedores podem variar, desde acesso compartilhado a uma base de dados comum, uso de replicadores de dados entre bases de dados distintas, troca de mensagens entre aplicações, ou o uso de programas exportadores/importadores, gerando arquivos em um formato previamente protocolado entre os desenvolvedores dos softwares envolvidos. Este trabalho visa propor uma alternativa para a integração de sistemas heterogêneos, fazendo uso da tecnologia XML para representar os dados que são trocados entre os aplicativos. Para tanto, sugere um framework a ser utilizado no planejamento da arquitetura dos softwares. O objetivo principal da adoção de um framework é a utilização de uma metodologia previamente desenvolvida e certificada, economizando tempo de análise para a solução de um problema.  O framework proposto subtrai dos desenvolvedores a necessidade de alteração do código fonte dos seus programas cada vez que a integração com um novo fornecedor de software se faz necessária, ou que há alteração no formato dos dados trocados entre os aplicativos. Este efeito é conseguido através da utilização de XSLT para a conversão de formatos de documentos XML trocados pelos softwares. Tal conversão é realizada por um processador XSLT externo aos programas envolvidos. Para simplificar o processo, foi desenvolvido o protótipo de uma ferramenta para a geração de templates XSLT. Templates são elementos da especificação XSLT capazes de realizar a transformação entre estruturas representadas em XML. O gerador de templates XSLT é uma ferramenta gráfica capaz de converter mapeamentos realizados entre estruturas XML em templates XSLT, podendo aplicar as transformações geradas a documentos XML, com a finalidade de teste ou transformação.|http://hdl.handle.net/10183/2826
ANAC : uma ferramenta para a automatização da análise da complexidade de algoritmos|2001|Open Access|Dissertação|Teoria : Ciência : Computação;Complexidade : Algoritmos|por|A análise de um algoritmo tem por finalidade melhorar, quando possível, seu desempenho e dar condições de poder optar pelo melhor, dentre os algoritmos existentes, para resolver o mesmo problema. O cálculo da complexidade de algoritmos é muito dependente da classe dos algoritmos analisados. O cálculo depende da função tamanho e das operações fundamentais. Alguns aspectos do cálculo da complexidade, entretanto, não dependem do tipo de problema que o algoritmo resolve, mas somente das estruturas que o compõem, podendo, desta maneira, ser generalizados. Com base neste princípio, surgiu um método para o cálculo da complexidade de algoritmos no pior caso. Neste método foi definido que cada estrutura algorítmica possui uma equação de complexidade associada. Esse método propiciou a análise automática da complexidade de algoritmos. A análise automática de algoritmos tem como principal objetivo tornar o processo de cálculo da complexidade mais acessível. A união da metodologia para o pior caso, associada com a idéia da análise automática de programas, serviu de motivação para o desenvolvimento do protótipo de sistema ANAC, que é uma ferramenta para análise automática da complexidade de algoritmos não recursivos. O objetivo deste trabalho é implementar esta metodologia de cálculo de complexidade de algoritmos no pior caso, com a utilização de técnicas de construção de compiladores para que este sistema possa analisar algoritmos gerando como resultado final a complexidade do algoritmo dada em ordens assintóticas.|http://hdl.handle.net/10183/2827
Qualidade de serviço em redes IP serviços diferenciados|2001|Open Access|Dissertação|Redes : Computadores;IP;Trafego : Redes : Computadores|por|Este trabalho tem como finalidade apresentar uma visão geral dos assuntos relacionados à definição, configuração e gerenciamento dos serviços habilitados pela arquitetura de Serviços Diferenciados em relação à priorização de tráfego. Nele, descrevemos as necessidades por QoS nas redes atuais e futuras, bem como o direcionamento da Internet 2 em relação ao QoS, mais especificamente voltada ao mecanismo DiffServ. Também são destacados os mecanismos de QoS para redes IP do fornecedor Cisco Systems, o qual possui a maior gama de mecanismos já implantados pelos fornecedores. Por fim, identificamos as características necessárias para participar do QBone, um testbed para Serviços Diferenciados.|http://hdl.handle.net/10183/2828
Geração de código no projeto de sistemas reativos a partir da linguagem RS|2000|Open Access|Dissertação|Programação;Sistemas reativos;RS|por|A linguagem síncrona RS é destinada ao desenvolvimento de sistemas reativos. O presente trabalho tem como objetivo criar meios que facilitem o uso da linguagem RS no projeto e implementação desses sistemas, permitindo que, à partir da especificação de um sistema reativo, seja realizada a sua implementação de forma automática. Deste modo, a linguagem RS é utilizada para a descrição do comportamento de um sistema em um alto nível de abstração, antes de serfeitas a decomposição do sistema em componentes de software ou hardware. A implmentação do protótipo do sistema computacional dedicado é obtida através de uma síntese automática desse modelo de alto nível. Foram implementados geradores de código que utilizam o código objeto fornecido pelo compilador da linguagem RS. Os geradores fazem a tradução para a linguagem C, para a linguagem JAVA, ou para a linguagem de descrição de hardware VHDL. A partir da síntese desses códigos poderá ser obtida a implementação do sistema em um micrcoomputador comercial, em um microcomputador Java de dedicado (ASIP Java), ou em um hardware de aplicação específica (ASIC). Foram realizados estudos de caso representativos dos sistemas reativos embaraçados e de tempo rel. Estes estudos de caso serviram para validar os geradores de código bem como para analisar o uso da linguagem RS no projeto e implementação desses sistemas.|http://hdl.handle.net/10183/2829
Simulação de escoamentos incompressíveis não newtonianos em dutos com expansão brusca|2003|Open Access|Dissertação|Física matemática;Dinâmica dos fluidos;Escoamentos incompressíveis|por|O objetivo deste trabalho é a simulação numérica de escoamentos incompressíveis bidimensionais em dutos com expansão brusca, considerando o raio de expansão de 3 : 1. As equações governantes implementadas são as de Navier, que junto com relações constitutivas para a tensão visam representar comportamentos não newtonianos. A integração temporal é feita usando o esquema explícito de Runge-Kutta com três estágios e de segunda ordem; as derivadas espaciais são aproximadas pelo método de diferenças finitas centrais. Escoamentos em expansões bruscas para fluidos newtonianos apresentam um número de Reynolds crítico, dependente do raio de expansão, na qual três soluções passam a ser encontradas: uma solução sim étrica instável e duas soluções assimétricas rebatidas estáveis. Aumentando o número de Reynolds, a solução passa a ser tridimensional e dependente do tempo. Dessa forma, o objetivo é encontrar as diferenças que ocorrem no comportamento do fluxo quando o fluido utilizado possui características não newtonianas.  As relações constitutivas empregadas pertencem à classe de fluidos newtonianos generalizados: power-law, Bingham e Herschel-Bulkley. Esses modelos prevêem comportamentos pseudoplásticos e dilatantes, plásticos e viscoplásticos, respectivamente. Os resultados numéricos mostram diferenças entre as soluções newtonianas e não newtonianas para Reynolds variando de 30 a 300. Os valores de Reynolds críticos para o modelo power-law não apresentaram grandes diferenças em comparação com os da solução newtoniana. Algumas variações foram percebidas nos perfis de velocidade. Entretanto, os resultados obtidos com os modelos de Bingham e Herschel-Bulkley apresentaram diferenças significativas quando comparados com os newtonianos com o aumento do parâmetro adimensional Bingham; à medida que Bingham é aumentado, o tamanho dos vórtices diminui. Além disso, os perfis de velocidade apresentam diferenças relevantes, uma vez que o fluxo possui regiões onde o fluido se comporta como sólido.|http://hdl.handle.net/10183/2851
Comportamento dos contaminantes nos solos e águas subterrâneas em depósito de resíduos do refino de petróleo - Borreiro - Na Refinaria Alberto Pasqualini, RS|2003|Open Access|Dissertação|Geoquímica;Petroleo : Refino : Resíduos contaminados;Águas subterrâneas;Rio Grande do Sul;Contaminação da água;Hidrocarbonetos|por|Resíduos do refino de petróleo dispostos no meio ambiente representam fontes de contaminação por metais pesados e hidrocarbonetos para solos, subsolos, águas superficiais e subterrâneas. O presente estudo foi realizado na área designada Borreiro da Refinaria Alberto Pasqualini, RS – REFAP, onde nas décadas de ‘70 e ‘80 foram depositados resíduos do refino de petróleo (borra) intercalados com camadas de aterros em volume total da ordem de 100.500 m3. O uso da área cessou há cerca de 20 anos, estando os materiais, desde então sujeitos aos mecanismos de atenuação natural. É objetivo do trabalho avaliar a situação da área, relacionando e descrevendo as interações dos sedimentos e das águas subterrâneas com os contaminantes. Para caracterização geológica e hidrogeológica da área foram realizadas 35 sondagens e construídos 6 poços de monitoramento com profundidades de 5 a 7 metros. Visando avaliar a contaminação do meio ambiente pela disposição de borra, foram amostrados os materiais do depósito, bem como os sedimentos cenozóicos que constituem a base do depósito, formada dominantemente por areia fina siltico-argilosa. Também foram coletadas, amostras de águas subterrâneas do aqüífero freático que juntamente com os sedimentos foram analisadas por Espectrofotometria de Absorção Atômica e por Cromatografia Gasosa.  Para caracterização da distribuição espacial e análise da mobilidade dos componentes da borra, foram construídos mapas de isoteores e perfis verticais das sondagens, comparando textura dominante e profundidade de cada amostra com seu conteúdo de contaminante. Foram realizados testes de lixiviação e de solubilização para avaliar a relação dos contaminantes com os materiais, permitindo classificar as amostras de acordo com a NBR-10.004, determinando, assim as zonas mais impactadas da área. A caracterização geológica e hidrogeológica serviu de base de dados para a apresentação da dispersão dos contaminantes no depósito, nos sedimentos cenozóicos e no aqüífero freático. Foi definida a dispersão dos contaminantes orgânicos nas fases, livre, adsorvida, residual, gasosa e dissolvida bem como a distribuição dos metais na superfície, no subsolo e nas águas subterrâneas.|http://hdl.handle.net/10183/2854
Usando bases de dados relacionais para geração semi-automática de ontologias destinadas à extração de dados|2003|Open Access|Dissertação|Armazenamento : Dados;Recuperacao : Informacao;Extracao : Dados;Extração semântica;Ontologias|por|Extração de dados é o processo utilizado para obter e estruturar informações disponibilizaadas em documentos semi-estruturados (ex.: páginas da Web). A importâmncia da extrtação de dados vem do fato que, uma vez extraídos, os dados podem ser armazenados e manipulados em uma forma estruturada. Dentre as abordagens existentes para extração de dados, existe a abordagem de extração baseada em ontologias. Nesta abordagem, ontologias são preciamente criadas para descrever um domínio de interesse, gerando um modelo conceitual enriquecido com informações necessárias para extração de dados das fontes semi-estruturadas. A ontologia é utilizada como guia ara um programa (¨parser¨) que executa a extração de dados dos documentos ou páginas fornecidos como enetrada. Oprocesso de criação da ontologia não é uma tarefa trtivial e requer um cuidado trabalho ee análise dos documentos ou páginas fontes dos dados. Este trabalho é feito manualmente por usuários especialistas no domínio de interesse da ontologia. Entretanto, em algumas situações os dados que se desejam extrair estão modelados em bancos de dados relacionais. Neste caso, o modelo realcional do banco de dados por ser utilizado para constrtução do modelo conceitual na ontologia.  As instâncias dos dados armazenados neste mesmo banco podem ajudar a gerar as informações sobre conteúdo e formato dos dados a serem extraídos. Estas informações sobre conteúdo e formato de dados, na ontologia, são representadas por expressões regulares e estão inseridas nos chamados ""data frames"". O objetivo deste trabalho é apresentar um método para criação semi-automática de ontologias de extração a partir das informações em um banco de dados já existente. O processo é baseado na engenharia reversa do modelo relacional para o modelo conceitual da ontologia combinada com a análise das instâncias dos dados para geração das expressões regulares nos ""data frames"".|http://hdl.handle.net/10183/2857
Terraços da margem leste da Laguna dos Patos, Litoral Médio do Rio Grande do Sul: estratigrafia e evolução holocênica|1999|Open Access|Dissertação|Geologia marinha;Estratigrafia;Terraços lagunares;Litoral, Região (RS)|por|O presente trabalho discute os terraços lagunares da margem leste da Laguna dos Patos, localizados próximo à cidade de Mostardas. Através desse estudo é apresentado um novo modelo evolutivo para os últimos 5.000 anos no Litoral Médio do Rio Grande do Sul. Novas ferramentas foram utilizadas pela primeira vez no estudo da planície costeira do Rio Grande do Sul. Com respeito a aquisição dos dados de subsuperfície foi utilizado com êxito um GPR (Ground Penetretion Radar). Na análise e interpretação dos dados foi feita uma tentativa de aplicação da estratigrafia de seqüências. Através do uso de um scanner foram elaborados perfis dos testemunhos de sondagem, identificando aspectos distintos na composição e porosidade de diferentes litologias. Outras ferramentas utilizadas constam de sensoriamento remoto, nivelamento altimétrico e batimétrico, vibracore e datação radiométrica. Dessa forma, foram definidas três feições de terraceamento lagunar de idade holocênica na área de estudo, cada uma relacionada a um nível lagunar distinto. Através dos dados obtidos em subsuperfície foi possível traçar um esboço cronoestratigráfico, bem como uma curva de variação relativa do nível lagunar para a região. Também, como resultado dos dados geológicos de superfície, associados a novas interpretações baseadas em fotografias aéreas e imagens de satélite, foi elaborado um mapa geológico atualizado.;The present work discusses the wave-cut terraces of the eastern margin of the Patos Lagoon, located close to the city of Mostardas. Through this study a new evolutionary model is presented for the last 5.000 years of the central coast of Rio Grande do Sul. New tools were used for the first time on studies of Rio Grande do Sul Coastal Plain. Regarding acquisition of subsurface data a Ground Penetration Radar (GPR) was successfully used. Sequence stratigraphy was tentatively used for analysis and data interpretation. By the use of a scanner core logs were constructed showing distinct aspects of composition and porosity of different lithologies. Other tools used were remote sensing, altimetrical and bathimetric leveling, vibracore and radiometric dating. By this way, three wave-cut terraces of Holocene age were well defined in the study area, each one related to a different lagoonal level. Through subsurface data it was possible to construct a cronostratigraphic sketch as well as a relative curve of lagoonal level variation for the area. Also, as a result of geological surface data plus new interpretations based on aerial photos and satellite images an updated geological map was elaborated.|http://hdl.handle.net/10183/2865
Mixed-signal analog-digital circuits design on the pre-diffused digital array using trapezoidal association of transistors|2001|Open Access|Tese|VLSI;CMOS analog design;Physical implementation;SOT array;TAT transistors;Analog cells;Sigma-Delta converters;Microeletrônica|eng|The mixed-signal and analog design on a pre-diffused array is a challenging task, given that the digital array is a linear matrix arrangement of minimum-length transistors. To surmount this drawback a specific discipline for designing analog circuits over such array is required. An important novel technique proposed is the use of TAT (Trapezoidal Associations of Transistors) composite transistors on the semi-custom Sea-Of-Transistors (SOT) array. The analysis and advantages of TAT arrangement are extensively analyzed and demonstrated, with simulation and measurement comparisons to equivalent single transistors. Basic analog cells were also designed as well in full-custom and TAT versions in 1.0mm and 0.5mm digital CMOS technologies. Most of the circuits were prototyped in full-custom and TAT-based on pre-diffused SOT arrays. An innovative demonstration of the TAT technique is shown with the design and implementation of a mixed-signal analog system, i. e., a fully differential 2nd order Sigma-Delta Analog-to-Digital (A/D) modulator, fabricated in both full-custom and SOT array methodologies in 0.5mm CMOS technology from MOSIS foundry.  Three test-chips were designed and fabricated in 0.5mm. Two of them are IC chips containing the full-custom and SOT array versions of a 2nd-Order Sigma-Delta A/D modulator. The third IC contains a transistors-structure (TAT and single) and analog cells placed side-by-side, block components (Comparator and Folded-cascode OTA) of the Sigma-Delta modulator.|http://hdl.handle.net/10183/2884
Um Modelo de simulação de processos de software baseado em conhecimento para o ambiente PROSOFT|2001|Open Access|Dissertação|Engenharia de software;Simulacao : Software;Sistemas baseados : Conhecimento;Prosoft;Sistemas multiagentes|por|Construção de software com qualidade tem motivado diversas pesquisas na área de Engenharia de Software. Problemas como a grande complexidade requerida pelas aplicações atuais e a necessidade de gerenciamento de um número cada vez maior de pessoas envolvidas em projetos são obstáculos para serem transpostos. Trabalhos relacionados a tecnologia de processos de software aparecem como uma proposta para se obter maior controle das atividades realizadas com o intuito de se obter maior qualidade. A simulação de processos de software, através da representação dos passos definidos em um modelo, tem sido utilizada no auxílio a gerentes de projetos de sistemas para fornecer-lhes informações preciosas sobre o desenvolvimento de um sistema especificado. A representação de conhecimento a respeito das características relacionadas a um ambiente de desenvolvimento ajuda na obtenção de simulações mais realísticas. A partir do modelo, o simulador obtém uma descrição do ambiente em que deve atuar, baseado no conhecimento que se tem a respeito do ambiente. Esse trabalho apresenta um modelo de simulação de processos de software baseado em conhecimento para ser inserido em um ambiente de engenharia de processos de software.  A função do modelo é simular um processo de software instanciado, procurando detectar inconsistências no mesmo que possam gerar problemas durante a sua execução, como aumento de custos e comprometimento da qualidade do(s) produto(s) obtido(s). Após a simulação o projetista pode constatar a necessidade de se refazer o modelo, ajustar parâmetros ou executar o processo de software. O objetivo da simulação, nesse trabalho, é auxiliar as pessoas responsáveis por um ambiente de desenvolvimento a obter modelos de processos validados. O modelo de simulação foi definido para ser utilizado no ambiente PROSOFT, que é um ambiente de desenvolvimento que permite a integração de novas ferramentas para desenvolvimento de software. O ambiente PROSOFT vem recebendo propostas de extensão que tem contribuído para o seu aprimoramento, fornecendo para seus usuários uma quantidade cada vez maior de ferramentas de auxílio a construção de artefatos de software. As propostas mais recentes foram um modelo para construção de sistemas especialistas, a definição de um ambiente cooperativo e um gerenciador de processos de software. ATOs algébricos (construções do PROSOFT) são utilizados para especificar formalmente o modelo de simulação definido neste trabalho. A validação é realizada através de um modelo em UML (Unified Method Language) que foi utilizado como base para a construção de um programa implementado usando a linguagem Java. Isso ocorre porque a ferramenta do PROSOFT (implementada em Java) que seria utilizada para validar as especificações algébricas ainda não está finalizada.|http://hdl.handle.net/10183/2885
Estimativas do erro nas aproximações de Galerkin para as equações de Navier-Stokes|2002|Open Access|Dissertação|Estimativas de erro;Aproximações de Galerkin;Equações de Navier-Stokes|por|Neste trabalho são provadas algumas estimativas de erro em espaços para as aproximações de Galerkin para a solução do sistema de equações de Navier-Stokes. Mostra-se que o erro decresce em proporção inversa aos autovalores do operador de Stokes.|http://hdl.handle.net/10183/2916
Reconhecimento de voz para comandos de direcionamento por meio de redes neurais|2000|Open Access|Dissertação|Reconhecimento : Padroes;Processamento : Sinais;Processamento : Voz;Reconhecimento : Fala;Redes neurais|por|Este trabalho relata o desenvolvimento de uma aplicação capaz de reconhecer um vocabulário restrito de comandos de direcionamento pronunciados de forma isolada e independentes do locutor. Os métodos utilizados para efetivar o reconhecimento foram: técnicas clássicas de processamento de sinais e redes neurais artificiais. No processamento de sinais visou-se o pré-processamento das amostras para obtenção dos coeficientes cepstrais. Enquanto que para o treinamento e classificação foram utilizadas duas redes neurais distintas, as redes: Backpropagation e Fuzzy ARTMAP. Diversas amostras foram coletadas de diferentes usuários no sentido de compor um banco de dados flexível para o aprendizado das redes neurais, que garantisse uma representação satisfatória da grande variabilidade que apresentam as pronúncias entre as vozes dos usuários. Com a aplicação de tais técnicas, o reconhecimento demostrou-se eficaz, distinguindo cada um dos comandos com bons índices de acerto, uma vez que o sistema é independente do locutor.|http://hdl.handle.net/10183/2947
Filmes finos dielétricos para a tecnologia do silício : processamento térmico e caracterização|2003|Open Access|Tese|Filmes finos dieletricos;Silício;Alumínio;Zircônia;Óxidos;Tratamento térmico;Mosfet;Difração de raios X;Microscopia de força atômica;Difusão;Feixes de íons;Deposicao de filmes finos|por|O rápido avanço tecnológico coloca a tecnologia do Si diante de um grande desafio: substituir o dielétrico de porta utilizado por mais de 40 anos em dispositivos MOSFET (transistor de efeito de campo metal-óxido-semicondutor), o óxido de silício (SiO2), por um material alternativo com maior constante dielétrica. Nesse contexto, vários materiais têm sido investigados. Nesta tese concentramos nossa atenção em três candidatos: o óxido de alumínio (Al2O3), o silicato de zircônio (ZrSixOy) e o aluminato de zircônio (ZrAlxOy). Nossos resultados experimentais baseiam-se em técnicas de análise com feixes de íons ou raios-X e de microscopia de força atômica. No caso do Al2O3, investigamos a difusão e reação de oxigênio através de filmes relativamente espessos (35 nm) quando submetidos a tratamento térmico em atmosfera oxidante, e os efeitos que esses processos provocam em filmes finos (6,5 nm) de Al2O3 depositados sobre uma estrutura SiO2/Si. Observamos que o processo de difusão-reação em filmes de Al2O3 é diferente do observado em filmes de SiO2: no primeiro caso, oxigênio difunde e incorpora-se em todo o volume do filme, enquanto que em filmes de SiO2, oxigênio difunde através do filme, sem incorporar-se em seu volume, em direção à interface SiO2/Si, onde reage.  Além disso, quando oxigênio atinge a interface Al2O3/Si e reage com o Si, além da formação de SiO2, parte do Si migra em direção ao Al2O3, deslocando parte dos átomos de Al e de O. Modelos baseados em difusão e reação foram capazes de descrever qualitativamente os resultados experimentais em ambos os casos. A deposição de filmes de Al2O3 sobre Si por deposição química de camada atômica a partir de vapor também foi investigada, e uma nova rotina de deposição baseada em préexposição dos substratos de Si ao precursor de Al foi proposta. As estruturas ZrSixOy/Si e ZrAlxOy/Si (ligas pseudobinárias (ZrO2)z(SiO2)1-z e (ZrO2)z(Al2O3)1-z depositadas sobre Si) foram submetidas a tratamentos térmicos em oxigênio ou vácuo com o objetivo de investigar possíveis instabilidades. Os tratamentos térmicos não provocaram instabilidades na distribuição de Zr, mas migração e incorporação de Si no filme dielétrico foram observadas durante os dois tratamentos para ambos os materiais.|http://hdl.handle.net/10183/2963
Implementação do protocolo TCP/IP para sistemas de intrumentação|2002|Open Access|Dissertação|Redes : Computadores;Tcp/ip;FPGA;Automação industrial|por|Baseado na tecnologia de interligação de redes, este trabalho apresenta uma proposta de conexão de dois sistemas com processamento próprio com o intuito de troca de informações, utilizando a pilha de protocolos TCP/IP. Este sistema será empregado em ambientes de controle industrial, permitindo o envio de informações do servidor de dados para o cliente. Os dados são constituídos de leituras feitas em equipamentos de campo, apresentando ao cliente remoto, medições dos mais diversos tipos. Por outro lado, o cliente poderá enviar comandos aos equipamentos de campo visando o telecontrole. Como ponto de partida para a elaboração do trabalho prático, foi utilizado o ambiente de controle do sistema de potência da companhia energética do estado do Rio Grande do Sul (CEEE). Um microcomputador com um browser acessa, através de uma rede local, os equipamentos controlados, que poderão ser qualquer tipo de equipamento de campo empregado em subestações de energia elétrica, como disjuntores, transformadores ou chaves.  Para permitir o acesso remoto de tais equipamentos, foi elaborado um servidor de dados constituído de um controlador de rede do tipo Ethernet e um microcontrolador de aplicação específica que se encarrega do processamento da pilha de protocolos. O controlador Ethernet utilizado é um circuito integrado dedicado comercial, que executa o tratamento dos sinais de nível físico e de enlace de dados conforme o padrão IEEE 802.2. O processador TCP/IP, enfoque principal deste trabalho, foi elaborado através da linguagem de programação C, e a seguir traduzido para o Java, que é o ponto de partida para a ferramenta SASHIMI, de geração da descrição em VHDL do microcontrolador de aplicação específica utilizado. O processador TCP/IP encarrega-se da aquisição de dados do equipamento de campo, do processamento da pilha de protocolos TCP/IP, e do gerenciamento do controlador Ethernet. A partir desta descrição VHDL, foi sintetizado o hardware do microcontrolador em um FPGA, que juntamente com o software aplicativo, também fornecido pela ferramenta utilizada, desempenha o papel de processador TCP/IP para o sistema proposto. Neste ambiente, então, o cliente localizado no centro de operação, acessa através de um browser o equipamento de campo, visando obter suas medições, bem como enviar comandos, destacando o aspecto bidirecional para a troca de dados e a facilidade de conexão de dois sistemas heterogêneos. Este sistema pretende apresentar baixo custo de aquisição e de instalação, facilidade de interconexão local ou remota e transparência ao usuário final.|http://hdl.handle.net/10183/2966
Desenvolvimento de sistemas tempo-real usando orientação a objetos : estudo sobre o mapeamento de especificações para linguagens de programação|2002|Open Access|Dissertação|Sistemas : Tempo real;Orientacao : Objetos|por|Este trabalho realiza um estudo sobre a criação de sistemas tempo-real usando orientação a objetos, com enfoque no mapeamento de especificações para linguagens de programação. O paradigma de orientação a objetos tem sido usado nas diferentes fases relacionadas com o desenvolvimento de sistemas tempo-real, variando desde a modelagem até o ambiente de programação e execução, mas atualmente estas iniciativas ainda focam etapas isoladas do ciclo de desenvolvimento. O objetivo deste trabalho é o de preencher esta lacuna, propondo um mapeamento entre uma metodologia ou ferramenta de análise e projeto de sistemas tempo-real orientados a objetos e uma linguagem ou ambiente de desenvolvimento baseado no paradigma de orientação a objetos que possua suporte para atender às restrições temporais especificadas. O mapeamento proposto foi desenvolvido utilizando estudos de caso clássicos em aplicações tempo-real que foram baseados em dois recentes padrões. O primeiro é o emergente padrão Real-Time UML, que visa realizar a especificação de requisitos temporais utilizando diagramas UML com extensões que os representem. O outro padrão é o Real-Time Specification for Java, que consiste de uma interface de programação (API) para desenvolvimento de aplicações tempo-real com a linguagem Java. O relacionamento entre stereotypes e tags usados para representar restrições temporais em diagramas UML e o código Java correspondente é explicado e um sumário da estratégia de mapeamento é discutido.|http://hdl.handle.net/10183/2969
O método de Perron : aplicações e extensões|2000|Open Access|Dissertação|Equações diferenciais parciais : Método de Perron : Operador diferencial Q : Extensão geométrica|por|Nesta dissertação apresentamos e desenvolvemos o Método de Perron, fazendo uma aplicação ao ploblema de Dirichlet para a equação das superfícies de curvatura média constante em R3. Apresentamos também uma extensão deste método dentro de EDP's e, por fim, obtemos uma extensão geométrica que se aplica a superfícies ao invés de gráficos. Comentamos a aplicação deste método geométrico á existência de superfícies mínimas tendo como bordo duas curvas convexas em planos paralelos do R3.;In this work we explain Perron's method and obtain an application of it to the Dirichlet Problem for the constant mean curvature surface equation in R3. We also obtain an extension of this method within the P.D.E theory and, finally, we obtain a geometric extension which applies to surfaces instead of graphs. This geometric extension can be used to prove the existence of a minimal compact surface having as boundary two convex curves in palallel plane of R3. We discuss this result at the final part of the work.|http://hdl.handle.net/10183/2984
ProTool : uma ferramenta de prototipação de software para o ambiente PROSOFT|2003|Open Access|Dissertação|Engenharia : Software;Prototipacao : Engenharia : Software;Especificacao algebrica;Prosoft|por|Dentre as principais áreas que constituem a Ciência da Computação, uma das que mais influenciam o mundo atual é a Engenharia de Software, envolvida nos aspectos científicos e tecnológicos do desenvolvimento de software. No desenvolvimento de software, a fase de especificação dos requisitos é uma das mais importantes, visto que erros não detectados nesta são propagados para as fases posteriores. Quanto mais avançado estiver o desenvolvimento, mais caro custa reparar um erro introduzido nas fases iniciais, pois isto envolve reconsiderar vários estágios do desenvolvimento. A prototipação permite que os requisitos do software sejam validados logo no início do desenvolvimento, evitando assim a propagação de erros. Paralelamente, a utilização de métodos formais visa revelar inconsistências, ambigüidades e falhas na especificação do software, que podem caso contrário, não serem detectadas. Usar a prototipação de software juntamente com uma notação formal enfatiza a especificação do problema e expõe o usuário a um sistema “operante” o mais rápido possível, de modo que usuários e desenvolvedores possam executar e validar as especificações dos requisitos funcionais.  O objetivo principal deste trabalho é instanciar uma técnica da área de Prototipação de Software que capacite o engenheiro de software gerar automaticamente protótipos executáveis a partir de especificações formais de tipos abstratos de dados, na notação PROSOFT-algébrico, visando a validação dos requisitos funcionais logo no início do desenvolvimento do software. Para tanto foi proposto um mapeamento da linguagem PROSOFT-algébrico para OBJ. Como OBJ possui um eficiente sistema de reescrita de termos implementado, a utilização deste propicia a prototipação de tipos abstratos de dados, especificados em PROSOFT-algébrico. Os componentes envolvidos na definição deste trabalho, assim como o mapeamento entre as linguagens, foram especificados algebricamente e implementados no ambiente de desenvolvimento de software PROSOFT. A implementação serviu para validar o mapeamento proposto através de dois estudos de caso. Por fim, são apresentadas as conclusões alcançadas e as atividades adicionais vislumbradas a partir do trabalho proposto.|http://hdl.handle.net/10183/2985
Modelo de metadados para armazenamento e recuperação de imagens estáticas no formato DICOM|2002|Open Access|Dissertação|Informática médica;Armazenamento : Imagem;Recuperacao : Imagem;Recuperação : Informação visual|por|Em linhas gerais, este trabalho aborda os temas de armazenamento de grandes volumes de imagens no formato DICOM, e a recuperação das mesmas com base em informações associadas a estas imagens (metadados independentes do conteúdo), informações obtidas na fase da interpretação das imagens (metadados descritivos de conteúdo), ou usando informações visuais que foram anotadas nas imagens ou extraídas das mesmas, por médicos especialistas em imagens médicas (metadados dependentes do conteúdo). Este trabalho foi desenvolvido com o propósito de elaborar uma modelagem conceitual que permita a descrição dos dados relevantes de imagens no formato DICOM, de maneira a facilitar a recuperação das mesmas posteriormente. As classes pertencentes ao modelo conceitual, decorrentes dessa modelagem, viabilizam a documentação de imagens médicas estáticas no formato DICOM. Visando o armazenamento de um grande volume de imagens médicas por um longo período de tempo, e considerando o desenvolvimento de uma solução economicamente viável para as instituições que provêm diagnóstico médico por imagens, o modelo propõe o armazenamento das imagens em um ambiente separado do banco de dados. Portanto, este trabalho apresenta uma solução que gerencia a localização das imagens em mídias on-line, near-line e off-line.  Este gerenciamento mantém o banco de dados atualizado quanto à localização atual das imagens, mantém as imagens armazenadas e distribuídas em mídias conforme a disponibilidade dos recursos físicos de armazenamento, e auxilia na recuperação das imagens. Este modelo serviu como base para a implementação de um sistema protótipo que possibilita a descrição e a recuperação de imagens DICOM. Os resultados obtidos através da implementação do sistema protótipo, em termos de armazenamento, recuperação e gerenciamento da localização das imagens nos diferentes ambientes (online, near-line e off-line), são apresentados e discutidos.|http://hdl.handle.net/10183/2987
Uma Proposta de arquitetura para editores diagramáticos com funcionalidade de execução|2002|Open Access|Dissertação|Engenharia : Software;Editor diagramatico;Case|por|Editores diagramáticos possuem aplicabilidade em diferentes áreas da computação, dentre elas ferramentas CASE, editores gráficos, CAD para circuitos eletrônicos, etc. Uma subclasse desse tipo de editores adiciona a funcionalidade de execução aos diagramas manipulados, permitindo ao usuário executar um diagrama através de técnicas de animação. Esta característica, por sua vez, acarreta na inserção de novos problemas para o projetista de software, sendo o principal deles o reuso independente das funcionalidades de edição e de execução de editores construídos. Com vistas a minimizar este problema este trabalho apresenta uma arquitetura que combina uma estrutura de componentes interconectáveis descrita por meio de BML com um mecanismo de interpretação de scripts como uma alternativa para construção de editores de diagramas com funcionalidade de execução.|http://hdl.handle.net/10183/2988
Estimação em classes de processos estocásticos com decaimento hiperbólico da função de autocorrelação|2002|Open Access|Tese|Processos estocásticos;Função de autocorrelação;Decaimento polinomial;Análise de séries temporais;Processos ARFIRMA|por|Neste trabalho analisamos processos estocásticos com decaimento polinomial (também chamado hiperbólico) da função de autocorrelação. Nosso estudo tem enfoque nas classes dos Processos ARFIMA e dos Processos obtidos à partir de iterações da transformação de Manneville-Pomeau. Os objetivos principais são comparar diversos métodos de estimação para o parâmetro fracionário do processo ARFIMA, nas situações de estacionariedade e não estacionariedade e, além disso, obter resultados similares para o parâmetro do processo de Manneville-Pomeau. Entre os diversos métodos de estimação para os parâmetros destes dois processos destacamos aquele baseado na teoria de wavelets por ser aquele que teve o melhor desempenho.;In this work we analyze stochastic processes with polynomial (also called hyperbolic) decay of the autocorrelation function. We emphasize the class of ARFIMA processes and the one obtained from the Manneville-Pomeau iterated function processes. The main goal is to compare different estimation methods for the fractional parameter in ARFIMA process, for both stationary and non-stationary case and, moreover, to get similar results for the parameter in the Manneville-Pomeau process. Among all estimation methods for the parameters of these two processes we stress the one based on the wavelet theory since this had the best performaance.|http://hdl.handle.net/10183/3031
Síntese e enriquecimento enantiomérico via catálise enzimática do sistema 5-bromo-12-oxa-pentaciclo[6.2.16,9.12,7.12,10]dodeca-4-eno-3-endo-ol e derivados. RMN de 1H e 13C: correlação de parâmetros teóricos e experimentais|2003|Open Access|Tese|Policiclicos : Sintese;Síntese orgânica|por|Partindo de ciclopentadieno, ciclohexadieno, p-benzoquinona e 2,5-dibromo-pbenzoquinona, os adutos 1, 5, 30 e 31 foram sintetizados. Os adutos 1, 5 e 30 foram utilizados como produtos de partida para a síntese de 13 (treze) novos compostos, em sua maioria com potenciais características para apresentarem atividade biológica inibidora de glicosidases e reguladora da liberação de Insulina no sangue. O aduto 31 é inédito na literatura até o momento. Cinco novas propostas de mecanismos são apresentadas. Os álcoois racêmicos 6 e 29 foram submetidos a reações de transesterificação catalisadas por lipase de Pseudomonas cepacia em diferentes preparações e seus enantiômeros separados com enantiosseletividade (E) maior que 100 em todos os casos. Este processo resultou, também, na obtenção dos respectivos acetatos 43 e 44 enantiomericamente puros e com excelentes rendimentos químicos. Os compostos 6, 29 e 34 depois de terem suas estruturas moleculares resolvidas através dos métodos espectroscópicos de rotina, tiveram suas estruturas moleculares calculadas pelo método ab initio e por Funcionais de Densidade. As geometrias otimizadas foram submetidas ao método GIAO para o cálculo dos tensores de blindagem magnética isotrópica. Estes cálculos mostraram-se eficazes na descrição dos deslocamentos químicos da maioria dos átomos, incluindo os dos anéis ciclopropanos presentes nas estruturas moleculares de cada composto. Algumas dificuldades foram encontradas para a descrição do sistema vinílico halogenado dos álcoois 6 e 29. Foram utilizadas moléculas modelo para verificar a extensão de tais dificuldades.|http://hdl.handle.net/10183/3051
Análise comparativa de desempenho de redes IP e ATM com tráfego multimídia interativo|2002|Open Access|Dissertação|Redes;Computadores;Desempenho;Qualidade;Servico;Multimídia;Redes atm;IP|por|A década de 80 é um marco para a área de comunicação de dados. Muitos estudos, pesquisas e especificações foram iniciados para permitir a integração de sistemas de comunicação para voz, vídeo e dados. Desde essa década, os cientistas procuram investir na Internet, que é um dos mais importantes e populares meios de transmissão de informações no mundo, para acomodar aplicações que permitam enviar imagens e sons por esta imensa rede de comunicação. Também nessa década, no final, o International Telecommunications Union – Telecommunication (ITU-T), especificou a tecnologia ATM, cujas pesquisas vinham ocorrendo desde o seu início. O serviço de rede tradicional na Internet é transmissão de datagramas ´besteffort ´, conforme será visto neste trabalho. Neste serviço, os pacotes da origem são enviados para o destino, sem garantia de entrega. Para aquelas aplicações que requerem garantia de entrega, o protocolo TCP (equivalente à camada 4 do RM-OSI) se encarregará da retransmissão dos pacotes que falharam ao atingir o destino para então se conseguir uma recepção correta. Para aplicações de comunicação tradicionais, como FTP e Telnet, em que uma entrega correta é mais importante que a perda de tempo da retransmissão, este serviço é satisfatório. Entretanto, uma nova classe de aplicações, as quais usam mídias múltiplas (voz, som e dados), começam a aparecer na Internet. Exemplos desta classe de aplicação são: vídeo teleconferência, vídeo sob demanda e simulação distribuída. Operações de modo tradicional para estas aplicações resultam em redução da qualidade da informação recebida e, potencialmente, ineficiência do uso da largura de banda.  Para remediar este problema é desenvolvido um ambiente de serviço para tempo real, no qual múltiplas classes de serviços são oferecidas. Este ambiente estende o modelo de serviços existentes para ir ao encontro das necessidades das aplicações multimídia com obrigatoriedade de tempo real, porém esta não é uma meta muito fácil. Contudo, a comunidade pesquisadora tem conseguido desenvolver alguns mecanismos que vão pouco a pouco permitindo que este objetivo se transforme em realidade. O ATM é uma tecnologia que provê dutos de alta velocidade para a transferência de qualquer tipo de informação em pequenos pacotes de tamanho fixo, chamados células. A idéia básica é estabelecer entre dois pontos que desejam se comunicar um circuito virtual que é mantido pelos comutadores de células para levar a informação de um lado a outro. A característica marcante do ATM é a Qualidade de Servico – QoS, que garante o desempenho predefinido que determinado serviço necessita. Isso permite suportar aplicações de tempo real que são sensíveis ao atraso. O atendimento à diversidade de características de tráfego e exigências distintas de grandes quantidades de serviços é possível pelo ATM graças ao controle de tráfego reunido à capacidade de multiplexação estatística do meio em altas taxas de transmissão.  O objetivo principal desta dissertação é elaborar uma comparação quantitativa e qualitativa do funcionamento de aplicações multimídia sobre IP com RSVP - Protocolo desenvolvido para realizar reservas de recursos integrante da arquitetura de Serviços Integrados (IntServ) proposta pelo IETF para oferecer qualidade de serviço para aplicações tais como aplicações multimídia. Essa proposta também inclui duas classes de serviços, sendo que nessa dissertação, o serviço de carga controlada é que está sendo utilizado. Isso deve-se a implementação dos módulos apresentados em [GRE 2001] e que foram utilizados na realização desse trabalho - e sobre ATM. A proposta final é a elaboração de uma técnica de análise baseado nas principais métricas de desempenho em redes que deve permitir uma melhor visualização do comportamento das tecnologias sob determinadas cargas de tráfego, permitindo assim uma tomada de decisão entre qual das tecnologias que deverá ser adotada em um dado momento, em uma dada situação, ou seja, a indicação do ponto de quebra tecnológica na situação modelada. Para que fosse possível fazer esta comparação, foi necessário dividir-se este trabalho em 3 grandes etapas, que são: • Estudo e desenvolvimento da técnica para análise do elemento carga de tráfego na tecnologia ATM; • Estudo e desenvolvimento da técnica para análise do elemento carga de tráfego na tecnologia IP com RSVP; • Comparativo quantitativo e qualitativo dos estudos acima.|http://hdl.handle.net/10183/3054
Soft IP para criptografia usando o algoritmo Rijndael e implementação em lógica programável|2002|Open Access|Dissertação|Microeletrônica;Criptografia;Vhdl;FPGA|por|A criptografia assumiu papel de destaque no cotidiano das pessoas, em virtude da necessidade de segurança em inúmeras transações eletrônicas. Em determinadas áreas, a utilização de hardware dedicado à tarefa de criptografia apresenta vantagens em relação à implementação em software, devido principalmente ao ganho de desempenho. Recentemente, o National Institute of Standards and Technology (NIST) publicou o novo padrão norte-americano de criptografia simétrica, chamado de Advanced Encryption Standard (AES). Após um período de aproximadamente 3 anos, no qual várias alternativas foram analisadas, adotou-se o algoritmo Rijndael. Assim, este trabalho apresenta um Soft IP do padrão AES, codificado em VHDL, visando a implementação em FPGA Altera. Todo o projeto foi construído com funções e bibliotecas genéricas, a fim de permitir a posterior implementação sobre outras tecnologias. Foram geradas duas versões: uma priorizando desempenho e outra priorizando a área ocupada nos componentes. Para cada uma das versões, produziu-se um circuito para encriptar e outro para decriptar. O desempenho alcançado em termos de velocidade de processamento superou todos os outros trabalhos publicados na área, sobre a mesma tecnologia. São apresentados os detalhes de implementação, arquiteturas envolvidas e decisões de projeto, bem como todos os resultados. A dissertação contém ainda conceitos básicos de criptografia e uma descrição do algoritmo Rijndael.|http://hdl.handle.net/10183/3061
Um Estudo de um sistema de informações hipermídia : caso particular da Assembléia Legislativa do Rio Grande do Sul|1998|Open Access|Dissertação|Sistemas : Informacao multimidia;Armazenamento : Informacao;Internet;Intranet|por|O presente trabalho se insere dentro de uma aplicação real na Assembléia Legislativa do Rio Grande do Sul (ALERGS), desenvolvida no Departamento de Sistemas de Informações, aproveitando-se do Sistema PRIMA/Vídeo – um sistema de veiculação interna de sinais de vídeo – já existente desde novembro de 1996. O PRIMA/Vídeo surgiu da necessidade de implantação de um sistema que permitisse o acompanhamento das múltiplas reuniões realizadas simultaneamente no âmbito do Palácio Farroupilha. Um projeto foi elaborado pela IBM do Brasil e implantado um sistema composto por dezesseis câmeras de sinais de vídeo ligadas a uma rede de 120 estações, que recebem analogicamente e em tempo real os sinais de vídeo. Este sistema permite o acompanhamento on-line de qualquer reunião nas dependências da ALERGS, além de receber imagens dos canais das emissoras a cabo. Entretanto, surgiu a necessidade de gravação destes vários eventos ocorridos para posterior exibição, criando, assim, um estudo para um sistema de informações composto de dados multimídia (sinais de vídeo do PRIMA/Vídeo).  O Sistema de Informações Hipermídia da Assembléia Legislativa (SIHAL) visa o armazenamento e a disponibilização de sinais de vídeo (imagem e som) aos usuários internos da ALERGS. Através deste novo Sistema, os usuários poderão assistir a reuniões realizadas anteriormente e terão opções com capacidade de: • armazenar e organizar fisicamente os vídeos analógicos das Sessões Plenárias; • organizar o agendamento das Sessões Plenárias solicitadas no canal analógico de vídeo do PRIMA/Vídeo; • armazenar digitalmente os principais trechos de vídeo dos eventos da Assembléia; • fornecer o download dos trechos digitalizados aos usuários pela interface do Sistema; • fornecer a fita de vídeo aos usuários do Sistema; • criar uma base de dados de vídeo referente às imagens digitalizadas das Sessões Plenárias; • fornecer acesso à base de dados de vídeo. A principal contribuição desta dissertação é o estudo sobre soluções mistas, integrando dados multimídia analógicos e digitais com o intuito de propor um Sistema de Informações Hipermídia, o qual foi denominado de SIHAL. Um protótipo do SIHAL será implementado como demonstração da viabilidade das idéias citadas no sistema proposto.|http://hdl.handle.net/10183/3074
Uma Linguagem visual de consulta a XML baseada em ontologias|2001|Open Access|Dissertação|Programação;XML (Linguagem de marcação);Linguagens : Consulta;Ontologias|por|O volume de informações armazenadas e representadas em XML cresce rapidamente, abrangendo desde a Web até bancos de dados corporativos. Nesse contexto, surge a necessidade de mecanismos de recuperação de dados nesse formato que sejam, ao mesmo tempo, mais eficientes e mais eficazes. Várias propostas de linguagens de consulta têm sido feitas, dentre as quais podem ser citadas XQL, XML-QL e Quilt. Essas linguagens, todas textuais, são mais indicadas para manipulação programática ou para usuários experientes. Visando atingir também os usuários menos experientes, foram propostas linguagens visuais, tais como XML-GL e Xing. Todas essas linguagens, entretanto, apresentam duas características comuns: a) o usuário precisa conhecer, pelo menos em um certo nível, a estrutura interna dos documentos; b) a mesma informação, se armazenada de formas diferentes, exige instruções de consulta diferentes. A solução para esses problemas apresentada neste trabalho envolve a utilização de um modelo conceitual para representar os conceitos e as relações entre conceitos que ocorrem em documentos XML pertencentes a um determinado domínio de problema. O modelo conceitual é representado por uma ontologia do domínio do problema. Essa associação permite que consultas possam ser elaboradas tendo como base os conceitos da ontologia. Para permitir a associação da ontologia a conjuntos de documentos XML, apresentam-se regras de mapeamento que permitem definir se um documento XML é compatível com uma determinada ontologia. A partir dessa definição, propõe-se uma linguagem visual para consultas a documentos XML com base em ontologias, e apresenta-se uma proposta de interface visual para essa linguagem.|http://hdl.handle.net/10183/3075
Expansão da ocupação urbana de Gramado: estudo de caso da aplicação de fotografias aéreas de pequeno formato e SIG|2002|Open Access|Dissertação|Fotografias aereas;Sensoriamento remoto;Ocupação urbana : Gramado, RS;Sistemas de Informação Geográfica (SIG)|por|Fotografias aéreas verticais de pequeno formato foram obtidas através de câmera fotográfica não-métrica a bordo de aeronave de pequeno porte, no ano de 1999, com o objetivo de se atualizar um conjunto de seis plantas cadastrais digitais de 1984, na escala 1:5.000, cobrindo a área urbana do município de Gramado, situado na região nordeste do Estado do Rio Grande do Sul, Brasil. As fotografias foram digitalizadas, armazenadas e georreferenciadas em um Sistema de Informações Geográficas. Posteriormente, foram digitalizados os perímetros das edificações e os limites dos bairros da área urbana nas plantas cadastrais digitais. Os dados atualizados das plantas cadastrais foram elaborados para a geração de análises temáticas e, portanto, não podem ser utilizados para a medição precisa das edificações mapeadas. Após sua digitalização, os polígonos das edificações foram estruturados topologicamente por meio do SIG utilizado, permitindo a geração automática de um centróide para cada edificação mapeada. A consulta ao banco de dados tornou possível o cálculo do número total e a localização dos centróides das edificações, para as duas datas estudadas.  Os dados tabulados foram representados por um mapa temático da expansão da ocupação urbana, na escala de 1:25.000. Outros produtos obtidos no presente estudo foram cartas-imagem da área urbana (1:50.000), elaboradas a partir de imagens do satélite LANDSAT 7 ETM+, bem como mapas temáticos dos aspectos físicos e antrópicos dos bairros da área urbana (escalas 1:15.000 a 1:25.000). Com base nos dados obtidos, verificou-se um crescimento de 164, 6 % do número total de edificações e uma expansão da ocupação orientada para a região SE da área urbana, em função do maior crescimento do setor secundário e do número de loteamentos na região meridional da área urbana durante o período de 15 anos. Através da interpretação dos dados geográficos, foram elaboradas recomendações para o planejamento urbano de Gramado.|http://hdl.handle.net/10183/3091
Anéis de fatoração única|2003|Open Access|Dissertação|Anéis;Fatoracao|por|Este trabalho tem por objetivo estudar condições necessárias e sufi- cientes sobre um determinado anel R, não necessariamente comutativo, para que suas extensões polinomiais apresentem fatoração única. O estudo de tal propriedade é feito para anéis primos Noetherianos e para anéis primos não necessariamente Noetherianos.|http://hdl.handle.net/10183/3106
Estimação e previsão em processos com longa dependência sazonais|2003|Open Access|Dissertação|Séries temporais;Sazonalidade|por|Neste trabalho analisamos alguns processos com longa dependência sazonais, denotados por SARFIMA(0,D, 0)s, onde s é a sazonalidade. Os estudos de estimação e previsão estão baseados em simulações de Monte Carlo para diferentes tamanhos amostrais e diferentes sazonalidades. Para estimar o parâmetro D de diferenciação sazonal utilizamos os estimadores propostos por Geweke e Porter-Hudak (1983), Reisen (1994) e Fox e Taqqu (1986). Para os dois primeiros procedimentos de estimação consideramos seis diferentes maneiras de compor o número de regressores necessários na análise de regressão, com o intuito de melhor comparar seus desempenhos. Apresentamos um estudo sobre previsão h-passos à frente utilizando os processos SARFIMA(0,D, 0)s no qual analisamos o erro de previsão, as variâncias teórica e amostral, o vício, o pervício e o erro quadrático médio.;In this work we analyze some long memory seasonal processes, denoted by SARFIMA(0,D, 0)s, where s is the seasonality. The estimation and forecas- ting analysis in these processes are based on Monte Carlo simulation studies for different seasonal parameters and different sample sizes. To estimate the fractional seasonal parameter D we consider the methods proposed by Geweke and Porter-Hudak (1983), Reisen (1994) and Fox and Taqqu (1986). For the first two estimation procedures we consider six different ways to choose the number of regressors in the linear regression, to better compare their performances. We also consider here the study of the h-steps ahead forecasting for these SARFIMA(0,D, 0)s processes analyzing the forecasting error, the theoretical and sample variances, the bias, the percentage bias and the mean square error.|http://hdl.handle.net/10183/3107
Um Modelo de estruturação de requisitos para o método fusion|2001|Open Access|Dissertação|Engenharia : Software;Engenharia : Requisitos;Teoria : Atividade;Metodo fusion|por|Uma definição confiável dos requisitos de um software depende diretamente da completa e correta compreensão sobre as necessidades do sistema e sua conseqüente representação de forma adequada ao processo de desenvolvimento. Uma proposta de modelagem de requisitos deve apresentar qualidades que colaborem para a compreensão mútua das necessidades entre os envolvidos no processo e que organizem os requisitos de forma a permitir o acompanhamento no desenvolvimento do software. O presente trabalho apresenta um modelo de estruturação de requisitos fundamentado em metodologias orientadas a objetivos com utilização de cenários e preceitos da Teoria da Atividade. O modelo tem sua argumentação nas premissas que cliente e usuários normalmente expressam suas necessidades através de objetivos almejados e que a ação humana deve ser analisada dentro de um contexto para que possa fazer sentido e ser compreendida.  Inserido no contexto do Projeto FILM1, cujo objetivo é expandir o Método Fusion, agregando uma etapa de modelagem de requisitos, o trabalho estabeleceu a qualidade de usabilidade como motivadora da definição de um modelo de estruturação de requisitos. A usabilidade é uma qualidade que visa facilitar a utilização do modelo como uma ferramenta de representação dos requisitos de forma inteligível, atuando tanto na especificação dos requisitos como na validação dos mesmos entre os envolvidos. Os requisitos são estruturados segundo uma abordagem voltada aos clientes e usuários do sistema. O modelo definido tem por objetivo prover a construção gradual e incremental do entendimento compartilhado entre os envolvidos sobre os domínios do problema e da solução, na concepção e no desenvolvimento do software. Metodologias orientadas a objetivos, operacionalizadas através de cenários, conjugadas a princípios da atividade oferecem um suporte adequado a estruturação de requisitos provendo usabilidade ao modelo. A avaliação da aplicabilidade do modelo é realizada com a modelagem de requisitos em três estudos de casos. Em cada caso são aplicadas técnicas de elicitação no sentido da afinar a sintonia com a estrutura do modelo de requisitos. A concepção do modelo, embasada em conceitos da Teoria da Atividade, é bastante adequado às atividades de elicitação em uma abordagem voltada aos clientes e usuários.|http://hdl.handle.net/10183/3109
Mineração de regras de associação no problema da cesta de compras aplicada ao comércio varejista de confecção|2002|Open Access|Dissertação|Banco : Dados;Descoberta : Conhecimento;Mineracao : Dados;Regras : Associacao;Informatica : Comercializacao : Produtos|por|A maioria das empresas interage com seus clientes através de computadores. Com o passar do tempo está armazenado nos computadores um histórico da atividade da empresa que pode ser explorado para a melhoria do processo de tomada de decisões. Ferramentas de descoberta de conhecimento em bancos de dados exploram este histórico a fim de extrair vários tipos de informação. Um dos tipos de informação que pode ser extraída destes tipos de bancos de dados são as regras de associação que consistem em relacionamentos ou dependências importantes entre itens tal que a presença de alguns itens em uma transação irá implicar a presença de outros itens na mesma transação. Neste trabalho são aplicadas técnicas de descoberta de conhecimento na área do comércio varejista de confecção. Foram detectadas algumas peculiaridades dos bancos de dados desta área sendo proposto um novo algoritmo para melhorar o desempenho da tarefa de extração de regras de associação. Para a validação dos resultados apresentados pelo algoritmo foi desenvolvido o protótipo de uma ferramenta para extração de regras de associação. Foram realizados experimentos com bancos de dados reais de uma empresa da área de comércio varejista de confecção para análise de desempenho do algoritmo.|http://hdl.handle.net/10183/3110
Reconhecimento automático de locutor utilizando medidas de invariantes dinâmicas não-lineares|2002|Open Access|Tese|Reconhecimento : Padroes;Reconhecimento : Voz;Processamento : Sinais|por|As técnicas utilizadas em sistemas de reconhecimento automático de locutor (RAL) objetivam identificar uma pessoa através de sua voz, utilizando recursos computacionais. Isso é feito a partir de um modelamento para o processo de produção da voz. A modelagem detalhada desse processo deve levar em consideração a variação temporal da forma do trato vocal, as ressonâncias associadas à sua fisiologia, perdas devidas ao atrito viscoso nas paredes internas do trato vocal, suavidade dessas paredes internas, radiação do som nos lábios, acoplamento nasal, flexibilidade associada à vibração das cordas vocais, etc. Alguns desses fatores são modelados por um sistema que combina uma fonte de excitação periódica e outra de ruído branco, aplicadas a um filtro digital variante no tempo. Entretanto, outros fatores são desconsiderados nesse modelamento, pela simples dificuldade ou até impossibilidade de descrevê-los em termos de combinações de sinais, filtros digitais, ou equações diferenciais. Por outro lado, a Teoria dos Sistemas Dinâmicos Não-Lineares ou Teoria do Caos oferece técnicas para a análise de sinais onde não se sabe, ou não é conhecido, o modelo detalhado do mecanismo de produção desses sinais. A análise através dessa teoria procura avaliar a dinâmica do sinal e, assumindo-se que tais amostras provêm de um sistema dinâmico não-linear, medidas qualitativas podem ser obtidas desse sistema.  Essas medidas não fornecem informações precisas quanto ao modelamento do processo de produção do sinal avaliado, isto é, o modelo analítico é ainda inacessível. Entretanto, pode-se aferir a respeito de suaO problema analisado ao longo deste trabalho trata da busca de novos métodos para extrair informações úteis a respeito do locutor que produziu um determinado sinal de voz. Com isso, espera-se conceber sistemas que realizem a tarefa de reconhecer um pessoa automaticamente através de sua voz de forma mais exata, segura e robusta, contribuindo para o surgimento de sistemas de RAL com aplicação prática. Para isso, este trabalho propõe a utilização de novas ferramentas, baseadas na Teoria dos Sistemas Dinâmicos Não-Lineares, para melhorar a caracterização de uma pessoa através de sua voz. Assim, o mecanismo de produção do sinal de voz é analisado sob outro ponto de vista, como sendo o produto de um sistema dinâmico que evolui em um espaço de fases apropriado. Primeiramente, a possibilidade de utilização dessas técnicas em sinais de voz é verificada. A seguir, demonstra-se como as técnicas para estimação de invariantes dinâmicas não-lineares podem ser adaptadas para que possam ser utilizadas em sistemas de RAL. Por fim, adaptações e automatizações algorítmicas para extração de invariantes dinâmicas são sugeridas para o tratamento de sinais de voz.  A comprovação da eficácia dessa metodologia se deu pela realização de testes comparativos de exatidão que, de forma estatisticamente significativa, mostraram o benefício advindo das modificações sugeridas. A melhora obtida com o acréscimo de invariantes dinâmicas da forma proposta no sistema de RAL utilizado nos testes resultou na diminuição da taxa de erro igual (EER) em 17,65%, acarretando um intrínseco aumento de processamento. Para sinais de voz contaminados com ruído, o benefício atingido com o sistema proposto foi verificado para relações sinal ruído (SNRs) maiores que aproximadamente 5 dB. O avanço científico potencial advindo dos resultados alcançados com este trabalho não se limita às invariantes dinâmicas utilizadas, e nem mesmo à caracterização de locutores. A comprovação da possibilidade de utilização de técnicas da Teoria do Caos em sinais de voz permitirá expandir os conceitos utilizados em qualquer sistema que processe digitalmente sinais de voz. O avanço das técnicas de Sistemas Dinâmicos Não-Lineares, como a concepção de invariantes dinâmicas mais representativas e robustas, implicará também no avanço dos sistemas que utilizarem esse novo conceito para tratamento de sinais vocais.|http://hdl.handle.net/10183/3111
Estudo dos compostos voláteis do alecrim utilizando as técnicas de microextração em fase sólida (SPME), hidrodestilação e extração com fluído supercrítico (SFE)|2003|Open Access|Dissertação|Alecrim;Óleos essenciais;Microextração em fase sólida;Extração com fluido supercrítico|por|Resumo não disponível.|http://hdl.handle.net/10183/3145
Síntese de novas bases de tröger, fluorescentes via transferência protônica intramolecular no estado excitado (ESIPT)|2003|Open Access|Dissertação|Corantes orgânicos fluorescentes;Transferencia protonica intramolecular no estado excitado|por|Neste trabalho, realizou-se a síntese e a caracterização de quatro novas bases de Tröger. Estes compostos pertencem à classe de heterociclos 2-hidroxifenibenzazóis, que caracterizam-se por apresentar uma forte emissão de fluorescência, devido à reação de transferência protônica intramolecular no estado excitado -ESIPT- por eles sofrrida quando excitados por luz ultravioleta. Os heterociclos sintetizadosapesentam ligações de hidrogênio intramolecular entrte o nitrogênio azólico e a hidroxila fenólica e um granded deslocamento de Stokes, características tíicas de compostos que sofrem a ESIPT. As bases de Tröger, que são quirais, são bem conhecidas como receptores moleculares devido a concavidade que estas moléculas apresentam, Os compostos obtidos são os primeiro exmeplos de bases de Tröger fluorescentes, via ESIP, na literatura, por isso, despertam grande interesse na síntese de sondas biológicas moleculares fluorescentes. A partir da resolução quiral das bases de Tröger, estas potencialmente podem vir a ser utilizadas como indutores quirais e/ou par ao reconhecimento enantiosseletivo do DNA. A resolução quiral foi feita para uma das bases de Tröger sintetizadas.|http://hdl.handle.net/10183/3147
Síntese, caracterização e estudo das propriedades adsorventes do xerogel p-anisidinapropilsilica|2003|Open Access|Dissertação|Gel de sílica : Síntese;Sol-gel|por|Neste trabalho foram estudadas a síntese, a caracterização, a estabilidade térmica e as propriedades adsorventes de um novo xerogel híbrido, p-anisidinapropilsílica. O material foi sintetizado a partir do método sol-gel utilizando como precursor orgânico a p-anisidinapropiltrimetoxisilano (AnPTMS), também sintetizado em nosso laboratório. O precursor orgânico foi gelatinizado simultaneamente com tetraetilortosilicato (TEOS). Foram sintetizadas cinco amostras de xerogéis contendo diferentes graus de incorporação orgânica. A incorporação orgânica foi monitorada a partir da variação da concentração de precursor orgânico adicionado, nos valores: 0,05; 0,15; 0,23; 0,35; e 0,46 mol.l-1, sendo que os xerogéis resultantes foram designados como A, B, C, D e E, respectivamente. Os xerogéis foram tratados termicamente sob vácuo e analisados por espectroscopia no infravermelho (Termoanálise na região do Infravermelho). Os espectros mostraram que a área sob a banda em 1500 cm-1, devida ao anel aromático da p-anisidina, diminui com o aumento da temperatura do tratamento térmico e esta diminuição é mais pronunciada nos materiais cuja incorporação orgânica foi menor. Entretanto, os xerogéis com maior incorporação orgânica (amostras D e E) apresentaram boa estabilidade térmica da fase orgânica até a temperatura de 300 oC  Foram obtidas isotermas de adsorção e dessorção de nitrogênio para as amostras A, B, C e D, sendo que a partir delas foram obtidas a área superficial, volume e distribuição do tamanho de poros dos xerogéis. Todas as amostras mostraram porosidade na região de mesoporos (2-50 nm), entretanto, foi observado que o aumento da incorporação orgânica resultou em diminuição da área superficial bem como do tamanho dos mesoporos dos xerogéis. Entretanto, todas as amostras mostraram boa estabilidade térmica morfológica, visto que não foram observadas variações significativas na distribuição de poros com aquecimento até a temperatura de 350 oC. Os xerogéis foram lavados com hexano e diclorometano para a remoção de parafina e pequenas frações de oligômero altamente organofuncionalizado residuais. A amostra D foi utilizada como fase estacionária em coluna de pré-concentração de uma amostra padrão de ésteres ftálicos, mostrando um promissor desempenho.|http://hdl.handle.net/10183/3152
Aplicação de filtros de Gabor no processo de classificação de imagens digitais com base em atributos de textura|2000|Open Access|Dissertação|Imagem digital : Métodos de classificação : Filtragem de imagens : Filtros de Gabor : Bandas texturais : Máxima verossimilhança Gaussiana|por|No processo de classificação de uma imagem digital, o atributo textura pode ser uma fonte importante de informações. Embora o processo de caracterização da textura em uma imagem seja mais difícil, se comparado ao processo de caracterização de atributos espectrais, sabe-se que o emprego daquele atributo pode aumentar significativamente a exatidão na classificação da imagem. O objetivo deste trabalho de pesquisa consiste em desenvolver e testar um método de classificação supervisionado em imagens digitais com base em atributos de textura. O método proposto implementa um processo de filtragem baseado nos filtros de Gabor. Inicialmente, é gerado um conjunto de filtros de Gabor adequados às freqüências espaciais associadas às diferentes classes presentes na imagem a ser classificada. Em cada caso, os parâmetros utilizados por cada filtro são estimados a partir das amostras disponíveis, empregando-se a transformada de Fourier. Cada filtro gera, então, uma imagem filtrada que quantifica a freqüência espacial definida no filtro. Este processo resulta em um certo número de imagens filtradas as quais são denominadas de ""bandas texturais"". Desta forma, o problema que era originalmente unidimensional passa a ser multi-dimensional, em que cada pixel passa a ser definido por um vetor cuja dimensionalidade é idêntica ao número de filtros utilizados. A imagem em várias ""bandas texturais"" pode ser classificada utilizando-se um método de classificação supervisionada. No presente trabalho foi utilizada a Máxima Verossimilhança Gaussiana. A metodologia proposta é então testada, utilizandose imagens sintéticas e real. Os resultados obtidos são apresentados e analisados.|http://hdl.handle.net/10183/3165
Ondas e instabilidades em plasmas com inomogeneidades suaves na densidade e no campo magnético de equilíbrio|2003|Open Access|Tese|Absorcao de ondas eletromagneticas;Plasmas;Densidade;Temperatura;Campos magnéticos;Cinética;Tensores;Relações de dispersão;Instabilidades em plasmas;Equilibrio em plasmas;Energia;Magnetosfera|por|No presente trabalho, estudamos a absorção e amplificação de ondas eletromagnéticas que se propagam em plasmas com densidade e temperatura fracamente inomogêneas, imersos em um campo magnético também inomogêneo, tendo como base a teoria cinética, dentro do contexto da aproximação local. Esse estudo se dá efetivamente a partir da obtenção do tensor dielétrico do plasma, que deve ser empregado na relação de dispersão. Iniciamos com uma revisão dos conceitos básicos sobre plasmas homogêneos e inomogêneos. Os fundamentos da teoria cinética também foram abordados. Apresentamos uma revisão de trabalhos anteriores que enfocam o mesmo tema, embora descrevendo separadamente os dois tipos de inomogeneidades. A partir desses trabalhos, obtivemos um tensor dielétrico geral, que descreve de forma simultânea as inomogeneidades do campo magnético de equilíbrio e da função distribuição de equilíbrio.Tal tensor foi obtido a partir de um sólido desenvolvimento teórico, que garante a correta descrição da troca de energia entre as ondas e as partículas do plasma.  Abordamos os aspectos gerais das instabilidades de deriva, direcionando o estudo à faixa de frequência das ondas híbridas inferiores, e às instabilidades LHDI e MTSI (IWI). Utilizamos perfis lineares de inomogeneidades de campo magnêtico ambiente e densidade para modelar a região da magnetosfera conhecida como neutral sheet. Particularizamos o tensor dielétrico para o estudo específico das instabilidades LHDI e MTSI (IWI), para o tipo de perfil citado acima. Apresentamos uma nova rela»c~ao de dispers~ao para plasmas inomogêneos, que incorpora explicitamente as derivadas espaciais do tensor dielétrico do plasma. Usamos o tensor que unifica os tratamentos das inomogeneidades do campo e densidade nessa relação de dispersão, e obtivemos uma descrição unificada das instabilidades LHDI e MTSI (IWI).|http://hdl.handle.net/10183/3177
DTA : discriminador de tráfego ATM|2001|Open Access|Dissertação|Redes : Computadores;Redes atm;Trafego : Redes : Computadores|por|As redes ATM têm se constituído na solução tecnológica ideal para o desenvolvimento da RDSI-FL. O ATM foi desenvolvido para dar suporte, com altas velocidades e com garantia de qualidade, a uma ampla variedade de serviços. A demanda por serviços de comunicação de dados, em velocidades maiores do que as redes de pacotes atuais são capazes de oferecer, tem crescido rapidamente, estimulando o desenvolvimento da tecnologia ATM, que tem sido muito promissora, devida a sua flexibilidade e eficiência. Para dar suporte à comunicação de dados em redes ATM, foram desenvolvidas soluções, como a emulação de redes locais (LANE), a interconexão de redes locais (LAN) e metropolitanas (MAN), e o protocolo IP sobre ATM. O ATM utiliza multiplexação estatística (assíncrona) para inserir os dados nos canais de comunicação de forma otimizada. Esta técnica, permite atender uma quantidade maior de fontes heterogêneas sob num único canal, porém, pode causar congestionamento na rede. O uso da multiplexação estatística, aliada à necessidade de garantir níveis de qualidade diferentes, para cada tipo de serviço, torna necessária a adoção de uma combinação de técnicas de controle de tráfego e congestionamento. Com este propósito, foram desenvolvidos diversas funções e mecanismos, como, por exemplo, controle de acesso (CAC), controle dos parâmetros de uso (UPC), descarte seletivo de células e conformação de tráfego (TS), os quais necessitam conhecer as características do tráfego emitido pela fonte. Por esta razão, toda e qualquer conexão ATM está associada a um contrato de tráfego, que especifica as exigências do usuário, quanto à qualidade de serviço (QoS) a ser oferecida pela rede, através de um descritor de tráfego. O trabalho apresentado nesta dissertação, tem por objetivo propor uma metodologia capaz de discriminar um fluxo de células ATM arbitrário, visando a obter os parâmetros descritores de UPC do mesmo. A discriminação de tráfego é relevante à medida que todos os usuários de redes ATM necessitam, de alguma forma, determinar os parâmetros que caracterizam seu fluxo, para poder negociar o contrato de trafego. Infelizmente, a maioria das aplicações ATM, não tem condições prévias de fornecer dados sobre o comportamento do fluxo que geram. Para estas situações, estamos propondo um Discriminador de Tráfego ATM (DTA), que infere a partir de uma amostra inicial do fluxo um conjunto mais provável de parâmetros, os quais constituirão o descritor de tráfego da fonte (STD).|http://hdl.handle.net/10183/3181
VOIP : um estudo experimental|2001|Open Access|Dissertação|Redes : Computadores;Tcp/ip;Tráfego : Voz : Computadores;Trafego : Redes : Computadores|por|Voz sobre IP (VoIP) é uma tecnologia que permite a digitalização e a codificação da voz e o empacotamento em pacotes de dados IP para a transmissão em uma rede que utilize o protocolo TCP/IP. Devido ao volume de dados gerados por uma aplicação VoIP, esta tecnologia se encontra em funcionamento, em redes corporativas privadas. Mas se a rede base para o transporte desta aplicação for a Internet, certamente, não deve ser utilizada para fins profissionais, pois o TCP/IP não oferece padrões de QoS (Qualidade de Serviço) comprometendo desta forma a qualidade da voz. A qualidade da voz fica dependente do tráfego de dados existentes no momento da conversa. Para realizar um projeto de VoIP é necessário conhecer todo o tráfego existente na rede e verificar o quanto isto representa em relação à banda total da rede. Também se deve conhecer o tipo de aplicação que se deseja implantar, verificando a banda a ser utilizada por esta, e então projetar como a rede deverá ser estruturada. Para auxiliar no projeto de VoIP, pretende-se mostrar o que está sendo desenvolvido para que o protocolo TCP/IP ofereça QoS e uma ferramenta para análise do tráfego de voz sobre redes TCP/IP e também análises dos resultados obtidos em experimentos simulando diversas situações práticas.|http://hdl.handle.net/10183/3182
Simulação do protocolo de adaptação ATM tipo 2 (AAL2)|2001|Open Access|Dissertação|Redes : Computadores;Tráfego : Voz : Computadores;Simulacao : Redes : Computadores;Redes atm;Protocolo AAL2|por|Este trabalho descreve o protocolo de adaptação ATM do tipo 2 (AAL2), e expõe as mudanças que foram realizadas no simulador de rede ATM com a finalidade de estudá-lo. É proposto que se conheça melhor este novo padrão, planejado para baixa taixa de transferência e multiplexação de usuários e pode contribuir nos serviços de tráfego de voz, quando ocorre a integração de diversas aplicações que é o objetivo da B-ISDN. Após algumas explanações sobre o ATM em geral, é descrita de forma detalhada a recomendação I.362 que padroniza o novo AAL. É explicado o comportamento da máquina de estados de transmissão, e como ocorre a temporização para carregar diversos pacotes de voz numa mesma célula. Como foi idealizado para a classe de serviços do tipo VBR em tempo real, comentam-se alguns mecanismos próprios do AAL2 para controle de tráfego e sua influência. Descreve-se a subcamada SSCS proposta para pacotes longos, recentemente padronizada. São apresentados alguns artigos e estudos realizados sobre o AAL2 que descreve quantitativamente o ganho esperado pelo protocolo e levanta algumas questões comparando-o ao AAL1. Por ter sido inicialmente uma necessidade da telefonia celular, realiza-se um breve comentário de estudos dirigidos para a área.  Com o intuito de realizar a simulação do protocolo, foi utilizado o simulador de redes ATM, desenvolvido pelo NIST, que possui certas qualidades desejadas para este trabalho, porém, foram necessárias realizar modificações para implementar a camada AAL que não estava prevista na arquitetura original dele. Para se criar um ambiente de testes propício, e com a preocupação de não corromper o funcionamento padrão, foram criadas estruturas semelhantes às existentes e realizadas mudanças sutis, reconhecidas normalmente pelo simulador original. A partir destas simulações, pretende-se deixar uma ferramenta para fazer análises deste protocolo; utilizando modelos de tráfego de voz, a fim de obter informações do seu comportamento. No entanto, este estudo limitou-se somente a demonstrar a verificação e validação do simulador a partir dos resultados gerados. Para verificar a integridade do código fonte original foram utilizados os exemplos do próprio NIST e assim garantir que nada foi alterado. Além disso, o novo módulo foi comparado a norma através de um ""debug"". Na validação, devido ao fato de não existir uma rede real montada e disponível para testes, foram empregados artigos para comparar os resultados e demonstrar a boa aproximação obtida. Acredita-se que desta forma obteve-se o resultado desejado de um ambiente para estudo e compreensão do AAL2 e que, futuramente, pode ser usado para todos os protocolos da camada AAL.|http://hdl.handle.net/10183/3196
Bifocal tree : uma técnica para visualização de estruturas hierárquicas|2002|Open Access|Dissertação|Computação gráfica;Visualizacao : Informacao;Estruturas hierárquicas|por|Estruturas de informações organizadas hierarquicamente estão presentes em muitas áreas. Pode-se citar como exemplos diagramas organizacionais, árvores genealógicas, manuais, estruturas de diretórios, catálogos de bibliotecas, etc. Na última década, várias técnicas têm sido desenvolvidas a fim de permitir a navegação em espaços de informações organizados dessa forma. Essas técnicas buscam proporcionar uma melhor percepção de alguns atributos ou fornecer mecanismos de interação adicionais que vão além da tradicional navegação com barras de rolagem ou câmeras 3D em visualização bi e tridimensional, respectivamente. Dentre as várias alternativas de representação utilizadas nas diversas técnicas para dados hierárquicos destacam-se dois grandes grupos: as que utilizam a abordagem de preenchimento do espaço e as baseadas em diagramas de nodos e arestas. Na primeira o espaço disponível para a representação da estrutura é subdividido recursivamente, de forma que cada subárea representa um nodo da hierarquia. Na segunda, os nodos são representados por figuras geométricas e os relacionamentos, por linhas. Outro critério utilizado para classificá-las é a estratégia que cada uma aplica para exibir os detalhes presentes na estrutura. Algumas técnicas utilizam o método Foco+Contexto de modo a fornecer uma representação visual inteira do espaço de informações, bem como uma visão detalhada de itens selecionados na mesma área de exibição. Outras utilizam a abordagem Visão Geral+Detalhe que possui a característica de exibir essas duas partes (conjunto total e subconjunto de interesse) em áreas separadas.  O objetivo do presente trabalho é investigar a integração dessas duas abordagens a partir da proposta da técnica Bifocal Tree. Esta estrutura utiliza um diagrama de nodos e arestas e incorpora os conceitos existentes na abordagem Foco+Contexto guardando, porém uma divisão mais perceptível da visão de contexto e de detalhe. Ela introduz o uso de um segundo foco proporcionando duas áreas de visualização onde são exibidos dois sub-diagramas conectados entre si. Um corresponde à subárvore que contém o trecho da estrutura de interesse do usuário, enquanto o outro representa o contexto da hierarquia visualizada. Possui ainda alguns mecanismos de interação a fim de facilitar a navegação e a obtenção das informações exibidas na estrutura. Experimentos baseados em tarefas realizadas por usuários com a Bifocal Tree, o Microsoft Windows Explorer e o browser MagniFind foram utilizados para a avaliação da técnica demonstrando suas vantagens em algumas situações.|http://hdl.handle.net/10183/3212
Espalhamento de luz estático e dinâmico em polímeros do tipo polimetacrilato, fluorescentes por transferência protônica intramolecular no estado eletrônico excitado (TPIEE)|2001|Open Access|Dissertação|Espalhamento de luz estático;Espalhamento de luz dinâmico;Polimetacrilatos : Caracterização;Corantes orgânicos fluorescentes;Transferencia protonica intramolecular no estado excitado|por|As técnicas de espalhamento de luz estático e dinâmico foram utilizadas para a caracterização dos copolímeros de PMMA-benzazolas e do PMMA, em clorofórmio e THF nos regimes diluído e semi-diluído. Os copolímeros foram obtidos pela polimerização do metil-metacrilato na presença dos corantes orgânicos do tipo benzazolas, que caracterizam-se por apresentar uma intensa emissão de fluorescência através de um mecanismo de transferência protônica intramolecular no estado eletrônico excitado. Através da técnica de espalhamento de luz estático foram obtidos parâmetros macromoleculares como a massa molar ponderal média, o raio de giro e o segundo coeficiente virial, bem como o módulo osmótico reduzido e o parâmetro g, relacionado com a arquitetura do polímero em solução. Estes parâmetros indicam que os copolímeros e o PMMA em solução diluída comportam-se como cadeias lineares flexíveis e apresentam-se como esferas homogêneas em solução.  Através da espectroscopia de correlação de fótons foram obtidas funções normalizadas de correlação temporal de intensidade correspondentes a um único processo dinâmico tanto para o PMMA como para os copolímeros em regime diluído, independentemente do solvente utilizado. Esta dinâmica corresponde à difusão das cadeias poliméricas em solução. Para as soluções em regime semi-diluído (5£c£30 g·L-1), as funções de correlação temporal de intensidade apresentaram um único decaimento exponencial correspondente a difusão cooperativa dos entrelaçamento das cadeias poliméricas em solução pode ser observado em ambos os solventes. O tempo de relaxação ( t) obtido sugere que o copolímero apresenta, neste regime de diluição, uma dinâmica diferente daquela observada para o PMMA. Portanto, a incorporação da benzazola na cadeia polimérica afeta a dinâmica do polímero tanto em THF como em clorofórmio. Para soluções contendo PMMA e o Copolímero 6 na concentração de 60 g·L-1 em THF observou-se o aparecimento de um segundo movimento, mais lento, nas funções normalizadas de correlação temporal de intensidade, sugerindo a formação de uma estrutura contínua em solução. O mesmo não ocorre utilizando-se clorofórmio como solvente.|http://hdl.handle.net/10183/3219
CaTLeT : ferramenta computacional de apoio ao ensino/aprendizado de teoria das categorias|2002|Open Access|Dissertação|Informática : Educação;Teoria : Categorias;Ensino a distância|por|Teoria das Categorias é uma ramificação da Matemática Pura relativamente recente, tendo sua base sido enunciada ao final da primeira metade do século XX. Embora seja Teoria de grande expressividade, sua aplicação efetiva tem encontrado até o momento grandes obstáculos, todos decorrência natural da brevidade de sua História. A baixa oferta de bibliografia (e predominantemente em língua inglesa) e a falta de uniformidade na exposição do que sejam os tópicos introdutórios convergem e potencializam outro grande empecilho à sua propagação - a baixa oferta de cursos com enfoque em Teoria das Categorias. Consegue, a despeito destes obstáculos, arrebanhar admiradores em inúmeros centros de reconhecida excelência técnica e científica. Dentre todas as áreas do conhecimento, atrai em especial a atenção da Ciência da Computação, por características como independência de implementação, dualidade, herança de resultados, possibilidade de comparação da expressividade de outros formalismos, forte embasamento em notação gráfica e, sobretudo, pela expressividade de suas construções [MEN2001].  No Brasil, já conta com o reconhecimento de seu papel no futuro da Ciência da Computação por parte de instituições como SBC e MEC. Os obstáculos aqui descritos, entretanto, ainda necessitam ser transpostos. O presente trabalho foi desenvolvido visando contribuir nesta tarefa. O projeto consiste em uma iniciativa aplicada em Ciência da Computação, a qual visa oportunizar o franco acesso aos conceitos categoriais introdutórios: uma aplicação de computador que faça amplo uso de representação diagramática para apresentar a proposição de conceitos básicos do grupo de pesquisa em Teoria das Categorias do Instituto de Informática da UFRGS. A proposição e implementação de uma ferramenta, embora não constitua iniciativa inédita no mundo, até onde se sabe é a segunda experiência desta natureza. Ademais, vale destacar que os conceitos tratados, assim como os objetivos visados, são atendidos de forma única e exclusiva por esta aplicação. Conjuntamente, vislumbra-se a aplicação desenvolvida desempenhando importante papel de agente catalisador na propagação da visão dos Grupos de Pesquisa em Teoria das Categorias da UFRGS e da PUC/RJ do que sejam os ""conceitos categoriais introdutórios"".|http://hdl.handle.net/10183/3221
MEPSOM : método de ensino de programação sônica para músicos|2002|Open Access|Tese|Computação musical;Programação sônica : Método de ensino para músicos;Educação musical;Software educacional|por|Esta tese de doutorado apresenta o MEPSOM - Método de Ensino de Programação Sônica de Computadores para Músicos. O MEPSOM consiste em um sistema de computação que disponibiliza um conjunto de atividades para programação de software musical composto de exemplos e exercícios. O método foi idealizado para ser uma ferramenta de auxílio ao professor em cursos de Computação Musical, disponibilizando recursos didáticos para o ensino de programação nas áreas de composição e educação musical. O MEPSOM foi implementado sob a forma de programas de computador e utilizado em cursos de Computação Musical na UFRGS. Nesta Tese de Doutorado apresentamos o projeto e a organização do MESPCM, a implementação do método, relatos de sua aplicação e os resultados obtidos. Também expomos a utilização do método em laboratório, através de estudo de caso, e os resultados da sua avaliação por estudantes que participaram de pesquisas de levantamento. Por fim, a partir da análise dos dados obtidos, sugerimos um conjunto de aspectos considerados relevantes para futuras aplicações do MEPSOM.|http://hdl.handle.net/10183/3222
Dinâmica de paredes de domínios magnéticos : um estudo através da impedanciometria|2001|Open Access|Tese|Paredes de domínios magnéticos;Medida de impedância elétrica;Densidade;Medida de largura;Medida de velocidade;Permeabilidade;Efeito de penetração;Condutividade elétrica;Ferro;Silício;Materiais nanoestruturados;Recozimento;Anisotropia magnética|por|Neste trabalho é apresentado um estudo sobre a dinâmica de paredes de domínios através de medidas de impedanciometria. É proposto um método que permite a obtenção dos seguintes parâmetros das amostras estudadas: mobilidade e velocidade crítica das paredes, largura dos domínios e de suas paredes, densidade de energia de parede e constante efetiva de troca. Todas essas informações são obtidas a partir do espectro em freqüência da permeabilidade e de relações apropriadas entre a permeabilidade e a impedância complexa. O elo de ligação entre essas quantidades é feita através do efeito da profundidade de penetração, cuja definição inclui a freqüência, a resistividade e a permeabilidade do material. O método foi aplicado ao estudo de dois tipos diferentes de materiais, (i) (110)[001]FeSi3%, policristalino e altamente texturizado com tamanho de grãos bastante grande (~ 5 mm) e (ii) amostras nanocristalinas obtidas através do recozimento de fitas amorfas de Fe73.5Cu1Nb3Si16.5B6. Enquanto o primeiro sistema foi utilizado para se fazer uma comparação entre os parâmetros aqui obtidos com aqueles de outros autores e técnicas, o segundo foi estudado em termos das modificações da anisotropia magnética associadas ao alívio das tensões internas com a temperatura de recozimento.|http://hdl.handle.net/10183/3223
Efeitos de alta densidade em processos Drell-Yan de altas energias|2002|Open Access|Dissertação|Cromodinâmica quântica;Interacoes fortes de particulas elementares;Altas energias;Hadrons;Partons;Quarks;Gluons;Espalhamento;Leptons;Densidade;Colisao de ions pesados|por|Um dos problemas teóricos mais importantes da Física de Partículas de Altas Energias é a investigação de efeitos de alta densidade na Cromodinâmica Quântica (QCD), que é a teoria que descreve as interações fortes. Tais efeitos são importantes pois determinam os observáveis em colisõesde altas energias. Em processos hadrônicos de energia suficientemente alta, espera-se a formação de sistemas densos o suficiente para que efeitos não lineares de QCD passem a ser significativos na descrição e na unitarização da seção de choque. Na descrição de processos de espalhamento de altas energias, evidências experimentais indicam que os hádrons são constituídos por partículas puntuais, as quais chamamos de pártons. Os pártons carregam uma fração x do momentum total do hádron, e são de dois tipos, quarks e glúons. Na interação entre as partículas ocorre a troca de momentum, definida como Q2. A descrição perturbativa padrão para a evolução dinâmica das distribuições de quarks q(x, Q2) e glúons g(x, Q2), pode ser dada pelas equações de evolução DGLAP, e tem obtido sucesso na descrição dos resultados experimentais para as presentes energias. Na evolução DGLAP, são considerados apenas processos de emissão, como a emissão de um glúon por um quark, o decaimento de um glúon em um par de quarks ou em um par de glúons  Estes processos de emissão tendem a aumentar a densidade de pártons na região de pequeno momentum, levando a um crescimento ilimitado das distribuições partônicas para x -+ O. Assim, é esperado que o crescimento da densidade de pártons leve a interação e recombinação destas partículas, dando origem a termos não lineares nas equações de evolução. O resultado seria um processo de saturação das distribuições de pártons na região de alta energia e pequena fração de momentum. Os efeitos que dão origem à redução do crescimento das distribuições de quarks e glúons em relação a evolução linear são chamados genericamente de efeitos de sombreamento. Um dos aspectos fenomenológicosinteressantes a ser investigado no regime cinemático abordado acima é o processo Drell-Yan de alta energia, o qual consiste em processos de espalhamento pp, pA e AA com a produção de pares de léptons. Com o advento dos novos aceleradores, novos resultados experimentais estarão disponíveis na literatura relacionados com este processo. Em nosso trabalho investigamos os efeitos das correções de unitariedade em processos pp, bem como os efeitos devido a presença do meio nuclear em colisõespA e AA, nas distribuições de quarks e glúons, para a descrição da seção de choque diferencial para o processo Drell-Yan em colisõespp, pA e AA, para energias existentes nos novos aceleradores RHIC (Relativistic Heavy Ion Collider) e LHC (Large Ion Collider). Os efeitos de alta densidade são baseados no formalismo de Glauber-Mueller. Os resultados aqui apresentados mostram que os efeitos de alta densidade nas distribuições partônicas são importantes para altas energias, pois a descrição da seção de choque para o processo Drell-Yan, quando os efeitos de alta densidade são considerados, apresenta significativas diferenças da descrição onde não considera-se tais efeitos.|http://hdl.handle.net/10183/3224
Medidas de estruturas cardíacas fetais através de imagens ecocardiográficas segmentadas|2002|Open Access|Dissertação|Informática médica;Processamento de imagens;Redes neurais|por|O presente trabalho implementa um método computacional semi-automático para obter medidas de estruturas cardíacas de fetos humanos através do processamento de imagens de ultra-som. Essas imagens são utilizadas na avaliação cardíaca pré-natal, permitindo que os médicos diagnostiquem problemas antes mesmo do nascimento. A dissertação é parte de um projeto desenvolvido no Instituto de Informática da Universidade Federal do Rio Grande do Sul, denominado SEGIME (Segmentação de Imagens Médicas). Neste projeto, está sendo desenvolvida uma ferramenta computacional para auxiliar na análise de exames ecocardiográficos fetais com o apoio da equipe de Cardiologia Fetal do Instituto de Cardiologia do Rio Grande do Sul. O processamento de cada imagem é realizado por etapas, divididas em: aquisição, pré-processamento, segmentação e obtenção das medidas. A aquisição das imagens é realizada por especialistas do Instituto de Cardiologia. No pré-processamento, é extraída a região de interesse para a obtenção das medidas e a imagem é filtrada para a extração do ruído característico das imagens de ultra-som. A segmentação das imagens é realizada através de redes neurais artificiais, sendo que a rede neural utilizada é conhecida como Mapa Auto-organizável de Kohonen. Ao final do processo de segmentação, a imagem está pronta para a obtenção das medidas.  A técnica desenvolvida nesta dissertação para obtenção das medidas foi baseada nos exames realizados pelos especialistas na extração manual de medidas. Essa técnica consiste na análise da linha referente à estrutura de interesse onde serão detectadas as bordas. Para o início das medidas, é necessário que o usuário indique o ponto inicial sobre uma borda da estrutura. Depois de encontradas as bordas, através da análise da linha, a medida é definida pela soma dos pixels entre os dois pontos de bordas. Foram realizados testes com quatro estruturas cardíacas fetais: a espessura do septo interventricular, o diâmetro do ventrículo esquerdo, a excursão do septum primum para o interior do átrio esquerdo e o diâmetro do átrio esquerdo. Os resultados obtidos pelo método foram avaliados através da comparação com resultados de referência obtidos por especialistas. Nessa avaliação observou-se que a variação foi regular e dentro dos limites aceitáveis, normalmente obtida como variação entre especialistas. Desta forma, um médico não especializado em cardiologia fetal poderia usar esses resultados em um diagnóstico preliminar.|http://hdl.handle.net/10183/3225
Polipropileno graftizado com metacrilato glicidila como agente compatibilizante em blendas de PP e nylon 6|2001|Open Access|Dissertação|Polipropileno;Misturas poliméricas : Nylon|por|Neste trabalho foi feito uma avaliação do polipropileno funcionalizado com metacrilato de glicidila (PP-GMA) como agente compatibilizante em blendas reativas de polipropileno e nylon 6. As reações de funcionalização do polipropileno (PP) com metacrilato de glicidila (GMA) foram feitas em estado fundido e os polímeros modificados (PP-GMA) foram utilizados em misturas de PP com nylon 6, para a avaliação das propriedades mecânicas, térmicas e morfológicas da blenda. As blendas foram preparadas variando a concentração de nylon 6 e também do agente compatibilizante. Após o estudo da ação da concentração de nylon 6 e do agente compatibilizante nas blendas, foi verificado o comportamento do PP-GMA com diferentes teores de incorporação molar do GMA no polipropileno. Utilizou-se dois tipos de nylon 6, um deles com maior número de terminais carboxílicos, a fim de avaliar o efeito desses grupos na compatibilidade do sistema. As blendas obtidas foram caracterizadas por calorimetria diferencial de varredura, microscopia eletrônica de varredura e análise das propriedades mecânicas e dinâmico-mecânicas.  Os resultados mostraram que a adição do PP-GMA provoca alterações na morfologia das blendas, apresentando uma melhor dispersão e redução no tamanho das partículas dispersas. Além disso, há um aumento nas propriedades mecânicas comparado com as propriedades das blendas não compatibilizadas, que pode ser atribuído a melhor adesão entre as fases para ambos os nylons utilizados neste trabalho. Foi feito estudo comparativo entre as blendas de PP/PP-GMA/Ny6 e PP/PP-MA/Ny6 na proporção de 63/7/30. O efeito do tipo de agente compatibilizante foi caracterizado por calorimetria diferencial de varredura, microscopia eletrônica de varredura e análise das propriedades mecânicas e dinâmico-mecânicas. Os resultados indicaram que o polipropileno modificado com anidrido maleico (PP-MA) mostrou um melhor efeito compatibilizante nesses sistemas. Ou seja, a interação dos terminais amino do nylon foram mais suscetíveis a ação do grupamento anidrido no PP-MA, do que o grupamento epóxido no PP-GMA.|http://hdl.handle.net/10183/3231
A sincronização de osciladores de Rössler acoplados|2002|Open Access|Dissertação|Sistemas dinâmicos;Dinâmica não-linear;Sistemas não lineares;Caos;Osciladores;Sincronizacao;Métodos Lyapunov;Estabilidade asimptótica|por|Neste trabalho utiliza-se como sistema dinâmico o circuito eletrônico que integra o sistema de equações acopladas de Rossler modificado. Este sistema possui uma nãolinearidade dada por uma função linear por partes e apresenta comportamento caótico para certos valores dos seus parâmetros. Isto e evidenciado pela rota de dobramento de período obtida variando-se um dos parâmetros do sistema. A caracterização experimental da dinâmica do sistema Rossler modificado e realizada através do diagrama de bifurcações. Apresenta-se uma fundamentação teórica de sistemas dinâmicos introduzindo conceitos importantes tais como atratores estranhos, variedades invariantes e tamb em uma análise da estabilidade de comportamentos assintóticos como pontos fixos e ciclos limites. Para uma caracterização métrica do caos, apresenta-se a definção dos expoentes de Lyapunov. São introduzidos também os expoentes de Lyapunov condicionais e transversais, que estão relacionados com a teoria de sincronizção de sistemas caóticos. A partir de uma montagem mestre-escravo, onde dois osciladores de Rossler estão acoplados unidirecionalmente, introduz-se a de nição de sincronização idêntica, sincronização de fase e variedade de sincronização.  Demonstra-se a possibilidade de sincronização em uma rede de osciladores caóticos de Rossler, acoplados simetricamente via acoplamento de primeiros vizinhos. A rede composta por seis osciladores mostrou ser adequada pelo fato de apresentar uma rica estrutura espacial e, ao mesmo tempo, ser experimentalmente implementável. Além da sincronização global (osciladores identicamente sincronizados), obtém-se a sincronização parcial, onde parte dos osciladores sincronizam entre si e a outra parte não o faz. Esse tipo de sincronização abre a possibilidade da formação de padrões de sincronização e, portanto, exibe uma rica estrutura de comportamentos dinâmicos. A sincronização parcial e investigada em detalhes e apresentam-se vários resultados. A principal ferramenta utilizada na análise experimental e numérica e a inspeção visual do gráfico yi yj , fazendo todas as combinações entre elementos diferentes (i e j) da rede. Na análise numérica obtém-se como resultado complementar o máximo expoente de Lyapunov transversal, que descreve a estabilidade da variedade de sincronização global.|http://hdl.handle.net/10183/3250
Competições magnéticas no sistema Fe xCo 1-xTa 2 O 6|2003|Open Access|Tese|Difração de raios X;Difração de nêutrons;Suscetibilidade magnética;Magnetização;Espectroscopia mossbauer;Calor especifico;Ferro;Cobalto;Temperatura de Neel;Gradiente de campo elétrico|por|São apresentados resultados obtidos a partir de difração de raios-X (DRX), difração de nêutrons (DN), susceptibilidade magnética (X(T)), magnetização (M(H)), espectroscopia Mõssbauer (EM) e calor específico (Cp) de amostras do sistema FexCo1-x Ta206. Difratogramas de DRX e de DN e as curvas M(H) indicam que as amostras estão bem cristalizadas e homogêneas, e que o sistema é uma solução sólida para toda faixa de substituição Fe -> Co. Os ajustes de DN revelam fases magnéticas com dois vetores de propagação (::I::~~ ~) para o CoTa206 e (~ O ~) e (O ~ ~) para o FeTa206' A segunda configuração permanece a mesma para as amostras ricas em Fe (0,46 :S x < 1,00), enquanto que as amostras ricas em Co (0,09 :S x < 0,46) apresentam configuração magnética indexada pelos vetores de propagação (::I::~~ O). O diagrama de fase temperatura vs. x exibe um ponto bicrítico em torno de T = 4,9 K e x = 0,46. A temperatura de Néel máxima das região rica em Fe é 9,5 K, e 7,1 K para a região rica em Co. No ponto bicrítico, o sistema mostra coexistência de ambas estruturas magnéticas. Esse comportamento bicrítico é interpretado como sendo induzido pelas competições entre as diferentes fases magnéticas e pela variação das propriedades cristalográficas.|http://hdl.handle.net/10183/3251
Mecanismo de suporte à percepção em ambientes cooperativos|2001|Open Access|Dissertação|Sistemas : Informação;Trabalho cooperativo|por|O objetivo deste trabalho é desenvolver um mecanismo para suporte à percepção de eventos no passado. Percepção pode ser conceituada como o conhecimento sobre as atividades do grupo, passadas, presentes e futuras, sobre o próprio grupo e seu status geral. Sem este conhecimento, o trabalho cooperativo coordenado e estruturado torna-se quase impossível. O suporte à percepção pode ser dividido em seis questões (o que, quando, como, onde, quem e quanto), analisadas sob ponto de vista de sistemas assíncronos e síncronos. A questão “quando” analisa o momento em que ocorre uma atividade, o que gera um evento, podendo ser no “passado”, “passado contínuo”, “presente” ou “futuro”. Uma atividade no “passado” é aquela que foi concluída em um momento passado e cujo registro interessa às outras atividades. Apesar de sua importância, o suporte à percepção de eventos no passado é ainda muito limitado nas ferramentas de groupware hoje disponíveis. Como conseqüência, situações como a ausência de um membro do grupo por um certo período de tempo não são tratadas. Como estas situações de ausência são bastante comuns, durante o trabalho em grupo, o seu tratamento é fundamental em um groupware.  Desta forma, a ausência de membros do grupo exige a contextualização não apenas daqueles que continuam no trabalho, mas, principalmente, daqueles que retornam ao ambiente cooperativo. Neste trabalho, é apresentado um mecanismo flexível para o suporte à percepção de eventos no passado destinado a cobrir a referida contextualização. Este mecanismo foi construído na forma de um framework, projetado para ser flexível a ponto de poder ser incluído em qualquer ferramenta de groupware, desde que seu autor o queira. Este framework, chamado de BW (Big Watcher), foi organizado em quatro pacotes: três independentes, que trocam informações, descritas no quarto pacote, através somente de classes de fachada. Estas informações são essencialmente eventos, os quais representam as atividades realizadas e já concluídas por algum membro desempenhando um papel dentro do grupo. Estas atividades são registradas pelo groupware junto ao framework, de modo que este groupware possa, através do framework, contextualizar seus membros. Além disso, o groupware também pode especializar várias classes dentro do framework BW, como a descrição dos papéis e a própria descrição dos eventos. Assim, este framework pode ser integrado a qualquer ferramenta de groupware em ambiente assíncrono que necessite de um mecanismo para o suporte à percepção de eventos no passado, para evitar que situações de ausência prejudiquem o andamento dos trabalhos. Finalmente, foi implementado e testado o framwork BW sobre o groupware CUTE/COPSE para validar as idéias desta dissertação.|http://hdl.handle.net/10183/3253
A equação de Poisson-Boltzmann em regiões com fronteira irregular|2002|Open Access|Dissertação|Equações diferenciais parciais;Espacos de Sobolev;Princípios variacionais|por|Propomos uma idealização da situação em que uma macromolécula é ionizada em um solvente. Neste modelo a área da superfície da molécula é suposta ser grande com respeito a seu diâmetro. A molécula é considerada como um dielétrico com uma distribuição de cargas em sua superfície. Utilizando as condições de transmissão, a distribuição de Boltzmann no solvente e resultados recentes sobre espaços de Sobolev no contexto de espaços métricos, bem como de integração sobre superfícies irregulares, o problema é formulado em forma variacional. Resultados clássicos do cálculo de variações permitem a resolução analítica do problema.|http://hdl.handle.net/10183/3257
A mineração na porção média da Planície Costeira do Rio Grande do Sul : estratégia para a gestão sob um enfoque de gerenciamento costeiro integrado|2002|Open Access|Tese|Geologia marinha;Planicie costeira : Rio Grande do Sul : Avaliacao geoambiental;Mineração : Rio Grande do Sul;Gerenciamento costeiro : Rio Grande do Sul|por|A porção média da Planície Costeira do Rio Grande do Sul constitui-se numa região crítica em termos de planejamento de uso devido a uma estreita conjunção de fatores de ordem econômica, ambiental, social e histórico-cultural, estabelecida, em princípio, pela presença de um importante complexo estuarinolagunar. Nessa área, bem como no restante da zona costeira do Brasil, as diretrizes para o uso sustentável dos recursos naturais estão materializadas em leis e programas governamentais, dos quais o Plano Nacional de Gerenciamento Costeiro representa a linha mestra para as ações nos três níveis de governo. A exploração de recursos minerais nessa região é uma atividade antiga, relativamente de pouca expressão no contexto estadual, mas de grande significado social, econômico e cultural em nível regional, sustentando a demanda de vários municípios da região. Caracteriza-se principalmente pela exploração de areia e argila para uso na construção civil e para aterro, apresentando ainda potencialidade alta para exploração de turfa e minerais pesados. Com o objetivo de contribuir para a solução dos conflitos gerados por um modelo de exploração mineral ainda inconsistente com as demandas atuais de conservação e desenvolvimento, realizou-se uma análise ambiental integrada da área dos entornos do estuário da Laguna dos Patos, compreendendo os municípios de Pelotas, Rio Grande e São José do Norte.  A análise considerou os marcos legais e institucionais, as características diferenciadas do meio físico-natural, os processos econômicos, sociais e culturais, as características da atividade de mineração na região e suas repercussões e interações no sistema ambiental como um todo. As informações disponíveis permitiram a geração de um banco de dados no Sistema de Informações Geográficas IDRISI 32®, na escala 1: 100.000, o qual forneceu a base para a análise interpretativa. Utilizando técnicas de geoprocessamento obteve-se uma síntese dos diagnósticos realizados através da definição, mapeamento e descrição de 19 unidades de planejamento, denominadas unidades geoambientais, posteriormente detalhadas em 108 unidades físico-naturais. A síntese de uma grande quantidade de dados, espacializada na forma de um mapa digital, auxiliou a definição dos critérios para elaboração de um mapa de vulnerabilidade ambiental relativa para a região.  Este, aliado ao plano de informação que contém todas as áreas com restrição legal de uso, possibilitou o mapeamento das áreas mais críticas para gestão ambiental. Adicionalmente, considerando a potencialidade de recursos minerais para uso na construção civil e para aterro, os critérios que determinam a maior ou menor atratividade econômica para a sua exploração e as áreas mais críticas em termos de gestão ambiental, elaborou-se um mapa prescritivo que indica as áreas que devem ser consideradas prioritárias para um gerenciamento preventivo. Finalmente, a análise ambiental integrada permitiu a elaboração de um modelo de um plano de gestão para o setor, onde é apresentada uma estrutura seqüencial e ordenada do plano, exemplificada, em cada passo, com as informações disponíveis.|http://hdl.handle.net/10183/3274
Estudo de reatividade e resolução enantiomérica do (+-)-10-exo-hidróxipentaciclo[6.2.1.1 3,6.0 2,7. 0 5.9] dodecan-4-ona|2001|Open Access|Dissertação|Policiclicos : Sintese organica;Catálise enzimática|por|Neste trabalho, realizou-se a síntese do ceto-álcool pentaciclíco (±)-5, assim como o estudo de reatividade do grupo carbonila do mesmo frente a reações de oximação e redução. Realizou-se também a resolução enantiomérica do composto (±)-5 através de reação de transesterificação com acetato de vinila catalisada pela lipase da Candida rugosa. Altos excessos enantioméricos foram obtidos (>95%, RMN) tanto para o álcool (+)-5 quanto para o éster formado (-)-8. Sugere-se a existência de uma interconversão enantiomérica no composto (+)-5, devido a observação de mistura racêmica, quando o mesmo foi analisado por cromatografia gasosa em coluna quiral. Um mecanismo para tal interconversão, o qual envolve um rearranjo intramolecular, é proposto.|http://hdl.handle.net/10183/3281
Análise do modelo t-J e sua aplicação aos compostos de óxidos de cobre|1999|Open Access|Dissertação|Sistemas eletronicos fortemente correlacionados;Modelo t-j;Cobre;Altas temperaturas criticas;Interacoes eletron-eletron;Modelo de heisenberg;Supercondutores de acoplamento forte;Polarons;Fermions;Metodos de funcoes de green;Diagramas de feynman;Tunelamento|por|Neste trabalho estudamos modelos teóricos que descrevem sistemas eletrônicos fortemente correlacionados, em especial o modelo t-J, e suas aplicações a compostos de óxidos de cobre, notadamente os compostos que apresentam supercondutividade de alta temperatura crítica e o composto Sr2CuO2Cl2. No primeiro capítulo do trabalho, fazemos uma exposição de três modelos que envolvem o tratamento das interações elétron-elétron, que são os modelos de Hubbard de uma banda, o modelo de Heisenberg e o modelo t-J. Na dedução deste último fazemos uma expansão canônica do hamiltoniano de Hubbard, no limite de acoplamento forte, levando-nos a obter um novo hamiltoniano que pode ser utilizado para descrever um sistema antiferromagnético bidimensional na presença de lacunas, que é exatamente o que caracteriza os compostos supercondutores de alta temperatura crítica na sua fase de baixa dopagem. Após termos obtido o hamiltoniano que descreve o modelo t-J, aplicamos à este uma descrição de polarons de spin, numa representação de holons, que são férmions sem spin, e spinons, que são bósons que carregam somente os graus de liberdade de spin. Utilizando uma função de Green para descrever a propagação do polaron pela rede, obtemos uma equação para a sua autoenergia somando uma série de diagramas de Feynman, sendo que para este cálculo utilizamos a aproxima ção de Born autoconsistente [1]. Do ponto de vista numérico demonstramos que a equação integral de Dyson resultante do tratamento anterior não requer um procedimento iterativo para sua solução, e com isto conseguimos trabalhar com sistemas com grande número de partículas. Os resultados mostram, como um aspecto novo, que o tempo de vida média do holon tem um valor bastante grande no ponto (Л, 0) da rede recíproca, perto da singularidade de Van Hove mencionada na literatura [2]. Este aspecto, e suas implicações, é amplamente discutido neste capítulo. No capítulo 3 estudamos o modelo estendido t-t'-J, com tunelamento à segundos vizinhos e a incorporação dos termos de três sítios [3]. Fazemos a mesma formulação do capítulo anterior, e discutimos as aplicações dos nossos resultados ao óxido mencionado anteriormente. Finalmente, no último capítulo apresentamos uma aplicação original do modelo t-J à uma rede retangular, levemente distorcida, e demonstramos que os resultados do capítulo 3 são reproduzidos sem necessidade de introduzir termos de tunelamento adicionais no hamiltoniano. Esta aplicação pode se tornar relevante para o estudo das fases de tiras encontradas recentemente nesses materiais de óxidos de cobre.;In this work we study theoretical models to describe strongly correlated electron systems, specially the t-J model, and its applications to the copper oxide compounds, particularly those compounds which exhibit high temperature superconductivity and the compound Sr2CuO2Cl2. In the first chapter of this work, we make a description of the three models involving the treatment of the electron-electron interactions, which are the oneband Hubbard model, the Heisenberg model, and the t-J model. In the deduction of the last one we use a canonical expansion of the Hubbard hamiltonian, in the strong coupling limit, leading us to get a new effective hamiltonian that can be used to describe a two-dimensional antiferromagnetic system in the presence of holes, which is precisely what characterizes the high temperature superconducting compounds at the low doping regime. After having obtained the effective hamiltonian that describes the t-J model, we apply it to a spin polaron description in a holon representation, which is a spinless fennion, and spinons, which are bosons canying only the spins degrees of freedom. Using a Green function formulation to describe the polaron propagation through the lattice, we obtain an equation for the self-energy by summing a series of Feynman diagrams. For this calculation we use the self-consistent Bom approximation [1]. From the numerical viewpoint we demonstrate that resulting integral Dyson equation of the former treatment do not require an iterative procedure for his solution, which allows us the work with systems of a large number of particles. The results show, as a new aspect, that the lifetime of the holon has a vecy large value at the point (Л, 0) of the reciprocal lattice, close to the van Hove singularity already mentioned in the literature [2]. This aspect and its implications is widely discussed in this chapter. In chapter 3 we study the extended t-t'-J model, with hopping to second neighbors and the inclusion of three-site terms [3]. We make the same formulation as in the previous chapter and discuss the application of our results to the insulating compound mentioned above. Finally, in the last chapter we present an original application of the t-J model to a rectangular lattice, slightly distorted, and demonstrate that the results obtained in chapter 3 are reproduced without necessity to introduce additional hopping terms into the hamiltonian. This application may become relevant for the study of the stripe phases recently found in these copper oxide materiais.|http://hdl.handle.net/10183/3307
A morfodinâmica praial como subsídio ao gerenciamento costeiro : o caso da Praia de Fora - Parque Estadual de Itapuã, RS|2002|Open Access|Dissertação|Gerenciamento costeiro;Morfodinâmica praial;Dinâmica sedimentar;Parque Estadual de Itapuã (Viamão, RS)|por|O Parque Estadual de Itapuã é considerado a última área representativa dos ecossistemas originais da região metropolitana de Porto Alegre, sendo que nos últimos 11 anos somente foi permitida a entrada de pesquisadores em suas dependências. Após este longo período em que esteve fechado, o Parque voltará a receber turistas no início de 2002, sendo que os principais destinos serão as praias. Dentre as oito praias existentes na área, somente três serão abertas à visitação, sendo a Praia de Fora a maior delas e a única voltada para a Lagoa dos Patos. Com objetivo de fornecer subsídios para o gerenciamento da Praia de Fora, efetuou-se a análise de uma série de parâmetros morfodinâmicos consagrados na literatura mundial, aprimorando o nível de conhecimento desta praia e suas relações com a Lagoa dos Patos. Para tanto, utilizou-se os modelos de praias de Banco Único e de Bancos Múltiplos da chamada Escola Australiana com a aplicação dos seguintes parâmetros: Parâmetro Adimensional Omega (Ù), Omega Teórico (Ùterorico), Parâmetro de Banco (B), Declividade da Face de Praia (Tang ß) e Parâmetro Dimensionador de Surfe (surf scaling parameter). Outros modelos utilizados foram o Perfil de Equilíbrio de DEAN (1973), o modelo de Transporte Longitudinal dos sedimentos da zona de surfe, (SPM, 1984), o Limite Externo e Limite Interno (Profundidade de Fechamento) e o modelo de Sentido Preferencial de Transporte Perpendicular (SUNAMURA & TAKEDA, 1984). Além destes, optou-se por realizar uma análise temporal da variação da linha de praia da Praia de Fora no período entre 1978 e 2001.  Os resultados obtidos indicam um ambiente com uma grande variação energética anual, sendo que a largura da face praial apresentou variações de até 16 m e a declividade de 2,8° a 11.3°. A análise dos parâmetros morfodinâmicos indicou uma praia com grande alternância de estágios morfodinâmicos, que reflete a presença de uma zona de transporte de sedimentos muito intenso por ação de ondas até a profundidade de 0,90 m, uma zona de transporte significativo entre 0,90 e 5 m e uma zona de transporte incipiente para as profundidades maiores que 6 m. O transporte no sentido longitudinal, apresenta uma bidirecionalidade com uma pequena resultante para SE e em sentido transversal apresenta uma maior incidência de transporte da zona de surfe para a antepraia, caracterizando setores erosivos na Praia de Fora. Estes setores erosivos foram detectados em 58% da extensão da Praia de Fora com uma taxa média de 3,75 m por ano. Já em 14% da extensão da praia foi observado acresção e em 28% da praia não houve variação significativa da posição da linha de praia. As taxas médias de deposição chegaram a 1,75 m por ano. A sistematização de todos os resultados permitiu a elaboração de subsídios para o gerenciamento da Praia de Fora. São eles: análise dos riscos para a segurança de banhistas na praia, manejo de dunas, restrições a explotação de areia de fundo nas adjacências da Praia de Fora e delimitação submersa do Parque de Itapuã junto à cota batimétrica dos –6m. Esta delimitação aumentaria em aproximadamente 10.920 hectares ou 196 %, a área atual de preservação do Parque de Itapuã.|http://hdl.handle.net/10183/3309
Inserção de testabilidade em um núcleo pré-projetado de um microcontrolador 8051 fonte compatível|2002|Open Access|Dissertação|Microeletrônica;Testes : Circuitos integrados;Sistemas digitais;Microprocessadores;Microcontroladores|por|No intuito de validar seus projetos de sistemas integrados, o Grupo de Microeletrônica da UFRGS tem investido na inserção de estruturas de teste nos núcleos de hardware que tem desenvolvido. Um exemplo de tal tipo de sistema é a “caneta tradutora”, especificada e parcialmente desenvolvida por Denis Franco. Esta caneta se utiliza de um microcontrolador 8051 descrito em VHDL, o qual ainda carece de estruturas dedicadas com funções orientadas à testabilidade. Este trabalho exemplifica a integração de teste em um circuito eletrônico préprojetado. Neste caso específico, foi utilizado o microcontrolador 8051 fonte compatível que será inserido no contexto da caneta tradutora. O método utilizado apoiou-se na norma IEEE1149.1, destinada a definir uma infra-estrutura baseada na técnica do boundary scan para o teste de placas de circuito impresso. São apresentadas características de testabilidade desenvolvidas para o microcontrolador, utilizando-se a técnica do boundary scan em sua periferia e a técnica do scan path em seu núcleo. A inserção destas características de teste facilita a depuração e testes em nível de sistema, imaginando-se o sistema como algo maior, fazendo parte do sistema da caneta tradutora como um todo. São elaborados exemplos de testes, demonstrando a funcionalidade do circuito de teste inserido neste núcleo e a possibilidade de detecção de falhas em pontos distintos do sistema. Finalmente, avalia-se o custo associado à integração desta infra-estrutura de teste, tanto em termos de acréscimo de área em silício, quanto em termos de degradação de desempenho do sistema.|http://hdl.handle.net/10183/3311
Estudo do crescimento de filmes de carbono sobre silício devido à irradiação com feixes de H e He|2001|Open Access|Dissertação|Feixes de íons;Impurezas;Gás;Carbono;Silício;Hélio;Hidrogênio;Temperatura;Íons;Retroespalhamento rutherford;Pressao;Densidade;Poder de freamento eletronico|por|É bem conhecido que técnicas experimentais que fazem uso de feixes iônicos induzem sobre a superfície dos alvos irradiados o depósito de diversos tipos de impurezas. Este depósito é causado pelas interações entre o feixe de íons, as moléculas de gás residual no interior das câmaras de irradiação e a superfície da amostra. Apesar do fato deste processo poder alterar significativamente os parâmetros de irradiações, bem como afetar a análise de materiais tratados, os parâmetros experimentais que influenciam na deposição das impurezas ainda não são bem conhecidos e nem o depósito se encontra suficientemente quantificado. No presente trabalho relatamos um estudo quantitativo da deposição de carbono sobre amostras de Si (100) irradiadas com feixes de He e H. A deposição de carbono foi determinada em função da fluência de irradiação, variando diversos parâmetros experimentais, tais como: pressão na câmara de irradiação, temperatura do alvo, densidade de corrente do feixe, energia do feixe e o estado da carga do íon. Em todos os casos a análise das amostras irradiadas foi feita pela técnica de Retroespalhamento de Rutherford em direção Canalizada (RBS/C) e através da reação nuclear ressonante 12C(a, a´)12C na energia de 4265 keV. Os resultados experimentais mostram que se consegue minimizar a deposição de C através: a) da redução do tempo de irradiação, b) da redução da pressão na câmara de irradiação, c) do aumento da temperatura do alvo durante a irradiação e d) minimização do poder de freamento do íon no alvo.|http://hdl.handle.net/10183/3312
Supercondutividade não adiabática em fuleretos alcalinos e correções de vértices|2001|Open Access|Dissertação|Supercondutores;Propriedades físicas;Fulerenos;Supercondutividade;Pares de Cooper;Teoria BCS;Compostos orgânicos;Líquido de Fermi;Interacoes eletron-fonon|por|Recentemente, tem sido questionada a validade do teorema de Migdal nos fuieretos dopados supercondutores. Motivados por esse problema, realizamos nesta dissertação uma revisão das propriedades físicas destes novos e notáveis materiais: os fulerenos e outros representantes desta família de compostos orgânicos que possuem estrutura geométrica de gaiola fechada. Em primeiro lugar abordamos, ainda que de maneira sucinta, alguns fundamentos da teoria microscópica BCS (Bardeen, Cooper e Schrieffer, 1961) da supercondutividade, tais como o problema da instabilidade do líquido de Fermi, a formação de pares de Cooper, o método da transformação canônica para demonstrar o aparecimento da interação efetiva atrativa entre os elétrons do par, as equações de Gor'kov demonstrando o surgimento do gap supercondutor, e a expressão BCS da temperatura crítica no limite de acoplamento fraco. Após, revisamos o trabalho realizado por Grimaldi, Cappelluti e Pietronero (1995), sobre a supercondutividade não adiabática nos fuieretos dopados, no qual são feitas correções de vértice para a interação elétron-fônon, usando o método perturbativo. Naquele trabalho eles utilizam um modelo de fônons de Einstein com uma única freqüência para caracterizar a função espectral de Eliashberg, necessária para obter tais correções de vértice  Nossa proposta neste trabalho é generalizar este modelo por um constituído de várias Lorentzianas truncadas, centradas nas freqüências dos principais modos de vibração da rede cristalina: os intermoleculares, os ópticos e os intramoleculares. Encontramos como resultado deste estudo que as correções de vértice, com contribuição multifonônica, introduzem modificações substancias como um aumento da temperatura crítica e variação no coeficiente isotópico, dando resultados mais próximos dos obtidos experimentalmente, em contraste daqueles obtidos na teoria de Migdal-Eliashberg, sem correções de vértice.|http://hdl.handle.net/10183/3313
Wireless LAN : a grande questão ; 802.11a ou 802.11b?|2002|Open Access|Dissertação|Redes : Computadores;IEEE 802.11;Redes locais : Computadores;Comunicação sem fio|por|Este trabalho apresenta, inicialmente, uma análise comparativa detalhada dos dois padrões, IEEE 802.11a e IEEE802.11b, que foram apresentados recentemente pelo IEEE na área de redes sem fio (wireless). São apresentadas as principais diferenças tecnológicas dos dois padrões, no que se refere, principalmente, à arquitetura, funções de controle, segurança, desempenho e custo de implementação destas duas tecnologias de redes wireless. São avaliados também os aspectos de interoperabilidade, quando estas redes são integradas em redes corporativas fixas, que são baseadas, principalmente, em redes Ethernet, tradicionalmente usadas em redes corporativas. São considerados também, aspectos de custo e flexibilidade de aplicação das duas tecnologias e mostram-se como estas diferenças devem ser levadas em conta em aplicações típicas de um ambiente corporativo. Finalmente, apresenta-se também, como estudo de caso, uma análise focalizada principalmente na integração da tecnologia wireless em aplicações típicas de uma grande empresa local. Consideram-se as vantagens e desvantagens de ambas as tecnologias, como solução para algumas aplicações típicas encontradas nesta empresa, e justifica-se a escolha da solução que foi adotada. Conclui-se com algumas projeções quanto ao futuro da tecnologia wireless no ambiente público e corporativo.|http://hdl.handle.net/10183/3314
Aquisição e otimização de mapas de navegação usando redes neurais|2001|Open Access|Dissertação|Modelos fisiológicos;Ciência da computação;Redes neurais;Navegação;Robótica;Matemática;Cartografia;Métodos de gradiente;Erros;Temperatura;Planejamento de caminho|por|A capacidade de encontrar e aprender as melhores trajetórias que levam a um determinado objetivo proposto num ambiente e uma característica comum a maioria dos organismos que se movimentam. Dentre outras, essa e uma das capacidades que têm sido bastante estudadas nas ultimas décadas. Uma consequência direta deste estudo e a sua aplicação em sistemas artificiais capazes de se movimentar de maneira inteligente nos mais variados tipos de ambientes. Neste trabalho, realizamos uma abordagem múltipla do problema, onde procuramos estabelecer nexos entre modelos fisiológicos, baseados no conhecimento biológico disponível, e modelos de âmbito mais prático, como aqueles existentes na área da ciência da computação, mais especificamente da robótica. Os modelos estudados foram o aprendizado biológico baseado em células de posição e o método das funções potencias para planejamento de trajetórias. O objetivo nosso era unificar as duas idéias num formalismo de redes neurais. O processo de aprendizado de trajetórias pode ser simplificado e equacionado em um modelo matemático que pode ser utilizado no projeto de sistemas de navegação autônomos. Analisando o modelo de Blum e Abbott para navegação com células de posição, mostramos que o problema pode ser formulado como uma problema de aprendizado não-supervisionado onde a estatística de movimentação no meio passa ser o ingrediente principal.  Demonstramos também que a probabilidade de ocupação de um determinado ponto no ambiente pode ser visto como um potencial que tem a propriedade de não apresentar mínimos locais, o que o torna equivalente ao potencial usado em técnicas de robótica como a das funções potencias. Formas de otimização do aprendizado no contexto deste modelo foram investigadas. No âmbito do armazenamento de múltiplos mapas de navegação, mostramos que e possível projetar uma rede neural capaz de armazenar e recuperar mapas navegacionais para diferentes ambientes usando o fato que um mapa de navegação pode ser descrito como o gradiente de uma função harmônica. A grande vantagem desta abordagem e que, apesar do baixo número de sinapses, o desempenho da rede e muito bom. Finalmente, estudamos a forma de um potencial que minimiza o tempo necessário para alcançar um objetivo proposto no ambiente. Para isso propomos o problema de navegação de um robô como sendo uma partícula difundindo em uma superfície potencial com um único ponto de mínimo. O nível de erro deste sistema pode ser modelado como uma temperatura. Os resultados mostram que superfície potencial tem uma estrutura ramificada.|http://hdl.handle.net/10183/3315
Estrutura nuclear de estrelas compactas|2000|Open Access|Dissertação|Estrelas;Matéria nuclear;Densidade;Hadrons;Quarks;Temperatura;Composicao estelar;Propriedades mecânicas;Dinâmica;Estática|por|Este trabalho tem como objetivo o estudo da matéria nuclear a altas densidades considerando-se as fases hadrônica e de quarks à temperatura nula e finita, com vistas a aplicações no estudo de propriedades estáticas globais de estrelas compactas. Parte dos cálculos apresentados nesta dissertação foram realizados por diferentes autores. Entretanto, em geral, estes trabalhos limitaram-se ao estudo da matéria nuclear em regiões de densidades e temperaturas específicas. Este estudo visa, por sua vez, o desenvolvimento de um tratamento amplo e consistente para estes sistemas, considerando-se diferentes regimes de densidade e temperatura para ambas as fases, hadrônica e de quarks. Buscamos com isso adquirir conhecimento suficiente que possibilite, não somente a ampliação do escopo dos modelos considerados, como também o desenvolvimento, no futuro, de um modelo mais apropriado à descrição de propriedades estáticas e dinâmicas de estrelas compactas. Ainda assim, este trabalho apresenta novos aspectos e resultados inéditos referentes ao estudo da matéria nuclear, como descrevemos a seguir. No estudo da matéria nuclear na fase hadrônica, consideramos os modelos da teoria quântica de campos nucleares desenvolvidos por J. D. Walecka, J. Zimanyi e S. A. Moszkowski, e por J. Boguta e A. R. Bodmer, e conhecidos, respectivamente, como Hadrodinâmica Quântica, ZM e Não-Linear. Nestes modelos a matéria nuclear é descrita a partir de uma formulação lagrangeana com os campos efetivos dos bárions acoplados aos campos dos mésons, responsáveis pela interação nuclear. Neste estudo consideramos inicialmente a descrição de propriedades estáticas globais de sistemas nucleares de muitos corpos à temperatura nula, como por exemplo, a massa efetiva do núcleon na matéria nuclear simétrica e de nêutrons. A equação de estado da matéria de nêutrons possibilita a descrição de propriedades estáticas globais de estrelas compactas, como sua massa e raio, através da sua incorporação nas equações de Tolman, Oppenheimer e Volkoff (TOV). Os resultados obtidos nestes cálculos estão em plena concordância com os resultados apresentados por outros autores. Consideramos posteriormente o estudo da matéria nuclear com graus de liberdade de bárions e mésons à temperatura finita, com particular atenção na região de transição de fase. Para este estudo, incorporamos aos modelos considerados, o formalismo da mecânica estatística à temperatura finita. Os resultados obtidos, para as propriedades da matéria nuclear à temperatura finita, concordam também com os resultados obtidos por outros autores. Um aspecto inédito apresentado neste trabalho refere-se à incorporação de valores para os pontos críticos da transição de fase, ainda não determinados por outros autores. O comportamento do calor específico também é analisado de forma inédita nesta dissertação no tratamento utilizado com os modelos Não-Linear e ZM. Utilizamos a equação de estado da matéria de nêutrons à temperatura finita nas equações TOV, determinando propriedades globais de uma estrela protoneutrônica. Observamos neste trabalho que ocorre um aumento da massa máxima da estrela com o aumento da temperatura, comportamento este já previsto por outros autores em diferentes modelos. Posteriormente incorporamos ao formalismo à temperatura finita, o equilíbrio químico, a presença de graus de liberdade leptônicos para elétrons e múons e a neutralidade de carga. Apresentamos nesta etapa do trabalho, uma forma alternativa para a incorporação destes ingredientes, baseada na determinação de uma fração relativa entre os potenciais químicos de prótons e nêutrons, à temperatura nula, extendendo este resultado à temperatura finita. Este procedimento permite a determinação da distribuição de núcleons e léptons no interior de uma estrela protoneutrônica, onde incluímos ainda a presença de neutrinos confinados. No estudo da matéria de quarks, consideramos o modelo de sacola do Massachussets Institute of Technology (MIT). Incorporando as equações TOV neste estudo, determinamos propriedades globais de estrelas de quarks, bem como a distribuição dos diferentes sabores de quarks no interior estelar. Como principal resultado, obtivemos uma equação de estado geral para a matéria hadrônica e de quarks, introduzida nas equações TOV, e analisamos a existência de estrelas híbridas. Os resultados obtidos nesta etapa do trabalho são totalmente coerentes com aqueles obtidos por outros autores.|http://hdl.handle.net/10183/3318
Validação do mecanismo de tolerância a falhas do SGBD InterBase através de injeção de falhas|2002|Open Access|Dissertação|Banco : Dados;Recuperacao : Erros;Tolerancia : Falhas;Injecao : Falhas;Deteccao : Erros|por|O presente trabalho explora a aplicação de técnicas de injeção de falhas, que simulam falhas transientes de hardware, para validar o mecanismo de detecção e de recuperação de erros, medir os tempos de indisponibilidade do banco de dados após a ocorrência de uma falha que tenha provocado um FUDVK. Adicionalmente, avalia e valida a ferramenta de injeção de falhas FIDe, utilizada nos experimentos, através de um conjunto significativo de testes de injeção de falhas no ambiente do SGBD. A plataforma experimental consiste de um computador Intel Pentium 550 MHz com 128 MB RAM, do sistema operacional Linux Conectiva kernel versão 2.2.13. O sistema alvo das injeções de falhas é o SGBD centralizado InterBase versão 4.0. As aplicações para a carga de trabalho foram escritas em VFULSWV SQL e executadas dentro de uma sessão chamada LVTO. Para a injeção de falhas foram utilizadas três técnicas distintas: 1) o comando NLOO do sistema operacional; 2) UHVHW geral no equipamento; 3) a ferramenta de injeção de falhas FIDe, desenvolvida no grupo de injeção de falhas do PPGC da UFRGS. Inicialmente são introduzidos e reforçados os conceitos básicos sobre o tema, que serão utilizados no decorrer do trabalho e são necessários para a compreensão deste estudo. Em seguida é apresentada a ferramenta de injeção de falhas Xception e são também analisados alguns experimentos que utilizam ferramentas de injeção de falhas em bancos de dados.  Concluída a revisão bibliográfica é apresentada a ferramenta de injeção de falhas – o FIDe, o modelo de falhas adotado, a forma de abordagem, a plataforma de hardware e software, a metodologia e as técnicas utilizadas, a forma de condução dos experimentos realizados e os resultados obtidos com cada uma das técnicas. No total foram realizados 3625 testes de injeções de falhas. Com a primeira técnica foram realizadas 350 execuções, com a segunda técnica foram realizadas 75 execuções e com a terceira técnica 3200 execuções, em 80 testes diferentes. O modelo de falhas proposto para este trabalho refere-se a falhas de crash baseadas em corrupção de memória e registradores, parada de CPU, aborto de transações ou reset geral. Os experimentos foram divididos em três técnicas distintas, visando a maior cobertura possível de erros, e apresentam resultados bastante diferenciados. Os experimentos com o comando NLOO praticamente não afetaram o ambiente do banco de dados. Pequeno número de injeção de falhas com o FIDe afetaram significativamente a dependabilidade do SGBD e os experimentos com a técnica de UHVHW geral foram os que mais comprometeram a dependabilidade do SGBD.|http://hdl.handle.net/10183/3371
Extensão do padrão ODMG para suportar tempo e versões|2002|Open Access|Dissertação|Banco : Dados orientados : Objetos;Versoes : Banco : Dados|por|Este trabalho apresenta uma extensão do padrão ODMG para o suporte ao versionamento de objetos e características temporais. Essa extensão, denominada TV_ODMG, é baseada no Modelo Temporal de Versões (TVM), que é um modelo de dados orientado a objetos desenvolvido para armazenar as versões do objeto e, para cada versão, o histórico dos valores dos atributos e dos relacionamentos dinâmicos. O TVM difere de outros modelos de dados temporais por apresentar duas diferentes ordens de tempo, ramificado para o objeto e linear para cada versão. O usuário pode também especificar, durante a modelagem, classes normais (sem tempo e versões), o que permite a integração desse modelo com outras modelagens existentes. Neste trabalho, os seguintes componentes da arquitetura do padrão ODMG foram estendidos: o Modelo de Objetos, a ODL (Object Definition Language) e a OQL (Object Query Language). Adicionalmente, foi desenvolvido um conjunto de regras para o mapeamento do TV_ODMG para o ODMG a fim de permitir o uso de qualquer ODBMS para suportar a extensão proposta.|http://hdl.handle.net/10183/3372
Uma abordagem Bottom-UP para a integração semântica de esquemas XML|2002|Open Access|Tese|Banco : Dados;XML (Linguagem de marcação);Dados heterogêneos;Integração : Esquemas|por|XML (eXtensibile Markup Language) é um padrão atual para representação e intercâmbio dos semi-estruturados na Web. Dados semi-estruturados são dados não convencionais cujas instâncias de uma mesma fonte de dados podem ter representações altamente heterogêneas. Em função isto, um esquema para estes dados tende a ser extenso para suportar todas as alternativas de representação que um dado pode assumir. Parte do grande volume de dados disponível hoje na Web é composto por fontes de dados heterogêneas XML sobre diversos domínios do conhecimento. Para realizar o acesso a estas fontes, aplicações na Web necessitam de um mecanismo de integração de dados. O objetivo principal deste mecanismo é disponibilizar um esquema de dados global representativo dos diversos esquemas XML das fontes de dados. Com base neste esquema global, consultas são formuladas, traduzidas para consultas sobre os esquemas XML, executadas nas fontes de dados e os resultados retornados à aplicação. Esta tese apresenta uma abordagem para a integração semântica de esquemas XML relativos a um domínio de aplicação chamada BInXS. BInXS adota um processo bottom-up de integração, no qual o esquema global é definido para um conjunto de esquemas XML representadas atrtavés de DTDs (Document Type Definitions). A vantagem do processo bottom-up é que todas as informações dos esquemas XML são consideradas no esquema global. Desta forma, toda a informação presente nas fontes de dados pode ser consultada.  O processo de integração de BInXS é baseado em um conjunto de regras e algoritmos que realizam a cnversão de cada DTD para um esquema canônico conceitual e a posterior integração semântica propriamente dita destes esquemas canônicos. O processo é semi-automático pois considera uma eventual intervenção de um usuário especialista no domínio para validar ou confirmar alternativas de resultado produzidas automaticamente. Comparada com trabalhos relacionados, BInXS apresenta as seguintes contribuições: (i) uma representação canônica conceitual para esquemas XML que é o resultado de uma anállise detalhada do modelo XML; (ii) um étodo de unificação que lida com as particularidades da integração de dados semi-estruturados e; (iii) uma estratégia de mapeamento baseada em expressões de consulta XPath que possibilita uma tradução simples de consultas globais para consultas a serem executadas nas fontes de dados XML.|http://hdl.handle.net/10183/3373
Desenvolvimento de um esquema XML para banco de dados sobre ovinos|2002|Open Access|Dissertação|Banco : Dados;XML (Linguagem de marcação);Informatica : Agropecuaria;Internet|por|Este trabalho utilizou tecnologias tais como XML (eXtensible Markup Language) e esquemas XML, com objetivo de aprimorar a ovinocultura tornando o setor primário mais competitivo. Foram elaborados arquivos XML com a mesma estrutura (equivalentes) dos arquivos primitivos da Associação Brasileira de Criadores de Ovinos ( A.R.C.O. ), para que os mesmos possam ser disponibilizados na Internet. Para obter a integridade destes dados na Internet criou-se os esquemas XML, que são arquivos contendo as regras de formação dos dados. Os arquivos XML ficarão protegidos contra dados indesejáveis e disponíveis ao produtor rural via Internet.|http://hdl.handle.net/10183/3374
Método para a avaliação de servidores WWW no ambiente corporativo|2002|Open Access|Dissertação|Engenharia : Software;Metricas : Software;World Wide Web (WWW);Internet;Sistemas operacionais|por|O principal objetivo deste trabalho é apresentar um método e métricas para a avaliação do serviço Internet mais amplamente utilizado: a World Wide Web. As características básicas e funcionamento do serviço, bem como algumas ferramentas de avaliação de desempenho, serão descritas. Estes capítulos servirão de base para os demais, onde serão apresentados o método para avaliação do serviço web e métricas usadas para análise de desempenho, disponibilidade, confiabilidade, facilidades de administração e recursos. Por fim, o método e métricas serão aplicados na Procempa – Companhia de Processamento de Dados do Município de Porto Alegre, onde será possível verificá-los na prática. Além disto, dados importantes sobre a infra-estrutura web da Procempa serão fornecidos, os quais permitem uma análise do ambiente web atual e futuro da empresa.|http://hdl.handle.net/10183/3375
Transições de fase em compostos de estrutura aberta sob altas pressões|2000|Open Access|Tese|Rubídio;Nióbio;Cesio;Altas pressões;Difração de raios X;Espectroscopia Raman;Espectroscopia no infravermelho;Transformações de fase;Zircônia;Silício;Carbono;Métodos computacionais;Tratamento térmico;Amorfizacao;Recristalizacao;Recozimento|por|Resumo não disponível.|http://hdl.handle.net/10183/3385
Uso do network simulator-NS para simulação de sistemas distribuídos em cenários com defeitos|2003|Open Access|Dissertação|Confiabilidade : Computadores;Sistemas distribuídos;Simulação;Tolerancia : Falhas|por|O desenvolvimento de protocolos distribuídos é uma tarefa complexa. Em sistemas tolerantes a falhas, a elaboração de mecanismos para detectar e mascarar defeitos representam grande parte do esforço de desenvolvimento. A técnica de simulação pode auxiliar significativamente nessa tarefa. Entretanto, existe uma carência de ferramentas de simulação para investigação de protocolos distribuídos em cenários com defeitos, particularmente com suporte a experimentos em configurações “típicas” da Internet. O objetivo deste trabalho é investigar o uso do simulador de redes NS (Network Simulator) como ambiente para simulação de sistemas distribuídos, particularmente em cenários sujeitos à ocorrência de defeitos. O NS é um simulador de redes multi-protocolos, que tem código aberto e pode ser estendido. Embora seja uma ferramenta destinada ao estudo de redes de computadores, o ajuste adequado de parâmetros e exploração de características permitiu utilizá-lo para simular defeitos em um sistema distribuído. Para isso, desenvolveu-se dois modelos de sistemas distribuídos que podem ser implementados no NS, dependendo do protocolo de transporte utilizado: um baseado em TCP e o outro baseado em UDP. Também, foram estudadas formas de modelar defeitos através do simulador. Para a simulação de defeito de colapso em um nodo, foi proposta a implementação de um método na classe de cada aplicação na qual se deseja simular defeitos.  Para ilustrar como os modelos de sistemas distribuídos e de defeitos propostos podem ser utilizados, foram implementados diversos algoritmos distribuídos em sistemas síncronos e assíncronos. Algoritmos de eleição e o protocolo Primário-Backup são exemplos dessas implementações. A partir desses algoritmos, principalmente do Primário-Backup, no qual a simulação de defeitos foi realizada, foi possível constatar que o NS pode ser uma ferramenta de grande auxílio no desenvolvimento de novas técnicas de Tolerância a Falhas. Portanto, o NS pode ser estendido possibilitando que, com a utilização dos modelos apresentados nesse trabalho, simule-se defeitos em um sistema distribuído.|http://hdl.handle.net/10183/3395
Adaptação dinâmica do timeout de detectores de defeitos através do uso de séries temporais|2003|Open Access|Tese|Confiabilidade : Computadores;Tolerancia : Falhas;Detecção : Falhas;Sistemas distribuídos;Séries temporais|por|Uma aplicação distribuída freqüentemente tem que ser especificada e implementada para executar sobre uma rede de longa distância (wide-área network-WAN), tipicamente a Internet. Neste ambiente, tais aplicações são sujeitas a defeitos do tipo colapso(falha geral num dado nó), teporização (flutuações na latência de comunicação) e omissão (perdas de mensagens). Para evitar que este defeitos gerem comseqüências indesejáveis e irreparáveis na aplicação, explora-se técnicas para tolerá-los. A abstração de detectores de defeitos não confiáveis auxilia a especificação e trato de algoritmos distribuídos utilizados em sistemas tolerantes a falhas, pois permite uma modelagem baseada na noção de estado (suspeito ou não suspeito) dos componentes (objetos, processo ou processadores) da aplicação. Para garantir terminação, os algoritmos de detecção de defeitos costumam utilizar a noção de limites de tempo de espera (timeout). Adicionalmente, para minimizar seu erro (falasas suspeitas) e não comprometer seu desempenho (tempo para detecção de um defeito), alguns detectores de defeitos ajustam dinamicamente o timeout com base em previsões do atraso de comunicação. Esta tese explora o ajuste dinâmico do timeout realizado de acordo com métodos de previsão baseados na teoria de séries temporais. Tais métodos supõem uma amostragem periódica e fornececm estimativas relativamente confiáveis do comportamento futuro da variável aleatória.  Neste trabalho é especificado uma interface para transformar uma amostragem aperiódica do atraso de ida e volta de uma mensagem (rtt) numa amostragem periódica, é analisado comportamento de séries reais do rtt e a precisão dee sete preditores distintos (três baseados em séries temporais e quatrro não), e é avaliado a influência destes preditores na qualidade de serviço de um detector de defeitos do estilopull. Uma arquitetura orientada a objetos que possibilita a escolha/troca de algoritmos de previsão e de margem de segurança é também proposta. Como resultado, esta tese mostra: (i) que embora a amostragem do rtt seja aperiódica, pode-se modelá-la como sendo uma série temporal (uma amostragem periódica) aplciando uma interface de transformação; (ii) que a série temporal rtt é não estacionária na maioria dos casos de teste, contradizendo a maioria das hipóteses comumente consideradas em detectores de defeitos; (iii) que dentre sete modelos de predição, o modelo ARIMA (autoregressive integrated moving-average model) é o que oferece a melhor precisão na predição de atrasos de comunicação, em termos do erro quadrático médio: (iv) que o impacto de preditores baseados em séries temporais na qualidade de serviço do detector de defeitos não significativo em relação a modelos bem mais simples, mas varia dependendo da margem de segurança adotada; e (v) que um serviço de detecção de defeitos pode possibilitar a fácil escolha de algoritmos de previsão e de margens de segurança, pois o preditor pode ser modelado como sendo um módulo dissociado do detector.|http://hdl.handle.net/10183/3402
Construção de um ambiente de desenvolvimento de software baseado em um sistema de gerência de workflow e outros produtos comerciais|2003|Open Access|Dissertação|Engenharia : Software;Ambiente : Software;Desenvolvimento : Software;Gerencia : Workflow;Sistemas : Workflow|por|Este trabalho apresenta uma arquitetura para Ambientes de Desenvolvimento de Software (ADS). Esta arquitetura é baseada em produtos comerciais de prateleira (COTS), principalmente em um Sistema de Gerência de Workflow – SGW (Microsoft Exchange 2000 Server – E2K) - e tem como plataforma de funcionamento a Internet, integrando também algumas ferramentas que fazem parte do grande conjunto de aplicativos que é utilizado no processo de desenvolvimento de software. O desenvolvimento de um protótipo (WOSDIE – WOrkflow-based Software Development Integrated Environment) baseado na arquitetura apresentada é descrito em detalhes, mostrando as etapas de construção, funções implementadas e dispositivos necessários para a integração de um SGW, ferramentas de desenvolvimento, banco de dados (WSS – Web Storage System) e outros, para a construção de um ADS. O processo de software aplicado no WOSDIE foi extraído do RUP (Rational Unified Process – Processo Unificado Rational). Este processo foi modelado na ferramenta Workflow Designer, que permite a modelagem dos processos de workflow dentro do E2K. A ativação de ferramentas a partir de um navegador Web e o armazenamento dos artefatos produzidos em um projeto de software também são abordados. O E2K faz o monitoramento dos eventos que ocorrem dentro do ambiente WOSDIE, definindo, a partir das condições modeladas no Workflow Designer, quais atividades devem ser iniciadas após o término de alguma atividade anterior e quem é o responsável pela execução destas novas atividades (assinalamento de atividades). A arquitetura proposta e o protótipo WOSDIE são avaliados segundo alguns critérios retirados de vários trabalhos. Estas avaliações mostram em mais detalhes as características da arquitetura proposta e proporcionam uma descrição das vantagens e problemas associados ao WOSDIE.|http://hdl.handle.net/10183/3404
Sub-ação para transformações unidimensionais|2003|Open Access|Tese|Transformações unidimensionais;Função Sub-Ação|por|Consideramos um potencial A α-Hölder e uma função ƒ: S1 ! S1, C2 e de grau 2 tal que a origem é um ponto crítico (ƒ´(0) = 0) e ƒ é uniformemente expansiva a menos de um intervalo [0, α+ε). Neste trabalho mostramos que, para um potencial genérico A, a medida invariante para ƒ que maximiza a ação dada por integral Adμ é única e unicamente ergódica no seu suporte. Estimamos também o comportamento assintótico de integrais que dependem de um parâmetro ξ ε R determinando cotas superiores para o limite lim sup 1/ξ log integral e»ª(x)d¹»(x); onde μξ é o estado de equilíbrio para o potencial ξA e as funções A e Ψ são α-Hölder.|http://hdl.handle.net/10183/3406
TVMSE : uma implementação do versionamento de esquemas segundo o modelo TVM|2003|Open Access|Dissertação|Banco : Dados orientados : Objetos;Banco : Dados temporais;Versoes : Banco : Dados;Esquema : Banco : Dados|por|Um esquema de banco de dados certamente sofrerá alguma alteração com o passar do tempo. Algumas das causas destas modificações são ocorrência de um aumento no domínio do sistema, erros ocorridos na fase de projeto, mudanças na realidade representada pelo sistema, ou a necessidade de melhoria no seu desempenho. O uso de bancos de dados temporais é uma alternativa para o armazenamento das informações da evolução, pois permite sua recuperação por meio do histórico de suas mudanças. O presente trabalho propõe um ambiente para implementar evolução de esquemas sobre um BDOO, utilizando o Modelo Temporal de Versões (TVM). Deste modo, características de versões e de tempo são utilizadas tanto no nível dos esquemas como nos dados armazenados. Estados são associados às versões de esquema para representar seus estágios de desenvolvimento durante a evolução. O gerenciamento das versões de esquema é realizado por intermédio de uma camada denominada meta-esquema. Em um outro nível, o gerenciamento das instâncias é realizado por meio de uma camada denominada metadados, inserida para cada versão de esquema definida. Por intermédio destes controles é possível analisar a evolução dos esquemas como um todo e, para cada esquema, as correspondentes versões de seus dados e sua evolução temporal. Algumas alternativas de consulta para um ambiente com estas características são analisadas. O trabalho apresenta, ainda, as características básicas de um protótipo implementado para verificar a viabilidade da proposta apresentada.|http://hdl.handle.net/10183/3407
Um processo auto-documentável de geração de ontologias de domínio para dados semi-estruturados|2002|Open Access|Dissertação|Armazenamento : Dados;Ontologias;Dados semi-estruturados;Integração : Esquemas|por|Dados são disponibilizados através dos mais distintos meios e com os mais variados níveis de estruturação. Em um nível baixo de estruturação tem-se arquivos binários e no outro extremo tem-se bancos de dados com uma estrutura extremamente rígida. Entre estes dois extremos estão os dados semi-estruturados que possuem variados graus de estruturação com os quais não estão rigidamente comprometidos. Na categoria dos dados semiestruturados tem-se exemplos como o HTML, o XML e o SGML. O uso de informações contidas nas mais diversas fontes de dados que por sua vez possuem os mais diversos níveis de estruturação só será efetivo se esta informação puder ser manejada de uma forma integrada e através de algum tipo de esquema. O objetivo desta dissertação é fornecer um processo para construção de uma ontologia de domínio que haja como esquema representativo de diferentes conjuntos de informação. Estes conjuntos de informações podem variar de dados semi-estruturados a dados estruturados e devem referir-se a um mesmo domínio do conhecimento. Esta proposta permite que qualquer modelo que possa ser transformado no modelo comum de integração possa ser utilizado com entrada para o processo de integração. A ontologia de domínio resultante do processo de integração é um modelo semântico que representa o consenso obtido através da integração de diversas fontes de forma ascendente (bottom-up), binária, incremental, semi-automática e auto-documentável.  Diz-se que o processo é ascendente porque integra o modelo que representa a fonte de interesse sobre a ontologia, é binário porque trabalha com dois esquemas a cada integração o que facilita o processo de documentação das integrações realizadas, é incremental porque cada novo esquema de interesse é integrado sobre a ontologia vigente naquele momento, é semiautomático porque considera a intervenção do usuário durante o processo e finalmente é autodocumentável porque durante o processo, toda integração de pares de conceitos semanticamente equivalentes é registrada. O fato de auto-documentar-se é a principal característica do processo proposto e seu principal diferencial com relação a outras propostas de integração. O processo de mapeamento utiliza, dos esquemas de entrada, toda a informação presente ou que possa ser inferida. Informações como se o conceito é léxico ou não, se é raiz e os símbolos que permitem deduzir cardinalidades são consideradas. No processo de integração são consideradas práticas consagradas de integração de esquemas de BDs, na identificação de relacionamentos entre objetos dos esquemas, para geração do esquema integrado e para resolução de conflitos. As principais contribuições desta dissertação são (i) a proposta de um metamodelo capaz de manter o resultado dos mapeamentos e das integrações realizadas e (ii) a especificação de um processo auto-documentável que de sustentação a auditoria do processo de integração.|http://hdl.handle.net/10183/3409
Hyper-Automaton: avaliação interativa de alunos em cursos na WEB baseado em autômatos finitos|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Avaliacao pedagogica;Automatos finitos|por|O objetivo desta dissertação é a elaboração de uma técnica da aplicação do formalismo de Autômatos Finitos com Saída (Máquina de Mealy e Máquina de Moore) como um modelo estrutural para a organização de hiperdocumentos instrucionais, em destacar especial, Avaliação e Exercício. Esse objetivo é motivado pela organização e agilização do processo de avaliação proporcionado ao professor e ao aluno. Existem diferentes técnicas de ensino utilizadas na Internet, algumas dessas continuam sendo projetadas com o uso de metodologias tradicionais de desenvolvimento, outras têm a capacidade de modelar de forma integrada e consistente alguns aspectos necessários para uma aplicação WEB. Para alcançar o objetivo proposto, foram realizadas pesquisas nas várias áreas abrangidas pelo tema em evidência, tanto relativo ao processo tradicional (aplicação de prova utilizando metodologia tradicional), como o desenvolvimento de software mediado por computador e uso da Internet em si. A modelagem de desenvolvimento para Internet deve integrar características de técnicas de projeto de sistemas de hipermídia devido à natureza hipertextual da Internet. O uso de hiperdocumento como autômatos com saída está na forma básica de representação de hipertexto, em que cada fragmento de informação é associado a um nodo ou a um link (estado/transições) do grafo. Sendo assim, os arcos direcionados representam relacionamentos entre os nodos ou links, ou seja, uma passagem do nodo origem para o nodo destino.  As n-uplas dos autômatos apresentam uma correspondência as estruturas de hiperdocumentos na WEB, seu estado/transição inicial corresponde a sua primeira página e suas transições definidas na função programa, funcionam como ligações lógicas, quando selecionadas durante a navegação do hipertexto. Entretanto, faz-se necessário um levantamento dos modelos de hipertextos e das ferramentas de implementação disponíveis para a Internet, a fim de que seja capaz de suportar as peculiaridades do ambiente. Tudo isso deve ser integrado preferencialmente em um paradigma de desenvolvimento amplamente aceito, para que os projetistas não tenham muitas dificuldades em assimilar os conceitos propostos. A proposta apresentada nesta dissertação, batizada de Hyper-Automaton (hipertexto e autômato), consiste na integração de um Curso na WEB, utilizando formalismo de Autômatos Finitos com Saída para a modelagem dos conceitos necessários e definição das fases adequadas para completar a especificação de Sistema Exercício e Avaliação, bem como a especificação da Geração Automática dos Exercícios e Avaliações baseadas em autômatos para a WEB.  Os modelos criados abrangem conceitos de Máquina de Mealy, Máquina de Moore e Aplicações de Hiperdocumentos e Ferramentas de Programação para Internet, os mesmos já testados em caso real. Os parâmetros apurados, nos testes, serviram a uma seqüência de etapas importantes para modelar e complementar a especificação do sistema projetado. Com os parâmetros e etapas de modelagem, a metodologia Hyper-Automaton consegue integrar, de forma consistente, as vantagens de várias técnicas específicas de modelagem de documentos e sistemas de hipermídia. Essas vantagens, aliadas ao suporte às ferramentas de desenvolvimento para Internet, garantem que a metodologia fique adequada para a modelagem de Sistemas com aplicação de métodos de autômatos para exercícios e avaliação na WEB.|http://hdl.handle.net/10183/3417
Uma Arquitetura de Hardware para estimação de movimento aplicada à compressão de vídeo digital|2003|Open Access|Dissertação|Televisão;Vídeo digital;Compressao : Video;Sistemas digitais|por|A tarefa de estimação de movimento, utilizada na compressão de vídeo digital, é normalmente realizada em hardware por processador dedicado, uma vez que demanda expressiva capacidade computacional. Este trabalho propõe e desenvolve uma arquitetura de hardware para realizar o cálculo dos vetores de movimento no contexto de compressão de vídeo digital. Essa arquitetura para estimação de movimento é composta pelos blocos: interface de entrada e saída (E/S), matriz de processamento com 64 elementos de processamento, unidade de comparação e unidade de controle. A arquitetura foi descrita em linguagem VHDL de maneira que o número de bits utilizados para representação da luminância dos pontos é configurável. A partir desta descrição, foi gerado um protótipo para dados representados em 4 bits utilizando um kit de desenvolvimento baseado no dispositivo FPGA XC2S150 da Xilinx. Para validação do algoritmo e da arquitetura implementada, além da simulação, foi desenvolvido um software para plataforma PC capaz de exercitar as funcionalidades do protótipo. O PC é utilizado como dispositivo controlador de E/S para esta validação, na qual uma implementação do algoritmo em software e outra em linguagem de descrição de hardware são comparadas.  A máxima freqüência de trabalho do protótipo, estimada por simulação da arquitetura mapeada no FPGA XC2S150, é de 33 MHz. A esta freqüência o núcleo da arquitetura paralela de 64 elementos de processamento realiza cerca de 2,1 GOps (bilhões de operações inteiras por segundo). Esta arquitetura de hardware calcula os vetores de movimento para vídeo no formato 640x480 pontos à taxa de 107,32 quadros por segundo, ou um quadro a cada 9,3 ms. A arquitetura implementada para luminânica em 4 bits ocupa 16 pinos de E/S, 71,1% dos blocos lógicos do FPGA e 83,3% dos blocos de memória disponíveis no dispositivo XC2S150.|http://hdl.handle.net/10183/3418
Análise da Máquina de Turing Persistente com múltiplas fitas de trabalho|2003|Open Access|Dissertação|Máquinas : Turing : Persistente : Paralela;Teoria : Ciência : Computação;Maquinas : Turing|por|Nos últimos 70 anos têm sido apresentadas várias propostas para caracteriza ção da noção intuitiva de computabilidade. O modelo de Computação mais conhecido para expressar a noção intuitiva de algoritmo é a Máquina de Turing. Esse trabalho apresenta máquinas abstratas que representam diferentes formas de comportamento computacional, sendo possível abordar a diversidade entre a Teoria da Computação Clássica (Máquina de Turing) e a Teoria da Computa- ção Interativa (Máquina de Turing Persistente). Com a evolução dos sistemas de computação, surgiu a necessidade de estender a de nição de Máquina de Turing para tratar uma diversidade de novas situações, esses problemas conduziram a uma mudança de paradigma. Neste contexto foi desenvolvido a Máquina de Turing Persistente, que é capaz de fundamentar a Teoria da Computação Interativa. Máquinas de Turing Persistentes (PeTM) são modelos que expressam comportamento interativo, esse modelo é uma extensão da Máquina de Turing. O presente trabalho tem como objetivo explorar paralelismo na Máquina de Turing Persistente, através da formalização de uma extensão paralela da PeTM e o estudo dos efeitos sobre essa extensão, variando o número de tas de trabalho. Contribui- ções desse trabalho incluem a de nição de uma máquina de Turing Persistente Paralela para modelar computação interativa e uma exposição de conceitos fundamentais e necessários para o entendimento desse novo paradigma. Os métodos e conceitos apresentados para formalização da computação na Máquina de Turing Persistente Paralela desenvolvidos nessa dissertação, podem servir como base para uma melhor compreensão da Teoria da Computação Interativa e da forma como o paralelismo pode ser especi cado em modelos teóricos.|http://hdl.handle.net/10183/3419
ComFIRM - Injeção de falhas de comunicação através da alteração de recursos do sistema operacional|2000|Open Access|Dissertação|Tolerancia : Falhas;Injecao : Falhas;Linux;Sistemas operacionais|por|Este trabalho trata da técnica de validação experimental de protocolos de comunicação confiável, através da injeção de falhas de comunicação. São estudadas inicialmente as técnicas de injeção de falhas, por hardware, software e simulação, e então são aprofundados os conceitos de injeção de falhas de comunicação, modelos de falha e especificação de experimentos de injeção de falhas. Em um segundo momento, são estudadas as formas de implementação de injetores de falhas em software, em suas duas formas mais comuns: no nível da aplicação e no nível do sistema operacional. São comentados os impactos da implementação de injetores no código da aplicação, por processos concorrentes à aplicação, em código utilizado pela aplicação e no meta-nível. Por fim, são estudados também que influências sofre a implementação de um injetor de falhas em um sistema operacional, e mais especificamente a de injetores de falhas de comunicação. O objetivo específico deste trabalho é implementar um injetor de falhas de comunicação bastante abrangente e flexível, situado dentro do núcleo do Sistema Operacional Linux. Para viabilizar esta implementação foi estudada também a arquitetura do Sistema Operacional Linux, sua decomposição em subsistemas e a interação entre estes.  Foram estudadas também as várias técnicas de programação e mecanismos que o Sistema Operacional Linux fornece aos seus subsistemas. Estando completas a revisão bibliográfica a respeito de injeção de falhas e o estudo do código do Sistema Operacional Linux, são apresentadas a proposta e a implementação da ferramenta ComFIRM—Communication Fault Injection through Operating System Resource Modification, suas características e sua inserção dentro do núcleo do Sistema Operacional Linux. Finalizando este trabalho, são apresentados uma pequena série de testes de funcionamento e experimentos realizados com a ferramenta ComFIRM, visando demonstrar a correção de seu funcionamento, o cumprimento de seus objetivos e também sua praticidade e flexibilidade de uso. São apresentadas as conclusões deste trabalho, propostas de melhorias à ferramenta apresentada, bem como possibilidades de trabalhos futuros.|http://hdl.handle.net/10183/3420
O problema de Dirichlet para a equação das superfícies mínimas em domínios não necessariamente convexos|2003|Open Access|Tese|Problema de Dirichlet;Equação das superfícies mínimas|por|Estudamos o problema de Dirichlet para a equação das superfícies mínimas em domínios limitados do plano. Provamos a existência e unicidade de gráficos mínimos sobre domínios limitados e não necessariamente convexos, com valores no bordo satisfazendo uma condição que denominamos condição da declividade limitada generalizada a qual, usando cilindros no lugar de planos, generaliza a condição clássica da declividade limitada. Com este resultado, dado um domínio limitado e suave qualquer do plano, conseguimos obter cotas explícitas para a norma C2 de dados no bordo deste domínio que garantem a existência de solução ao correspondente problema de Dirichlet.|http://hdl.handle.net/10183/3422
Técnicas de detecção de Sniffers;Sniffer detection techniques |2003|Open Access|Dissertação|Security;Intrusion detection;Sniffer;Sniffer detection;Detecção : Intrusão;Seguranca : Computadores|por|A área de Detecção de Intrusão, apesar de muito pesquisada, não responde a alguns problemas reais como níveis de ataques, dim ensão e complexidade de redes, tolerância a falhas, autenticação e privacidade, interoperabilidade e padronização. Uma pesquisa no Instituto de Informática da UFRGS, mais especificamente no Grupo de Segurança (GSEG), visa desenvolver um Sistema de Detecção de Intrusão Distribuído e com características de tolerância a falhas. Este projeto, denominado Asgaard, é a idealização de um sistema cujo objetivo não se restringe apenas a ser mais uma ferramenta de Detecção de Intrusão, mas uma plataforma que possibilite agregar novos módulos e técnicas, sendo um avanço em relação a outros Sistemas de Detecção atualmente em desenvolvimento. Um tópico ainda não abordado neste projeto seria a detecção de sniffers na rede, vindo a ser uma forma de prevenir que um ataque prossiga em outras estações ou redes interconectadas, desde que um intruso normalmente instala um sniffer após um ataque bem sucedido. Este trabalho discute as técnicas de detecção de sniffers, seus cenários, bem como avalia o uso destas técnicas em uma rede local. As técnicas conhecidas são testadas em um ambiente com diferentes sistemas operacionais, como linux e windows, mapeando os resultados sobre a eficiência das mesmas em condições diversas.;The area of Intrusion Detection, although widely searched, does not answer to some real problems as the level of attacks, dimension and complexity of networks, fault tolerance, authentication and privacy, interoperability and standardization. A current research at the Institute of Computer Science of UFRGS, more specifically in the Security Group (GSEG), aims at developing a Distributed Intrusion Detection System with features of fault tolerance. This project, called Asgaard, is the accomplishment of a system whose objective is not only restricted to be another tool concerning Intrusion Detection, but also a platform that makes possible to add new modules and techniques, which is an advance with respect to other Intrusion Detection Systems in progress. A point which has not yet been investigated in this project would be the network sniffer detection, which is supposed to be a way to prevent that an attack proceeds to other hosts and interconnected networks, once a intruder usually installs a sniffer after a well-performed attack. This work explores the sniffers detection techniques, their sets, as well as verifies these techniques in a local area network. The known techniques are tested in an environment with different operating systems, as linux and windows, explaining the results on the efficiency of these systems in several conditions.|http://hdl.handle.net/10183/3423
XML Integrator: interoperabilidade de Fontes de Dados Heterogêneas, baseada no Mapeamento de Esquemas Conceituais|2003|Open Access|Dissertação|Banco : Dados heterogeneos;XML (Linguagem de marcação)|por|O acesso integrado a informações provenientes de banco de dados autônomos e heterogêneos, localizadas em diferentes ambientes de hardware e software, vem sendo amplamente pesquisado pela comunidade de banco de dados, com diversas soluções propostas. A maioria delas baseia-se na comparação e na integração ou mapeamento dos esquemas conceituais dos bancos de dados participantes, implementados através de uma camada adicional de software, em um nível superior ao dos bancos de dados existentes. Inicialmente, as metodologias de acesso integrado eram limitadas às informações provenientes de banco de dados. Entretanto, com o crescimento das redes de computadores e, conseqüentemente, com a intensa utilização da Internet, novas fontes de informações passaram a ser utilizadas neste ambiente, tais como fontes de dados semi-estruturadas. Estender o acesso integrado também a esses tipos de informações tornou-se importante.  Este trabalho tem como objetivo propor a utilização de um metamodelo XML como modelo de dados canônico, através do qual é possível obter a representação conceitual dos esquemas de exportação provenientes de bancos de dados relacionais, objeto-relacionais e documentos XML, permitindo, desta forma, o acesso integrado a fontes de dados estruturadas e semi-estruturadas, a partir de metodologias inicialmente voltadas à interoperabilidade de banco de dados heterogêneos. Além do metamodelo apresentado, este trabalho incluiu o desenvolvimento da ferramenta XML Integrator, cujo objetivo é fornecer ao usuário mecanismos de apoio ao processo conversão dos esquemas conceituais locais de fontes de dados heterogêneas para o Metamodelo XML, bem como de extração de um esquema conceitual correspondente a um documento XML ou a uma classe de documentos XML. Para isso, a ferramenta utiliza interfaces gráficas, que guiam o usuário através dos diversos passos, desde a seleção da fonte de dados a ser convertida, até a geração do esquema de exportação propriamente dito.|http://hdl.handle.net/10183/3425
Data Mining no Varejo: estudo de caso para loja de materiais de construção|2003|Open Access|Dissertação|Armazenamento : Dados;Mineração de dados;Banco : Dados|por|Este trabalho apresenta um estudo de caso de mineração de dados no varejo. O negócio em questão é a comercialização de móveis e materiais de construção. A mineração foi realizada sobre informações geradas das transações de vendas por um período de 8 meses. Informações cadastrais de clientes também foram usadas e cruzadas com informações de venda, visando obter resultados que possam ser convertidos em ações que, por conseqüência, gerem lucro para a empresa. Toda a modelagem, preparação e transformação dos dados, foi feita visando facilitar a aplicação das técnicas de mineração que as ferramentas de mineração de dados proporcionam para a descoberta de conhecimento. O processo foi detalhado para uma melhor compreensão dos resultados obtidos. A metodologia CRISP usada no trabalho também é discutida, levando-se em conta as dificuldades e facilidades que se apresentaram durante as fases do processo de obtenção dos resultados. Também são analisados os pontos positivos e negativos das ferramentas de mineração utilizadas, o IBM Intelligent Miner e o WEKA - Waikato Environment for Knowledge Analysis, bem como de todos os outros softwares necessários para a realização do trabalho. Ao final, os resultados obtidos são apresentados e discutidos, sendo também apresentada a opinião dos proprietários da empresa sobre tais resultados e qual valor cada um deles poderá agregar ao negócio.|http://hdl.handle.net/10183/3430
Estimativa de capacitâncias e consumo de potência em circuitos combinacionais CMOS no nível lógico|2001|Open Access|Tese|Microeletrônica;Consumo : Potencia;Portas logicas|por|Esta tese propõe o desenvolvimento de um método de estimativa de capacitâncias e de potência consumida nos circuitos combinacionais CMOS, no nível de portas lógicas. O objetivo do método é fazer uma previsão do consumo de potência do circuito na fase de projeto lógico, o que permitirá a aplicação de técnicas de redução de potência ou até alteração do projeto antes da geração do seu leiaute. A potência dinâmica consumida por circuitos CMOS depende dos seguintes parâmetros: tensão de alimentação, freqüência de operação, capacitâncias parasitas e atividades de comutação em cada nodo do circuito. A análise desenvolvida na Tese, propõe que a potência seja dividida em duas componentes. A primeira componente está relacionada ao consumo de potência devido às capacitâncias intrínsecas dos transistores, que por sua vez estão relacionadas às dimensões dos transistores. Estas capacitâncias intrínsecas são concentradas nos nodos externos das portas e manifestam-se em função das combinações dos vetores de entrada. A segunda componente está relacionada às interconexões entre as células do circuito. Para esta etapa utiliza-se a estimativa do comprimento médio das interconexões e as dimensões tecnológicas para estimar o consumo de potência. Este comprimento médio é estimado em função do número de transistores e fanout das várias redes do circuito.  Na análise que trata das capacitâncias intrínsecas dos transistores os erros encontrados na estimativa da potência dissipada estão no máximo em torno de 11% quando comparados ao SPICE. Já na estimativa das interconexões a comparação feita entre capacitâncias de interconexões estimadas no nível lógico e capacitâncias de interconexões extraídas do leiaute apresentou erros menores que 10%.|http://hdl.handle.net/10183/3431
A instabilidade causada pela migração dependente da densidade em metapopulações|2003|Open Access|Dissertação|Caos;Migração;Estabilidade;Metapopulação|por|Neste trabalho analisamos os efeitos causados pela migração dependente da densidade em metapopulações, modelada como um sisitema de n sítios discretos no tempo e no espaço. A análise em diferentes funções que descrervem a dinâmica local do sistema e para configurações da rede na forma unidimensional (anéis cíclicos) e na forma bidimensional (superfície toroidal). Para os anéis cíclicos, obtemos os padrões espaciais causados pela migração dependente da densidade. Além disso, observamos que padrões mais irregulares e complexos aparecem de forma mais intensa em uma das funçoes analisadas na descrição do processo de dinâmica local. Através de várias evidências numéricas determinamos, para dinâmica local, descrita pela função exponencial logística, a região onde a migração dependente da densidade induz caóticos no sistema. esta região é crescente conforme ocorre o crescimento na fração migratória máxima. Para redes bidimentsionais na forma vizinhança de Moore apresentamos as instabilidades causadas pela migração dependente da densidade nas mesmas funções utilizadas para deescrever o processo de dinâmica local do caso anterior. Através do cálculo do espectro de Lyapunov confirmamos os padrões caóticos encontrados, classificando-os como caos espaço temporal completamente desenvolvido e supressão de caos.|http://hdl.handle.net/10183/3440
Polimerização eletroquímica do furfural em meio aquoso de ftalato ácido de potássio sobre platina e carbono vítreo reticulado|2003|Open Access|Tese|Furfural : Polimerização eletroquímica;Eletrodo de platina;Carbono vítreo reticulado|por|O presente trabalho apresenta um estudo sistemático para a obtenção de um filme polimérico a partir da eletrooxidação do furfural (2-furanoaldeído). O filme foi crescido sobre a superfície do eletrodo de platina (Pt) e sobre carbono vítreo reticulado (CVR). Três técnicas eletroquímicas foram usadas: cronopotenciometria com correntes de 10 mA, voltametria cíclica por ciclagens sucessivas no intervalo de potencial de 2,0 V à 2,70 V (Ag/AgCl) e a cronoamperometria, no potencial de 2,65 V (Ag/AgCl). Diferentes eletrólitos foram testados em solução aquosa sobre Pt. O sal biftalato de potássio foi o eletrólito suporte mais adequado para formação do filme sobre ambos eletrodos, Pt e CVR. Os resultados obtidos confirmam a formação de um filme branco sobre a superfície dos eletrodos, entretanto, com alguma solubilização no próprio meio. Esta solubilidade do filme em meio aquoso permitiu atribuir-lhe características de polieletrólito. Evidências desta característica se confirmam pelas propriedades físico-químicas das soluções do filme testadas resultando no aumento da acidez e no aumento da condutividade do meio, quando se comparam as soluções de biftalato ácido de potássio com as do filme polimérico  Os resultados revelam a formação de um filme poroso e espesso sobre a superfície dos eletrodos, com características que dependem do método eletroquímico empregado, bem como do tempo de polarização. A visualização do filme foi registrada por fotografias digitais e caracterizada por microscopia eletrônica de varredura. O crescimento do filme pelo método cronopotenciométrico forneceu os melhores resultados em termos de aderência e volume. Uma observação importante refere-se ao caráter condutor do filme formado, uma vez que medidas eletroquímicas dos eletrodos modificados não acusaram um decaimento significativo das correntes. Além das medidas eletroquímicas, a condutividade do polímero, determinada pelo método das quatro pontas, resultou num valor de 100 µS cm-1 para o obtido potenciostaticamente e de 150 µS cm-1 para o obtido galvanostaticamente. A caracterização do filme envolveu as medidas térmicas de calorimetria diferencial de varredura (DSC) e a análise termogravimétrica (TGA). As medidas espectroscópicas como o ultravioleta, infravermelho, Raman, ressonância magnética nuclear de H1 e de C13 diretamente com o filme formado ou através de suas soluções em solventes adequados, confirmaram a participação de ambos os anéis ftálico e furânico na estrutura do filme.|http://hdl.handle.net/10183/3450
Síntese, caracterização e propriedades do polímero cloreto de 3-n-propil-1-azônia-4-azabiciclo[2.2.2]octano silsesquioxano|2003|Open Access|Dissertação|Sol-gel;Polímeros : Síntese|por|Neste trabalho encontra-se descrita a síntese do polímero híbrido organo inorgânico, cloreto de 3-n-propil-1-azônia-4-azabiciclo [2.2.2] octano silsesquioxano (dabcosilsesquioxano), através do método sol-gel. O polímero foi obtido por reação entre o precursor orgânico cloreto de 3-n-propiltrimetoxisilano-1-azônia-4-azabiciclo[2.2.2]octano (dabcosil), obtido em nosso laboratório, e o precursor inorgânico tetraetilortosilicato (TEOS) usando relações molares TEOS/dabcosil de 0 até 49, em meio ácido (pH entre 3 e 4). A caracterização destes materiais, realizada por espectroscopia no infravermelho e por análise termogravimétrica, comprovou que são materiais híbridos e que eles apresentam uma estabilidade térmica até 300ºC. Os polímeros com alto grau de conteúdo orgânico mostraram-se solúveis em água e foram facilmente depositados sobre matrizes de sílica, alumina e sílica modificada com um filme de óxido de alumínio (Al/SiO2). Estudos da lixiviação dos polímeros em água, nestas matrizes, mostraram que os mesmos tem melhor aderência na matriz de Al/SiO2. O polímero dabcosilsesquioxano com relação molar TEOS/dabcosil 0,33 impregnado na matriz Al/SiO2 foi usado como adsorvente para CuCl2, ZnCl2 e CdCl2 em solução etanólica, sendo que a capacidade de adsorção dos cloretos metálicos seguiu a ordem CdCl2 > ZnCl2 > CuCl2. A seletividade para a adsorção dos íons metálicos, em situação competitiva, também foi estudada. Os melhores resultados foram encontrados para o cádmio.  Os materiais híbridos com menor grau de incorporação orgânica, mostraram-se insolúveis em água. A morfologia destes polímeros foi estudada através da microscopia eletrônica de varredura e de isotermas de adsorção e dessorção de nitrogênio, sendo então obtidas a área superficial específica, a distribuição de tamanho de poros e uma estimativa do tamanho médio de partículas. Um estudo exploratório da potencialidade destes materiais como adsorventes de cátions cádmio em solução aquosa mostrou resultados muito positivos.|http://hdl.handle.net/10183/3451
Defeitos pontuais nos compostos intermetálicos ZrNi e Zr/sub 2/Ni estudados por dinâmica molecular|2002|Open Access|Tese|Dinâmica molecular;Defeitos puntuais;Estequiometria;Energia;Vacancias cristal;Zircônio;Níquel;Intersticiais;Análise numérica;Defeitos anti-sítio;Propriedades fisicas dos materiais;Energia nuclear|por|Empregamos a técnica de Dinâmica Molecular para estudar propriedades de defeitos pontuais nos compostos intermetálicos ZrNi e Zr2Ni. Descrevemos as configurações estáveis de defeitos e mecanismos de migração, assim como as energias envolvidas. Os potenciais interatômicos foram derivados do Embedded Atom Model. No intuito de levar em conta a variação de estequiometria causada pela presença de alguns tipos de defeitos em intermetálicos, apresentamos um método numérico que fornece a energia efetiva de formação de defeitos e aplicamos o método ao ZrNi e Zr2Ni. Os resultados mostraram que vacâncias são mais estáveis na sub—rede do Ni, com energia de formação de 0,83 e-0,61 eV em ZrNi e Zr2Ni, respectivamente. Vacâncias de Zr são instáveis em ambos compostos; elas decaem espontaneamente em pares anti—sítio e vacância de Ni. Configurações e energias de formação de intersticiais também foram calculadas e mostraram comportamentos similares. Em ZrNi, a migração de vacâncias ocorre preferencialmente nas direções [025] e [100], com as respectivas energias de migração 0,67 e 0,73 eV, e é um processo essencialmente bidimensional no plano (001). Em Zr,Ni, a migração de vacâncias é unidimensional, ocorrendo na direção [001], com energia de migração de 0,67 eV. Em ambos compostos a presença de defeitos de anti—sítio de Ni diminui a energia de migração da vacância de Ni em até 3 vezes e facilita a movimentação em três dimensões. Mecanismos de anel não são energeticamente eficientes em comparação com saltos diretos. As configurações estáveis de intersticiais em ambos compostos consistem em um átomo de Ni sobre o plano (001) entre dois vizinhos de Zr fora do plano. Intersticiais de Zr são instáveis e tendem a deslocar um átomo de Ni, ocupando seu sítio. Energias de deslocamento foram estudadas através de simulações de irradiação de ambos compostos.  Durante o processo de colisão binária, um potencial universal ZBL foi usado para colisões a curta distância. Para distâncias intermediárias usamos um potencial de união arbitrário. Zr mostrou—se mais difícil de ser arrancado de seu sítio do que Ni. Encontramos valores de energia de deslocamento no intervalo de aproximadamente 29 eV até 546 eV. Alguns resultados experimentais são mostrados e apresentam boa concordância com os cálculos.;We have employed the Molecular Dynamics approach to study the properties of point defects in the ZrNi and Zr2Ni intermetallic compounds. We describe the defects stable configuration and migration mechanisms, as well as the energetics involved. The interatomic potentials were derived from the Embedded Atom Model. In order to take into account the change of stoichiometry caused by the presente of some types of defects in intermetallics, we present a numerical method which retums the effective defect formation energy and apply the method to ZrNi and Zr2Ni. The results showed that vacancies are most stable in the Ni sublattice, with formation energy of 0.83 and 0.61 eV in ZrNi and Zr2Ni, respectively. Zr vacancies are unstable in both compounds; they spontaneously decay to pairs of Ni vacancy and antisite defect. The interstitial configurations and formation energies were also calculated, with similar behaviors. In ZrNi, vacancy migration occurs preferentially in the [025] and [100] directions, with migration energy of 0.67 and 0.73 eV, respectively, and is essentially a two—dimensional process, in the (001) plane. In Zr2Ni, vacancy migration is one—dimensional, occurring in the [001] direction, with a migration energy of 0.67 eV. In both compounds, the presence of Ni antisite defects decreases the Ni vacancy migration energy by up to a factor of three, and facilitates three—dimensional motion. Ring mechanisms are not energetically efficient compared to direct vacancy jumps. The stable interstitial configurations for both compounds consist of a Ni atom lying on the (001) plane between two out—of—plane nearest—neighbor Zr atoms. Displacement threshold energies were calculated through irradiation simulations of both compounds.  During the collision process, a universal ZBL potential was used for the Glose encounter cases. For intermediate distances we used a bridging arbitrary potential. Zr showed to be harder to displace than Ni. We found displacement threshold energies ranging from roughly 29 eV up to 546 eV. Some experimental results are given and show good agreement with the calculations.|http://hdl.handle.net/10183/3453
Estudo de propriedades magnéticas de filmes finos de cobalto sobre Si(111)|2003|Open Access|Dissertação|Propriedades magnéticas;Cobalto;Silício;Filmes finos;Anisotropia magnética;Microscopia de força atômica;Difração de raios X;Microscopia de tunelamento;Efeito kerr otico;Magnetização|por|O crescimento de lmes nos ferromagnéticos sobre uma superfícies vicinal induz uma anisotropia uniaxial que atua juntamente com a anisotropia magnetocrislanina. Neste estudo, lmes nos de Co foram depositados sobre Si(111) para investigar o papel dessa anisotropia nas propriedades magnéticas do lme. Os substratos foram preparados quimicamente via uma solução de NH4F e caracterizados via microscopia de força atômica. Os lmes, depositados via desbaste iônico, foram caracterizados estruturalmente via difratometria de raio-x e microscopia de tunelamento. As propriedades magnéticas foram determinadas via magnetometria a efeito Kerr magnetoóptico, onde observou-se a presen ça de uma anisotropia uniaxial dominante. Um modelo fenomelógico de reversão da magnetização via rotação coerente foi aplicado para ajustar as curvas de histerese, e as constantes de anisotropia uniaxial para cada espessura foram determinadas.|http://hdl.handle.net/10183/3464
Resolução de sistemas de equações lineares através de métodos de decomposição de domínio|2004|Open Access|Dissertação|Equacoes lineares;Análise numérica|por|A paralelização de métodos de resolução de sistemas de equações lineares e não lineares é uma atividade que tem concentrado várias pesquisas nos últimos anos. Isto porque, os sistemas de equações estão presentes em diversos problemas da computação cientí ca, especialmente naqueles que empregam equações diferenciais parciais (EDPs) que modelam fenômenos físicos, e que precisam ser discretizadas para serem tratadas computacionalmente. O processo de discretização resulta em sistemas de equações que necessitam ser resolvidos a cada passo de tempo. Em geral, esses sistemas têm como características a esparsidade e um grande número de incógnitas. Devido ao porte desses sistemas é necessária uma grande quantidade de memória e velocidade de processamento, sendo adequado o uso de computação de alto desempenho na obtenção da solução dos mesmos. Dentro desse contexto, é feito neste trabalho um estudo sobre o uso de métodos de decomposição de domínio na resolução de sistemas de equações em paralelo. Esses métodos baseiam-se no particionamento do domínio computacional em subdomínios, de modo que a solução global do problema é obtida pela combinação apropriada das soluções de cada subdomínio. Uma vez que diferentes subdomínios podem ser tratados independentemente, tais métodos são atrativos para ambientes paralelos.  Mais especi camente, foram implementados e analisados neste trabalho, três diferentes métodos de decomposição de domínio. Dois desses com sobreposição entre os subdomínios, e um sem sobreposição. Dentre os métodos com sobreposição foram estudados os métodos aditivo de Schwarz e multiplicativo de Schwarz. Já dentre os métodos sem sobreposição optou-se pelo método do complemento de Schur. Todas as implementações foram desenvolvidas para serem executadas em clusters de PCs multiprocessados e estão incorporadas ao modelo HIDRA, que é um modelo computacional paralelo multifísica desenvolvido no Grupo de Matemática da Computação e Processamento de Alto Desempenho (GMCPAD) para a simulação do escoamento e do transporte de substâncias em corpos de águas.|http://hdl.handle.net/10183/3466
Filtragem de informações no ambiente do Direito|2002|Open Access|Dissertação|Recuperacao : Informacao;Serviço : Filtragem|por|Os Sistemas de Recuperação de Informações (SRI) computadorizados são sistemas capazes de armazenar, recuperar e manter informações, visando minimizar o esforço humano na realização de tais atividades. A classificação de textos é um subdomínio dos sistemas de recuperação de informações que tem como objetivo classificar um texto em uma ou mais categorias existentes. Pode ser utilizada na classificação de mensagens, notícias e documentos, na filtragem de informações, na sumarização de textos, além de auxiliar profissionais na execução destas tarefas. A filtragem automatizada das mensagens de correio eletrônico é uma forma de organizar o trabalho do usuário. O volume de informações divulgadas através deste serviço torna fundamental um sistema de filtros para melhor uso do serviço. Sieve é uma proposta para padrão de linguagens de filtro de mensagens. O Direto é um software de correio, agenda e catálogo corporativos que visa atender todo Governo do Estado do Rio Grande do Sul. Foi desenvolvido na PROCERGS, Companhia de Processamento de Dados do Estado do Rio Grande do Sul, utilizando a linguagem Java e utiliza os serviços de IMAP, SMTP, LDAP e SGBD. Está disponível com licença de software livre.  O objetivo deste trabalho é aplicar técnicas de filtragem no Direto. O trabalho apresenta uma solução para filtrar as mensagens de correio do Direto utilizando Sieve. Também é especificado um serviço de canais de informação que visa divulgar informações de forma eficiente no Estado. Este serviço possui vários canais, cada um destinado a divulgar informações de determinado domínio. O usuário assina os canais que desejar e pode criar filtros para melhor refinamento das informações que deseja receber. Os filtros utilizam técnicas de classificação de textos no processo de filtragem.|http://hdl.handle.net/10183/3467
XHA: eXtensible Hyper-Automation|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância|por|O objetivo geral desta dissertação é estudar as possibilidades de flexibilização da função de saída do Sistema Hyper-Automaton além das rígidas possibilidades utilizadas atualmente com a utilização direta do HTML, objetivando eliminar as limitações como execução de aplicações proprietárias, caracteres incompatíveis entre browsers, excesso de tráfego na rede, padronizar aplicações, incrementar recursos didáticos, melhorar o suporte a aplicações multimídia atuais e futuras, facilitar a manutenção, implementação e reuso, alterar o layout de saída no browser de maneira dinâmica, explorar outros recursos de links, estabelecer padrões de organização do material instrucional criado pelo professor e muitas outras. Tal sistema anteriormente desenvolvido e funcionando adequadamente, é baseado no formalismo de Autômatos Finitos com Saída como modelo estrutural para organização de hiperdocumentos instrucionais, em especial em cursos na Web, tornando o material hipermídia independente do controle da aplicação. O Sistema Hyper-Automaton, tornou-se portanto, um sistema semi-automatizado para suporte a cursos na Web.  Com o desdobramento da pesquisa, esta procurou ir mais além e descreveu possibilidades de não só estudar os aspectos possíveis de formatação de saída do sistema, mas reestruturá-lo totalmente sobre uma linguagem de markup padrão, buscando atualizá-lo tecnologicamente definindo outras possibilidades para que significativos trabalhos futuros possam de maneira mais clara serem alcançados. Dessa maneira, esta dissertação centra-se no estudo da aplicação de formas de flexibilização do Sistema Hyper-Automaton, tanto na parte da estruturação de documentos que são conteúdos instrucionais armazenados, bem como, na forma desse material tornar-se-á disponível através de navegadores WWW compatíveis com as tecnologias propostas, possibilitando o incremento substancial de funcionalidades necessárias para cursos onde a Web é o principal meio. Esta pesquisa dá prosseguimento a dois trabalhos anteriormente concluídos no PPGC e do Curso de Bacharelado em Ciência da Computação da UFRGS no ano de 2000, na seqüência, Hyper-Automaton: Hipertextos e Cursos na Web Utilizando Autômatos Finitos com Saída, dissertação de mestrado de Júlio Henrique de A. P. Machado e Hyper-Automaton: Implementação e Uso, trabalho de diplomação de Leonardo Penczek.|http://hdl.handle.net/10183/3469
Determinação dos índices de vulnerabilidade física através de técnicas de sensoriamento remoto e geoprocessamento - municípios de Tavares e São José do Norte - litoral médio leste do RS|2000|Open Access|Dissertação|Sistemas de Informação Geográfica (SIG);Sensoriamento remoto;Meio ambiente : Risco;Uso da Terra : Litoral médio (RS)|por|Nos últimos anos a preocupação com a preservação dos recursos naturais, dos solo e das águas tem aumentado significativamente. Desta forma, a intensificação de estudos nestas áreas se faz necessária para tentar minimizar os impactos causados pela ação do homem, com vistas a recuperação. A área de estudo abrange os municípios de Tavares e S. José do Norte, situado no Litoral Médio Leste do Rio Grande do Sul, entre as coordenadas 31° 00’ e 32° 10’ de latitude Sul e 50° 00’ e 52° 10’ de longitude Oeste. Este trabalho busca fornecer subsídios para a identificação dos índices da vulnerabilidade física associados ao uso do solo da área de estudo visando a adequação do uso da terra, bem como o planejamento e o desenvolvimento de novas atividades. O estudo foi desenvolvido utilizando técnicas de sensoriamento remoto e geoprocessamento, onde os fatores foram cruzados via regra de decisão fazendo uso do SIG. Os resultados evidenciaram a necessidade de cuidados especiais no desenvolvimento de atividades sustentadas e ocupação humana.|http://hdl.handle.net/10183/3496
SAACI : sistema de apoio à aprendizagem colaborativa na internet|1999|Open Access|Dissertação|Informática : Educação;Ensino a distância;Aprendizagem colaborativa;Internet|por|O atual contexto social tem sido marcado pelo constante avanço tecnológico e científico, pela rápida defasagem dos conhecimentos e pela demanda por profissionais mais qualificadas e capazes de aprender e resolver problemas colaborativamente. A fim de atender esta demanda, o ensino tradicional tem que mudar, de forma que as pessoas possam desenvolver habilidades de “aprender a aprender” e “aprender colaborativamente”. A Aprendizagem Colaborativa tem sido apontada como uma alternativa a este problema, possibilitando uma aprendizagem flexível, ativa e centrada no aluno. Os alunos aprendem em colaboração com outros alunos, o que possibilita uma aprendizagem mais profunda, através de trocas de idéias, dúvidas e pontos de vista. Por outro lado, o advento da rede Internet e a consolidação desta como um importante meio de comunicação mundial têm introduzido novas possibilidades à Aprendizagem Colaborativa Apoiada por Computador (Computer Supported Collaborative Learning – CSCL). Uma atenção especial tem sido dada ao WWW, serviço que possui grande potencial como meio ativo de aprendizagem, embora seja amplamente utilizado como meio passivo de transmissão de informações pela Internet. Este cenário tem viabilizado a utilização da infra-estrutura da Internet para a criação de um sistema de CSCL de grande alcance e a um custo relativamente baixo.  O objetivo principal do presente trabalho é propor e implementar um modelo de CSCL baseado na Internet que forneça, de forma integrada, o suporte necessário para o desenvolvimento efetivo de atividades colaborativas de aprendizagem. O trabalho realizado pode ser dividido em três etapas principais: (1) o levantamento teórico, que consistiu no estudo das principais áreas relacionadas com o trabalho desenvolvido, o que foi fundamental para a definição do modelo de sistema de aprendizagem colaborativa proposto; (2) a definição do modelo, que consistiu no levantamento das atividades básicas que devem ser apoiadas por um ambiente de CSCL e na proposição do modelo de um sistema de CSCL baseado na Internet com suporte às atividades levantadas; (3) a implementação do modelo, que possibilitou a verificação da viabilidade da utilização do serviço WWW como base para um ambiente de CSCL, através da utilização de recursos de programação para web e da integração de ferramentas já existentes na Internet.|http://hdl.handle.net/10183/3503
Análise o comportamento hidrodinâmico e sedimentológico do estuario do Rio Piranji - CE (NE/Brasil)|2003|Open Access|Dissertação|Geologia marinha;Hidrodinâmica estuarina;Sedimentologia;Dinâmica ambiental|por|Cerca de dois terços das maiores áreas metropolitanas mundiais estão nas proximidades dos estuários, portanto são regiões comumente sujeitas à situações de risco impostas pela pressão populacional. Instalações portuárias, efluentes diversos, uso indevido das suas margens, estão entre os riscos potenciais à grande diversidade de seus recursos naturais. A ação das marés, das ondas e o aporte fluvial interagem tornando bastante complexa a caracterização do seu funcionamento. A presente dissertação foca os seguintes aspectos: hidrodinâmica, transporte de sedimentos em suspensão, sedimentologia, morfologia e classificação do ambiente. A hidrodinâmica e o transporte de sedimentos foram estudados em duas etapas de campo em condições de maré de sizígia e de quadratura, cada qual com abrangência de dois ciclos de maré (25 hs). Estas etapas foram realizadas numa estação fixa com medições de: maré, velocidade da corrente, material em suspensão, sólidos totais dissolvidos, salinidade, pH, oxigênio dissolvido e temperatura. As características sedimentológicas foram avaliadas pela coleta de sedimentos de fundo em 24 pontos. Os aspectos morfológicos foram reconhecidos por uma batimetria detalhada e por aerofotografias. Os resultados revelaram a dominância das correntes de maré vazante em sizígia e de enchente em quadratura. O fluxo total de sedimentos por ciclo de maré na sizígia foi de 71 ton. (vazante), enquanto que na quadratura foi de 2,2 ton. (enchente).  Os sedimentos de fundo variaram de areia muito grossa a silte. A profundidade máxima foi de 7,4 m (maré alta de sizígia). A comparação das aerofotografias revela a instabilidade morfológica da desembocadura. Relaciona-se este fato à influência variável da deriva litorânea e da descarga hidráulica. Este estuário foi classificado como: de frente de barreira arenosa; de circulação do tipo xvi parcialmente misturada e, por vezes, verticalmente homogênea; de Tipo 2b (quadratura); de tipo intermerdiário 2a e 2b (sizígia); de negativo; de mesomaré e de energia mista.|http://hdl.handle.net/10183/3514
Geoprocessamento aplicado à identificação de áreas para rejeitos e estimativa de recurso de carvão na região da Mina Leão II|2000|Open Access|Dissertação|Geoprocessamento;Deposição de rejeitos;Carvão;Geoestatística;Mina Leão II|por|Imagens georreferenciadas LANDSAT 5 TM da região da Mina Leão II, situada na Depressão Central do Estado do Rio Grande do Sul, foram processadas e classificadas digitalmente com objetivo de gerar o mapa de uso e cobertura do solo. Destas imagens, a drenagem foi extraída na forma vetorial, com o objetivo de determinar a faixa de proteção em torno dela. Dados topográficos plani-altimétricos analógicos foram tratados gerando o modelo digital do terreno e mapas de declividades. Foram definidos critérios para selecionar sítios adequados à colocação de rejeitos de carvão. Imagens de uso e cobertura do solo, declividades, rede de drenagem, litologias, estruturas geológicas, e distância a partir da boca da mina foram transformadas em sete fatores. Três fatores são absolutos ou restrições: zona de proteção da drenagem, zona de restrição em torno dos falhamentos e declividades superiores a 8%. Os restantes, são fatores relativos: uso e cobertura do solo reclassificado, declividade inferior a 8%, substrato litológico e distância a partir da mina. Aos quatro fatores relativos foi atribuída uma ponderação pareada. Através das ferramentas computacionais de apoio à decisão, em um Sistema de Informação Geográfica, os oito diferentes fatores foram cruzados, resultando um mapa temático que localiza e classifica sítios para a locação de rejeitos de carvão. As classes identificadas foram: área de restrição, péssima, regular, boa e ótima.  O mapa de uso e cobertura do solo foi reclassificado em função de ser elaborada uma imagem de superfície de atrito, a partir do local da boca da mina, com a finalidade de se projetar vias de menor custo, desde a mina até a BR290 e a um porto situado no rio Jacuí. Dados sobre a espessura da camada de carvão inferior, ""I"", de uma campanha de sondagem de 182 furos, foram tratados por metodologia de geoestatística. Estudos de estatística descritiva, análise de continuidade espacial e estimação foram realizados, culminando com a cubagem da camada na área de estudo. Foi escolhido o processo de interpolação através da krigagem ordinária. A tonelagem da camada de carvão ""I"" foi estimada na ordem de 274.917.234 a 288.046.829 t. com nível de confiança de 95%.|http://hdl.handle.net/10183/3525
Componentes de percepção para o ambiente PROSOFT cooperativo|2001|Open Access|Dissertação|Engenharia : Software;Prosoft;Trabalho cooperativo;Interface : Usuario|por|A Engenharia de Software tornou-se uma das principais áreas de concentração do mundo da Ciência da Computação, pois ela abrange métodos para construir sistemas, ferramentas que fornecem apoio a construção e os procedimentos a seguir para que os métodos estejam de acordo com as ferramentas usadas [PRE 95]. No desenvolvimento de sistemas cada vez mais complexos vê-se a necessidade da utilização de equipes de pessoas no projeto e implementação. Essas pessoas podem estar geograficamente dispersas e portanto possuem a necessidade de troca de informações para que os sistemas desenvolvidos sejam adequados com o objetivo inicial e de qualidade. Assim, os sistemas cooperativos, chamados de groupware, tornaram-se uma importante ferramenta utilizada por esse grupo de profissionais para que as tarefas desenvolvidas em grupo se tornem interativas e eficientes.  A interação entre as pessoas que estão trabalhando cooperativamente deve ser a mais produtiva possível, sendo semelhante ao trabalho em grupo em um único local. Assim, a percepção das atividades que estão sendo realizadas devem estar disponíveis para cada profissional, através da Interface Homem-Computador do sistema groupware que estão utilizando. Este trabalho apresenta uma “biblioteca” de Componentes de Percepção que fornecem as informações necessárias para que as pessoas que estão participando da tarefa cooperativa tenham a percepção das atividades que estão sendo realizadas, como também quem e como as estão fazendo. Esses componentes são uma extensão do Ambiente PROSOFT Cooperativo, fornecendo assim uma especificação formal de forma a garantir completeza, corretude e ausência de ambigüidades que são muito difíceis de se conseguir com uma descrição informal.|http://hdl.handle.net/10183/3526
Simulação e controle de um sistema de suspensão simplificado|2002|Open Access|Dissertação|Análise de sistemas;Estruturas de veículos;Simulação;Modelos dinâmicos;Métodos Runge-Kutta;Sistema de suspensão simplificado;Software matlab|por|As aplicações da mecânica vibratória vêm crescendo significativamente na análise de sistemas de suspensões e estruturas de veículos, dentre outras. Desta forma, o presente trabalho desenvolve técnicas para a simulação e o controle de uma suspensão de automóvel utilizando modelos dinâmicos com um, dois e três graus de liberdade. Na obtenção das equações do movimento para o sistema massa-mola-amortecedor, o modelo matemático utilizado tem como base a equação de Lagrange e a segunda lei de Newton, com condições iniciais apropriadas. A solução numérica destas equações é obtida através do método de Runge-Kutta de 4ª ordem, utilizando o software MATLAB. Para controlar as vibrações do sistema utilizou-se três métodos diferentes de controle: clássico, LQR e alocação de pólos. O sistema assim obtido satisfaz as condições de estabilidade e de desempenho e é factível para aplicações práticas, pois os resultados obtidos comparam adequadamente com dados analíticos, numéricos ou experimentais encontrados na literatura, indicando que técnicas de controle como o clássico podem ser simples e eficientes.|http://hdl.handle.net/10183/3539
Proposta de um modelo gráfico e navegacional básico para interfaces de aplicações educacionais baseado em validação experimental|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Estrutura : Navegação|por|O desenvolvimento de projetos de interfaces gráficas está apoiado em guias de recomendações. Estes guias apresentam regras genéricas aos projetos de Interfaces Homem Computador–IHC. Entretanto, aplicações particulares, como as educacionais, não encontram regras específicas que atendam as necessidades do usuário-alvo. Ainda, a análise comparativa entre guias disponíveis aponta contradições entre as recomendações genéricas com aquelas específicas aplicadas a um determinado ambiente. A necessidade de um modelo de referência para a construção de interfaces gráficas amigáveis ao usuário e a escassez e contradições de recomendações específicas às aplicações educacionais motivaram o trabalho de pesquisa prática junto aos usuáriosalvo. Para a identificação das recomendações sobre aspectos gráficos básicos e elementos de navegação necessários a uma efetiva interação com interfaces dedicadas a aplicações educacionais, foi desenvolvido um instrumento de pesquisa que permitiu a investigação das preferências relativas aos aspectos pesquisados junto ao público-alvo.  Os dados coletados foram tratados estatisticamente e os resultados obtidos contrariam tanto critérios adotados em interfaces de sistemas de ensino disponíveis na Internet como algumas recomendações sobre os mesmos disponíveis na literatura. Os resultados obtidos apontam, também, para a preferência dos usuários por elementos de navegação que não são referidos nos guias de recomendações consultados. A análise dos resultados possibilitou a geração de um modelo básico que recomenda preferências sobre aspectos gráficos básicos, como aplicação de cores em fontes e fundos de tela, tipologia de fontes para textos e paginação, e também, sobre componentes de navegação, como posicionamento e preferência por tipo de recurso de navegação. O modelo proposto está fundamentado nas recomendações de Nielsen [NIE 00], o qual recomenda que as necessidades dos usuários na interatividade com a interface sejam identificadas junto a estes usuários. As recomendações apresentadas neste trabalho foram utilizadas, inicialmente, nos ambientes educacionais desenvolvidos dentro dos projetos Tapejara [TAP 00] e LaVia [LAV 00].|http://hdl.handle.net/10183/3543
Raízes polinomiais em corpos finitos|2004|Open Access|Dissertação|Raízes polinomiais;Corpos finitos|por|Este trabalho é um estudo sobre propriedades de decomposição de polinômios em corpos finitos. Em particular fazemos um estudo sobre métodos de fatoração e cálculos de raízes. Procedemos inicialmente com um apanhado de conceitos e teoremas que embasam o trabalho. Com o objetivo de determinar raízes de polinômios em corpos finitos, alguns tópicos tornam-se pré-requisitos. O primeiro deles é a própria representação dos elementos dos corpos finitos. O outro é o estudo de métodos determinísticos ou probabilísticos para fatorar polinômios sobre corpos finitos. Os métodos estudados são o de Berlekamp, Cantor-Zassenhaus e Lidl-Niederreiter. Fazemos finalmente o estudo de métodos que podem ser empregados para determinarmos as raízes de polinômios pertencentes a corpos finitos. Métodos estes que apresentam variações de acordo com o tamanho do corpo.|http://hdl.handle.net/10183/3554
Abordagem baseada em objetivos para construção de casos de uso e cenários|2003|Open Access|Dissertação|Orientacao : Objetos;Sistemas interativos;Interface : Usuario;Engenharia : Software|por|Para o desenvolvimento de sistemas interativos que respeitem critérios de usabilidade em adição aos critérios de qualidade convencionais, é necessário que, desde suas primeiras etapas, as áreas de Engenharia de Software (ES) e de Interação Humano- Computador (IHC) sejam consideradas, simultaneamente e de maneira integrada. Essas duas áreas investigam modelos, conceitos, técnicas e práticas que refletem diferentes perspectivas sobre a atividade de desenvolvimento, uma orientada mais ao sistema (ES) e outra, mais ao usuário (IHC). Para conciliar estas perspectivas, é necessário o estabelecimento de um entendimento mútuo e a utilização conjunta e integrada de conceitos, técnicas e práticas de desenvolvimento de ambas as áreas. Este trabalho visa mostrar as possibilidades desta integração, através da combinação dos conceitos de Casos de Uso (Use Cases) e Cenários (Scenarios), importantes técnicas de modelagem amplamente utilizadas respectivamente nas áreas de ES e IHC, em diferentes contextos, com diferentes visões; mas apresentando similaridades valiosas para propiciarem o uso complementar de ambas as técnicas.  Para sistematizar esta integração, é proposta uma abordagem teleológica – baseada em objetivos – de construção sistemática de casos de uso com quatro diferentes níveis de abstração, desde os mais abstratos casos de uso essenciais até os cenários, aqui utilizados como instâncias concretas de casos de uso. Com esta abordagem, pretende-se construir um modelo de casos de uso que permita especificar requisitos funcionais, conjuntamente com requisitos de interação, de maneira compreensível e praticável e que sirva como ponto de partida à continuidade do desenvolvimento orientado a objetos de software. Com o intuito de exemplificar a proposta, é descrita e discutida a aplicação passo a passo desta abordagem a um exemplo.|http://hdl.handle.net/10183/3610
Animação bidimensional para World Wide Web baseada em autômatos finitos|2002|Open Access|Dissertação|Internet;Teoria : Automatos;Automatos finitos;Animacao : Computacao grafica;Recuperacao : Informacao;Armazenamento : Dados|por|Este trabalho aplica a Teoria de Autômatos na proposição de uma nova alternativa para prover animações 2D na World Wide Web, verificando as contribuições alcançadas para as questões relacionadas ao espaço de armazenamento, reutilização e manutenção do conteúdo e suporte à recuperação de informação. Para este objetivo, é proposto o modelo AGA (Animação Gráfica baseada em Autômatos Finitos), o qual especifica a animação a partir de uma estrutura baseada em autômatos finitos com saída. Esse modelo é definido de tal forma que os mesmos autômatos utilizados na especificação, ao serem simulados, realizam o controle da animação durante a apresentação. O modelo AGA apresenta características que favorecem a redução do espaço de armazenamento da animação, provêem suporte à recuperação de informação, colaboram com a reutilização e manutenção do conteúdo das animações. Uma implementação multiplataforma foi desenvolvida para apresentar animações especificadas nesse modelo na Web. Essa implementação proporciona a elaboração de consultas ao conteúdo da animação, além dos recursos tradicionais de reprodução. A partir dessa implementação, o AGA foi submetido a um estudo de caso prático, onde os resultados obtidos são comparados com o produzidos pelo GIF (Graphic Interchange Format).  Esse comparativo demonstra que o AGA possui várias vantagens em relação à estrutura adotada pelo GIF. O modelo AGA é estendido utilizando autômatos temporizados para prover restrições temporais às especificações e também ampliar as funcionalidades de interação com o observador da animação. Essa extensão, chamada de modelo AGA-S (Animação Gráfica baseada em Autômatos Temporizados Sincronizados), é definida a partir do autômato temporizado proposto por Alur e Dill. Para esse modelo, é definida uma operação formal para sincronização dos componentes da animação e adicionada uma estrutura baseada em autômatos finitos para controlar a interação do observador com a animação.|http://hdl.handle.net/10183/3616
FRIDA: um método para elicitação e modelagem de RNFs|2004|Open Access|Tese|Requisitos : Software;Desenvolvimento : Software;Qualidade : Software;Engenharia : Software|por|Os requisitos direcionam o desenvolvimento de software porque são cruciais para a sua qualidade. Como conseqüência tanto requisitos funcionais quanto não funcionais devem ser identificados o mais cedo possível e sua elicitação deve ser precisa e completa. Os requisitos funcionais exercem um papel importante uma vez que expressam os serviços esperados pela aplicação. Por outro lado, os requisitos não funcionais estão relacionados com as restrições e propriedades aplicadas ao software. Este trabalho descreve como identificar requisitos não funcionais e seu mapeamento para aspectos. O desenvolvimento de software orientado a aspectos é apontado como a solução para os problemas envolvidos na elicitação e modelagem dos requisitos não funcionais. No modelo orientado a aspectos, o aspecto é considerado o elemento de primeira ordem, onde o software pode ser modelado com classes e aspectos. As classes são comumente usadas para modelar e implementar os requisitos funcionais, já os aspectos são adotados para a modelagem e implementação dos requisitos não funcionais. Desse modo, é proposta a modelagem dos requisitos não funcionais através das fases do ciclo de vida do software, desde as primeiras etapas do processo de desenvolvimento. Este trabalho apresenta o método chamado FRIDA – From RequIrements to Design using Aspects, cujo objetivo é determinar uma forma sistemática para elicitar e modelar tanto os requisitos funcionais quanto os não funcionais, desde as fases iniciais do ciclo de desenvolvimento.  Em FRIDA, a elicitação dos requisitos não funcionais é realizada usando-se checklists e léxicos, os quais auxiliam o desenvolvedor a descobrir os aspectos globais – utilizados por toda a aplicação – bem como, os aspectos parciais que podem ser empregados somente a algumas partes da aplicação. O próximo passo consiste na identificação dos possíveis conflitos gerados entre aspectos e como resolvê-los. No método FRIDA, a identificação e resolução de conflitos é tão importante quanto a elicitação de requisitos não funcionais, nas primeiras fases do ciclo de vida do software. Além disso, é descrito como usar a matriz de conflitos para automatizar esse processo sempre que possível. A extração dos aspectos e sua modelagem visual são características muito importantes, suportadas pelo método, porque elas possibilitam a criação de modelos que podem ser reutilizados no futuro. Em FRIDA, é demonstrado como transformar os requisitos em elementos da fase de projeto (classes e aspectos) e como traduzir esses elementos em código. Outra característica do método FRIDA é que a conexão entre diagramas, que pertencem a diferentes fases do processo de desenvolvimento do software, permite um alto nível de rastreabilidade. Em resumo, FRIDA requer que o desenvolvedor migre de uma visão puramente funcional para outra que contemple também os requisitos não funcionais.|http://hdl.handle.net/10183/3618
Uma Solução baseada em políticas para gerenciamento integrado de QoS e Multicast em redes IP|2003|Open Access|Dissertação|Comunicacao multicast;Redes : Computadores;Política : Redes : Computadores|por|Aplicações como videoconferência, vídeo sob-demanda, aplicações de ensino a distância, entre outras, utilizam-se das redes de computadores como infra-estrutura de apoio. Mas para que tal uso seja efetivo, as redes de computadores, por sua vez, devem fornecer algumas facilidades especiais para atender às necessidades dessas aplicações. Dentre as facilidades que devem ser fornecidas estão os suportes à qualidade de serviço (QoS - Quality of Service) e as transmissões multicast. Além do suporte a QoS e multicast nas redes, é necessário fornecer um gerenciamento da rede adequado às expectativas de tais aplicações. Soluções que fornecem gerenciamento de forma individual para tais facilidades, já foram propostas e implementadas. Entretanto, estas soluções não conseguem agir de modo integrado, o que torna a tarefa do gerente da rede extremamente complexa e difícil de ser executada, pois possibilitam um fornecimento não adequado das facilidades desejadas às aplicações. Nesta dissertação é apresentada uma solução para gerenciamento integrado de QoS e multicast. Fazem parte da solução: a definição e implementação de uma arquitetura para gerenciamento integrado de QoS e multicast, utilizando gerenciamento baseado em políticas (PBNM - Policy-Based Network Management), além da validação da solução proposta através da implementação de um protótipo. Um ambiente, condições de teste, e análise dos resultados obtidos, também são apresentados durante a dissertação.|http://hdl.handle.net/10183/3632
SAM: um sistema adaptativo para transmissão e recepção de sinais multimídia em redes de computadores|2003|Open Access|Tese|Redes;Computadores;Multimídia|por|Esta Tese apresenta o SAM (Sistema Adaptativo para Multimídia), que consiste numa ferramenta de transmissão e recepção de sinais multimídia através de redes de computadores. A ferramenta pode ser utilizada para transmissões multimídia gravadas (vídeo sob demanda) ou ao vivo, como aulas a distância síncronas, shows e canais de TV. Seu maior benefício é aumentar o desempenho e a acessibilidade da transmissão, utilizando para isso um sinal codificado em camadas, onde cada camada é transmitida como um grupo multicast separado. O receptor, utilizando a ferramenta, adapta-se de acordo com a sua capacidade de rede e máquina no momento. Assim, por exemplo, um receptor com acesso via modem e outro via rede local podem assistir à transmissão na melhor qualidade possível para os mesmos. O principal foco da Tese é no algoritmo de controle de congestionamento do SAM, que foi denominado ALM (Adaptive Layered Multicast). O algoritmo ALM tem como objetivo inferir o nível de congestionamento existente na rede, determinando a quantidade de camadas que o receptor pode suportar, de forma que a quantidade de banda recebida gere um tráfego na rede que seja eqüitativo com outros tráfegos ALM concorrentes, ou outros tráfegos TCP concorrentes.  Como se trata de transmissões multimídia, é desejável que a recepção do sinal seja estável, ou seja, sem muitas variações de qualidade, entretanto, o tráfego TCP concorrente é muito agressivo, dificultando a criação de um algoritmo estável. Dessa forma, desenvolveu-se dois algoritmos que formam o núcleo desta Tese: o ALMP (voltado para redes privativas), e o ALMTF (destinado a concorrer com tráfego TCP). Os elementos internos da rede, tais como os roteadores, não necessitam quaisquer modificações, e o protocolo funciona sobre a Internet atual. Para validar o método, se utilizou o software NS2 (Network Simulator), com modificações no código onde requerido. Além disso, efetuou-se uma implementação inicial para comparar os resultados das simulações com os da implementação real. Em http://www.inf.unisinos.br/~roesler/tese e também no CD anexo a esta Tese, cuja descrição encontra-se no Anexo B, podem ser encontrados todos os programas de apoio desenvolvidos para esta Tese, bem como a maior parte da bibliografia utilizada, o resultado das simulações, o código dos algoritmos no simulador e o código do algoritmo na implementação real.|http://hdl.handle.net/10183/3633
Depuração de programas paralelos : projeto de uma interface intuitiva|2002|Open Access|Tese|Programação;Programação paralela;Depuracao paralela|por|A programação paralela é sem dúvida mais complexa do que a programação seqüencial. O controle de múltiplos processos e de suas interações são as principais razões para tal complexidade. Apesar da existência de algumas ferramentas que atendem à fase de desenvolvimento de programas paralelos, a complexidade é normalmente passada para as ferramentas paralelas, isto é, as ferramentas não são de fácil utilização. Assim, existe uma necessidade de ambientes e ferramentas realmente fáceis de usar no âmbito da programação paralela. Embora existam algumas ferramentas interessantes, inclusive algumas comerciais, seu uso permanece insuficiente, em parte devido à complexidade na utilização de algumas delas, em parte devido ao seu uso específico em determinadas plataformas. Portanto, existe ainda um grande campo de estudo no que diz respeito a melhorias de projeto sobre ferramentas existentes e desenvolvimento de ferramentas com um maior número de recursos. Provavelmente, a ferramenta paralela mais necessária aos programadores é o depurador paralelo. Por sua vez, ferramentas de depuração paralela estão entre as mais complexas de se desenvolver e talvez isso explique o motivo pelo qual poucas têm sido efetivamente utilizadas.  Este trabalho descreve uma contribuição no campo da depuração paralela através da análise de interfaces de depuração paralela e da proposta de um modelo. A partir deste modelo, uma interface de depuração paralela – PADI (PArallel Debugger Interface) foi desenvolvida e seu principal objetivo é o de oferecer uma interface intuitiva e de fácil utilização. O modelo proposto e conseqüentemente a ferramenta PADI tratam da depuração paralela simbólica on-line. A depuração on-line trata do oferecimento de acesso aos símbolos do programa, como variáveis e registradores. A depuração on-line diferencia-se da off-line pelo tipo de interação com a execução do programa. A depuração on-line oferece interação direta com a aplicação, enquanto que a off-line interage com um arquivo de monitoração gravado durante a execução da aplicação paralela. A depuração on-line é similar à depuração seqüencial tradicional e, conseqüentemente, é de mais fácil utilização por parte da maioria dos programadores.|http://hdl.handle.net/10183/3639
Híbridos à base de anilina/sílica obtidos através do processo sol-gel: síntese, caracterização e propriedades|2003|Open Access|Tese|Gel de sílica : Síntese;Sol-gel|por|Neste trabalho inicialmente foi sintetizado o precursor organoalcoxisilano 3- anilinapropiltrimetoxisilano, APTMS, a partir da reação entre a anilina e o 3- cloropropiltrimetoxisilano, CPTMS, usando hidreto de sódio, NaH, como ativador de base. O precursor inorgânico tetraetilortosilicato, TEOS, foi polimerizado na presença do precursor orgânico, e catalisador. Durante a etapa de polimerização dos precursores alcóxidos, foram investigadas as influências da temperatura, do tipo e concentração do catalisador e da concentração de precursor orgânico adicionado nas propriedades finais dos híbridos anilinapropilsilica resultantes. Os materiais híbridos foram caracterizados através das técnicas: espectroscopia no infravermelho (FT-IR), técnica de espalhamento de Raios-x a baixos ângulos (SAXS), microscopia eletrônica de varredura (SEM), isotermas de adsorção e dessorção de nitrogênio, análise elementar, espectroscopia de aniquilação de pósitrons (PALS), análise termogravimétrica (TGA), espectroscopia de elétrons dispersos (EDS). A potencialidade de aplicação dos híbridos obtidos como materiais adsorventes na extração dos cátions Cu(II), Zn(II), Cd(II) e Co(II), em solução, foi investigada usando-se isotermas de adsorção pelo método batelada. Através das técnicas de caracterização foi possível observar que as propriedades morfológicas dos materiais como área superficial, distribuição do tamanho de poros, forma e tamanho das partículas, além da estabilidade e grau de incorporação orgânica podem ser controladas em maior ou menor grau, dependendo da variável usada durante a polimerização dos precursores alcóxidos. O material apresentou propriedades promissoras como adsorvente seletivo para o cobre.|http://hdl.handle.net/10183/3644
Uma Ferramenta para auxiliar o professor no ensino a distância|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Inteligencia artificial distribuida;Sistemas multiagentes|por|Com a proliferação de computadores pessoais e a popularização da Internet, as novas tecnologias da comunicação começam a provocar impactos no setor educacional, com a promessa de construção de cenários inovadores, apoiados em diferentes formas de educação baseada na Web. Estas inovações tecnológicas aplicadas ao ensino caracterizam a intensificação dos processos de educação à distância como uma das tendências mais marcantes desse final de milênio. A tendência destas inovações é crescer e juntamente com a Internet contribuir para a melhoria do ensino e/ou aprendizagem a distância. A Educação à Distância (EAD) tem sido um termo usado para qualquer forma de estudo em que os alunos não estejam em contato direto com seu professor. Este tipo de ensino pode ser considerado como uma alternativa educacional capaz de propiciar a ampliação de oportunidades educativas, através de programas de qualidade. Atualmente, para desenvolver ambientes para o ensino a distância são utilizados os conceitos de Inteligência Artificial Distribuída, mais precisamente o conceito de sistemas multiagentes, com a finalidade de aprimorar e monitorar o ensino através da Internet.  O sistema AME-A proposto por D’Amico é um exemplo de ambiente que utiliza a tecnologia de multiagentes. Ele é constituído de vários agentes inteligentes que atuam de forma concorrente e comunicam-se através de mensagens. Dessa maneira, transformam o sistema em um conjunto de agentes cooperantes em busca de um objetivo que é o de ensinar e/ou aprender. Este trabalho tem como objetivo utilizar o estudo e a abordagem de modelagem do sistema de ensino/aprendizagem definido por D’Amico no que se refere ao agente ferramentas para o professor. Propõe-se a desenvolver uma ferramenta que irá auxiliar o professor na distribuição de seus conhecimentos na Internet, armazenando as informações inseridas pelo professor bem como fornecer orientações a respeito de determinadas operações que o sistema realiza. Estas orientações também apresentam como foco os seguintes fatores: descrição da importância da motivação e interatividade num curso a distância, sugestões de metodologias de ensino que o professor pode usar nas suas aulas, orientações a respeito de avaliação do aprendizado do aluno, dicas sobre conteúdos de materiais complementares às aulas e por fim, menciona exemplos de ferramentas em modo texto e multimídia que podem ser utilizadas para comunicação.|http://hdl.handle.net/10183/3670
Um estudo sobre curvas NURBS|2003|Open Access|Dissertação|Algoritmos numéricos;Algoritmos algebricos;Curvas NURBS;B-spline|por|O objetivo primordial desse trabalho está concentrado no estudo de Curvas NURBS (B-spline Racional N˜ao-Uniforme). A literatura em português sobre NURBS é escassa, pouco difundida e os textos e artigos existentes tendem a ser rigorosos, longos e teóricos. Assim, o presente estudo está direcionado para os conceitos matemáticos de NURBS, para o qual foi utilizado uma ferramenta chamada DesignMentor com a finalidade de testar os algoritmos desses conceitos. NURBS são funções paramétricas que podem representar qualquer tipo de curva. NURBS são usadas em computação gráfica na indústria de CAD/CAM e estão sendo consideradas um padrão para criar e representar objetos complexos (indústria automobilística, aviação e embarcação). As ferramentas de criação gráfica mais sofisticadas provêem uma interface para usar NURBS, que são flexíveis suficiente para projetar uma grande variedade de formas. Hoje é possível verificar o uso expandido de NURBS, modelando objetos para as artes visuais, arte e escultura; também estão sendo usados para modelar cenas para aplicações de realidade virtual. NURBS trabalha bem em modelagem 3D, permitindo facilidade para manipular e controlar vértices, controlar curvatura e suavidade de contornos. NURBS provêm uma base matemática, unificada para representar formas analíticas e livres além de manter exatidão e independência de resolução matemática.|http://hdl.handle.net/10183/3686
Interface de cálculo modal e visualização para estruturas flexíveis tipo Euler-Bernoulli|2003|Open Access|Dissertação|Cálculo modal;Estruturas flexíveis|por|Neste trabalho, foram analisadas, implementadas e mostradas as características de uma interface com estrutura de programação genérica para o ambiente Windows, criando facilidades de rapidez, confiabilidade e a apresentação de resultados aos usuários do sistema matemático Maple na criação e uso de aplicações matemáticas. A interface utilizou como modelo de implementação cálculos modais de vigas clássicas de Euler-Bernoulli. O usuário encontra, em um único sistema, cálculo para vigas clássicas, terá uma entrada de dados facilitada de variáveis que serão substituídas automaticamente no programa fonte da viga e a geração de resultados em um ambiente amigável com dados e gráficos mostrados de forma organizados.|http://hdl.handle.net/10183/3691
Uma Proposta de uso da Arquitetura Trace como um sistema de detecção de intrusão|2002|Open Access|Dissertação|Redes : Computadores;Seguranca : Redes : Computadores;Gerencia : Redes : Computadores;Detecção : Intrusão|por|Este trabalho propõe a utilização da arquitetura Trace como um sistema de detecção de intrusão. A arquitetura Trace oferece suporte ao gerenciamento de protocolos de alto nível, serviços e aplicações através de uma abordagem baseada na observação passiva de interações de protocolos (traços) no tráfego de rede. Para descrever os cenários a serem monitorados, é utilizada uma linguagem baseada em máquinas de estado. Esta linguagem permite caracterizar aspectos observáveis do tráfego capturado com vistas a sua associação com formas de ataque. O trabalho mostra, através de exemplos, que esta linguagem é adequada para a modelagem de assinaturas de ataques e propõe extensões para permitir a especificação de um número maior de cenários ligados ao gerenciamento de segurançaa. Em seguida, é descrita a implementação do agente de monitoração, componente-chave da arquitetura Trace, e sua utilização para detectar intrusões. Esse agente (a) captura o tráfego da rede, (b) observa a ocorrência dos traços programados e (c) armazena estatísticas sobre a sua ocorrência em uma base de informações de gerenciamento (MIB { Management Information Base). O uso de SNMP permite a recuperação destas informações relativas µa ocorrências dos ataques. A solução apresentada mostrou ser apropriada para resolver duas classes de problemas dos sistemas de detecção de intrusão: o excesso de falsos positivos e a dificuldade em se modelar certos ataques.|http://hdl.handle.net/10183/3704
Visualização de estruturas internas em volumes de dados multimodais|2002|Open Access|Tese|Informática médica;Computação gráfica;Visualizacao volumetrica;Processamento : Imagens médicas|por|Com o aperfeiçoamento de técnicas de aquisição de imagens médicas, como, por exemplo, a tomografia computadorizada e ressonância magnética, a capacidade e a fidelidade do diagnóstico por imagens foram ampliadas. Atualmente, existe a tendência de utilizarem-se imagens através de diversas modalidades para um único diagnóstico, principalmente no caso de doenças graves. Entretanto, o registro e a fusão dessas imagens, chamadas mutimodais, em uma única representação 3D do paciente é uma arefa extremamente dif[icil, que consome tempo e que está sujeita a erros. Sendo assim, a integração de imagens de diferentes modalidades tem sido objeto de pesquisa sob a denominação de Visualização de Volumes de Dados Multimodais. Sistemas desenvolvidos com este objetivo são usados, principalmente, para combinar informações metabólicas e funcionais com dados de anatomia, aumentando a precisão do diagnóstico, uma vez que possibilitam extrrair uma superfície ou região da imagem que apresenta a anatomia, e, então, observar a atividade funcional na outra modalidade. Durante a análise de tais imagens, os médicos estão interessados e quantificar diferentes estruturas. Seusobjetivos envolvem, por exemplo, a visualização de artérias e órgãos do corpo humano para análise de patologias, tais como tumores, má-formações artério-venosas, ou lesões em relação às estuturas que as circundam. Assim, um dos principais obetivos de um algoritmo de visualização volumétrica é permitir a identificação e exploração de estruturas internas no volume. Como o volume é normalmente um ""bloco de dados"", não se pode visualizar o seu interior, a menos que se assuma que é possível ver através de voxels transparentes, ou que é possivel remover voxels que estão na frente na qual o usuário está interessado, o que foi feito através de técnicas de segmentação ou de corte. Este trabalho presenta uma abordagem para a visualização de estruturas internas em volumes de dados multimodais. A abordagem está fundamentada na utilização de ferramentas de corte, tanto geométricas quanto baseadas em conteúdo, evitando, assim, o uso de técnicas de segmentação; e na integração dos dados multimodais na etapa de acumulação de pipeline de visualização volumétrica. Considerando que as aplicações que suportam este tipo de visualização envolvem a integração de várias ferramentas, tais como registro, corte e visualização, também é apresentado o projeto de um framework que permite esta integração e um alto grau de interação com usuário. Para teste e validação das técnicas de visualização de estruturas internas propostas e do algoritmo desenvolvido, que consiste numa extensão do algoritmo de ray casting tradicional, foram implementadas algumas classes desse framework. Uma revisão baseada na análise e na classificação das ferramentas de corte e funções de transferências, que correspondem a técnicas que permitem visualizar estruturas internas, também é apresentada.|http://hdl.handle.net/10183/3705
Estudo da evolução estrutural induzida por irradiação iônica em multicamadas de Fe/sub 30/Co/sub 70/Cu|2002|Open Access|Dissertação|Íons;Temperatura;Evaporação;Multicamadas;Energia;Ligas;Propriedades estruturais;Difração;Refletividade;Espectroscopia;Difração de raios X;Cristalinidade;Ondas eletromagnéticas;Transformações de fase;Ferro;Cobalto;Cobre;Silício|por|Neste trabalho, relata-se o comportamento estrutural de multicamadas de Fe30Co70/Cu, quando submetidas a irradiações com íons de He+ e Kr+ e tratamentos térmicos. As multicamadas foram confeccionadas à temperatura ambiente, numa câmara de ultra-alto vácuo por evaporação alternada de uma liga Fe30Co70 e Cu. As energias das irradiações foram escolhidas de tal forma que os íons atravessassem a multicamada, ficando alojados no substrato de Si. As doses foram calculadas para que os dois tipos de íons depositassem a mesma energia total na multicamada. Após a confecção dos filmes, os mesmos foram submetidos a tratamentos térmicos de 10 minutos em temperaturas variando de 350 a 450 oC. Este processo quebra a estrutura de camadas e forma uma liga heterogênea de grãos magnéticos embebidos numa matriz metálica de Cu. Para investigar a evolução das propriedades estruturais das amostras utilizou-se difração, refletividade e espectroscopia de absorção de raios-X. Os resultados da refletividade de raios-X mostraram claramente que uma estrutura de multicamadas é formada após a evaporação, fato que é comprovado pelo pico de Bragg observado nas curvas de refletividade. As irradiações com He+ e Kr+ não destroem a estrutura de multicamadas, ao contrário, elas provocam um alisamento de todas as interfaces.  Pelas medidas de difração de raios-X verificou-se que as irradiações resultam no aumento de cristalinidade e no crescimento de tamanho dos cristalitos do sistema. A irradiação com He+ não provoca mudanças significativas na multicamada, mas a irradiação com Kr+ induz uma transformação de fase de bcc para fcc/hcp nas camadas de Fe30Co70. Esta nova fase induzida pela irradiação com Kr+ é metaestável, pois um simples tratamento térmico de 450 oC durante 10 minutos destrói a nova estrutura fcc/hcp, fazendo com que a liga retorne a estrutura bcc. As medidas de absorção de raios-X estão em perfeito acordo com as medidas de difração e refletividade, mostrando que as irradiações promovem uma melhora na ordem cristalográfica. De acordo com o que foi observado das medidas, a irradiação com He+ não provoca nenhum efeito significativo na estrutura da liga de FeCo. Contudo, a irradiação com Kr+ induz uma transformação de fase nesta liga de bcc para uma estrutura fcc e/ou hcp. Tal transformação é induzida pela irradiação com Kr+ porque estes íons transferem mais energia, em colisões nucleares, do que os íons de He+. Além disso, estes íons provocam um grande número de cascatas de colisões, fazendo com que os átomos contidos dentro do volume destas cascatas tornem-se móveis. Desta forma, os átomos de Fe e Co se rearranjam em uma estrutura fcc, que é imposta pelas camadas de Cu adjacentes. Embora a estrutura fcc seja muito mais provável, também é possivel que a estrutura induzida seja hcp.|http://hdl.handle.net/10183/3713
Estudo das propriedades magnéticas e estruturais de filmes ultrafinos de Fe, Co e Ni/Au(111) produzidos por eletrodeposição|2002|Open Access|Tese|Propriedades magnéticas;Propriedades estruturais;Filmes finos;Eletrodeposição;Ferro;Cobalto;Níquel;Ouro;EXAFS;Retroespalhamento rutherford;Anisotropia magnética perpendicular;Degradação;Hidrogênio;Microscopia de tunelamento|por|Neste trabalho foram estudadas as propriedades magnéticas e estruturais de filmes ultrafinos de Fe, Co e Ni produzidos por eletrodeposição sobre substratos de Au(111). Os estágios iniciais de crescimento dos filmes foram estudados por técnicas de caracterização “in-situ”. Uma nova técnica de caracterização do estado magnético de filmes ultrafinos eletrodepositados (EC-AGFM) foi utilizada, mostrando-se uma poderosa ferramenta para o estudo das propriedades magnéticas dos filmes. Outras técnicas, como STM “in-situ”, PMOKE “in-situ”, EXAFS, XRD, RBS foram utilizadas. A análise dos dados revelaram resultados diferentes para os filmes de Fe e Co/Au(111), em comparação aos filmes de Ni/Au(111). Enquanto a anisotropia magnética perpendicular (PMA) foi observada para os filmes de Fe e Co/Au(111), não foi observada para os filmes de Ni/Au(111). Os resultados são interpretados em termos das contribuições para a anisotropia magnética dos filmes. No caso do níquel, a degradação de suas propriedades magnéticas são atribuídas à incorporação de hidrogênio durante a deposição. Os resultados das análises magnética e estrutural são correlacionados a fim de compreender o comportamento das propriedades observadas. Os resultados são comparados aos obtidos por técnicas em vácuo.|http://hdl.handle.net/10183/3714
Dinâmica estocástica de íons sujeitos a um conjunto quase-monocromático de ondas do tipo híbrida inferior|2003|Open Access|Tese|Difusão estocástica;Íons;Propagacao de ondas eletromagneticas em plasmas;Sistemas hamiltonianos;Transformações de fase|por|Neste trabalho, estudamos a interação de íons com um conjunto quase-monocromático de ondas eletrostáticas de frequência na faixa das frequências híbridas inferiores, propagando-se perpendicularmente a um campo magnético uniforme. Consideramos que as fases das ondas são aleatoriamente distribuídas (ondas incoerentes), tratando o caso de ondas de fases coerentes (ondas coerentes) como um caso particular. Derivamos o Hamiltoniano adequado a esse sistema, e deduzimos as equações de movimento, cujas soluções são analisadas numericamente, mostrando a ocorrência de difusão estocástica no espaçoo de fase ângulo-ação, para amplitudes de onda suficientemente grandes. Também fazemos estimativas sobre a amplitude mínima (threshold) para o aparecimento de ilhas de primeira ordem no espaço de fase. Estimamos, também, o limiar para as ilhas de segunda ordem e de ordens maiores, bem como o limiar de estocasticidade. A análise mostra que para o caso de várias ondas o comportamento estocástico ocorre antes do limiar de estocasticidade comparado com o caso de uma onda. No caso de ondas coerentes, observa-se que o limiar de estocasticidade diminui com o aumento do número de ondas que comp˜oem o conjunto de ondas, proporcionalmente ao inverso da raiz quadrada deste número, portanto, tendendo a ser nulo no limite em que o número de ondas no pacote tende a infinito. No caso de ondas incoerentes, observa-se também uma diminuição do limiar de estocasticidade com o aumento do número de ondas, mas nesse caso, saturando com valor até um terço do valor do limiar de estocasticidade para o caso de uma onda.  Observa-se também que o limite superior da região de estocasticidade no espaço de fase aumenta com o aumento do número de ondas. No caso de ondas coerentes, esse aumento é proporcional à raiz cúbica do número de ondas que compõem o conjunto de ondas. No caso de ondas incoerentes o limite superior da região de estocasticidade têm um aumento de até o dobro em relação ao caso de uma onda. A análise também mostra que o mecanismo da estocasticidade para o caso de várias ondas é diferente do mecanismo atuante no caso de uma onda. No caso de uma onda, a estocasticidade ocorre por superposição de ilhas de ordens maiores do que um, com o aumento da intensidade da onda. No caso de várias ondas, a presençaa de ondas de frequências próximas à frequência de ressonância causa pequenas perturbações na trajetória principal das partículas, causada pela onda central, espalhando-a pelo espaço de fase de forma mais eficiente que o mecanismo de estocasticidade para o caso de uma onda.|http://hdl.handle.net/10183/3718
Fenomenologia em cromodinâmica quântica com propagador de glúon modificado|2003|Open Access|Tese|Gluons;Cromodinâmica quântica;Teoria de campos;Difração;Pomerons;Fontes de infravermelho;Espalhamento proton proton;Mesons|por|A aplicabilidade e usos de propagadores de glúon modificados na Cromodinâmica Quântica (QCD) em diferentes espécies de processos é analisada. Os propagadores modificados de glúon são obtidos por diversos métodos, em especial, as equações de Dyson-Schwinger e simulações numéricas em teoria de campos na rede. Os processos em que estes propagadores são empregados em QCD podem ser divididos em duas classes: os difrativos e os perturbativos. Nos primeiros, a troca do pomeron é relevante e as propriedades infravermelhas da teoria são importantes, como no espalhamento elástico próton-próton e na produção de mésons vetoriais massivos. Os processos perturbativos, como o decaimento de mésons massivos e fatores de forma de mésons, aparentemente não permitem o uso de um propagador modificado, entretanto, o uso destes permite uma melhor descrição dos dados experimentais, assim como no caso dos processos difrativos.|http://hdl.handle.net/10183/3721
Análise do padrão W3C / XForms 1.0|2004|Open Access|Dissertação|Banco : Dados;XML (Linguagem de marcação)|por|Internet. Desde a versão 2.0 do HTML, entretanto, pouco foi melhorado no modelo de formulários proposto. Ao mesmo tempo, as necessidades dos desenvolvedores e os requisitos dos usuários cresceram dramaticamente. O W3C apontou uma resposta para as necessidades levantadas, o padrão XForms. O padrão XForms visa substituir o modelo de formulários definido no HTML por um modelo que separa o propósito da apresentação, adicionando, desta forma, a característica de independência de plataforma. A proposta deste trabalho é analisar o padrão XForms em relação à utilização de formulários HTML tradicionais, e à outras soluções existentes para automação de formulários na Internet, utilizando para isto uma aplicação piloto que procure utilizar alguns dos principais recursos disponíveis no padrão. Os pontos fortes, pontos fracos, dificuldades e lições aprendidas capturadas durante o desenvolvimento da aplicação piloto formam uma base de conhecimento apresentada neste trabalho.|http://hdl.handle.net/10183/3722
Ambiente visual para programação distribuída em java|2001|Open Access|Dissertação|Linguagens : Programacao;Java (Linguagem de programação);Programacao visual;Programacao distribuida;Objetos distribuidos|por|Em vista da maior complexidade da programação paralela e distribuída em relação à programação de ambientes centralizados, novas ferramentas vêm sendo construídas com o objetivo de auxiliar o programador desses ambientes a desempenhar sua tarefa de formas mais eficazes e produtivas. Uma das ferramentas que há algum tempo tem sido usada na programação centralizada e aos poucos está sendo empregada também na programação concorrente é a programação visual. A programação visual se vale da presença de elementos visuais na especificação dos programas como peças chaves do processo de desenvolvimento de software. No caso específico da programação concorrente, a programação visual é especialmente útil pela capacidade que os gráficos têm de representar de forma mais adequada estruturas bidimensionais. Um programa concorrente, por relacionar no espaço diversos elementos com seus próprios fluxos de execução, faz surgir duas dimensões de análise que são mais difíceis de serem observadas através de programas textuais. Atualmente existem ferramentas de programação visual paralela e distribuída, mas a ênfase é dada na programação paralela, sem muita atenção a aplicações de sistemas abertos ou cliente-servidor. Além disso, tais ferramentas sofrem da falta de apoio à engenharia do software.  Considerando essas deficiências, este trabalho apresenta uma ferramenta de programação visual para o desenvolvimento de aplicações compostas por objetos distribuídos que ofereça também a possibilidade de aplicar os principais conceitos da engenharia de software, como reutilização e orientação a objeto. Nesta ferramenta, o programador especifica de maneira visual a estrutura do seu programa, insere o código textual para a lógica da aplicação e o ambiente se encarrega do tratamento da distribuição e da comunicação de mais baixo nível. A aplicação é representada como um grafo dirigido, onde os nodos representam os objetos distribuídos e os arcos indicam os relacionamentos existentes entre esses objetos. A especificação dos programas é modular, baseando-se na reunião de componentes reutilizáveis, o que torna o sistema altamente configurável e extensível. Tanto a implementação da ferramenta quanto o código das aplicações geradas usam a linguagem de programação Java. A linguagem de programação visual projetada não especifica detalhes a respeito de como irá funcionar a comunicação e distribuição dos objetos. Portanto, foram implementados componentes para comunicação e outros recursos de programação distribuída, como locks e dados globais para serem usados nas aplicações. Para validar os principais objetivos da ferramenta, foram implementados alguns exemplos de aplicações distribuídas, como um pequeno sistema de bate-papo.|http://hdl.handle.net/10183/3723
Aplicando o conhecimento sobre os aspectos estruturais da organização no processo de modelagem de workflow|2002|Open Access|Dissertação|Sistemas : Informação;Modelagem : Workflow;Estrutura organizacional|por|Uma organização é um arranjo sistemático composto de duas ou mais pessoas que compartilham um objetivo comum. A estrutura organizacional envolve um conjunto de aspectos ou parâmetros estruturais, os quais são freqüentemente conhecidos através da análise de documentos de planejamento existentes na organização, além de entrevistas com os funcionários e com a direção. Todavia, nem sempre a organização apresenta estas fontes de informação, dificultando o seu entendimento. Autores e profissionais da administração argumentam que a estrutura de uma organização deve ser delineada em função das necessidades dos seus processos de negócio e não vice-versa. Seguindo esta linha de raciocínio, pode-se, concluir que a estrutura dos processos de negócio está refletida na estrutura organizacional. Um processo de negócio é um conjunto de um ou mais procedimentos ou atividades relacionadas as quais, coletivamente, realizam um objetivo de negócio no contexto de uma estrutura organizacional. As organizações modernas apresentam demandas relacionadas à automação de seus processos de negócio devido à alta complexidade dos mesmos e a necessidade de maior eficiência na execução. Por este motivo é crescente a difusão de sistemas baseados em tecnologias de informação capazes de proporcionar uma melhor documentação, padronização e coordenação dos processos de negócio. Neste contexto, a tecnologia de workflow tem se mostrado bastante eficaz, principalmente, para a automatização dos processos de negócio.  Contudo, por ser uma tecnologia emergente e em evolução, workflow apresenta algumas limitações. Uma das principais é a ausência de técnicas que garantam correção e eficiência ao projeto de workflow nas fases de análise de requisitos e modelagem. Nestas fases, os projetistas precisam adquirir conhecimento sobre a organização e seus processos de negócio. O entendimento da organização pode ser dificultado devido à ausência de documentos de planejamento e a problemas de conflitos de linguagem e resistências culturais que podem surgir nas entrevistas. Este trabalho tem por objetivo investigar as relações entre diferentes tipos de estrutura organizacional e (sub)processos de workflow específicos. Caso existentes, tais relações podem tanto facilitar o Projeto de workflow a partir do conhecimento da estrutura organizacional, como, também, permitir o entendimento da organização a partir de processos de workflow já existentes.|http://hdl.handle.net/10183/3740
Escalonamento de um Job Shop : análise de um algoritmo com regras heurísticas|2001|Open Access|Dissertação|Administração : Produção;Sistemas : Manufatura|por|O presente trabalho visa definir um modelo de alocação dos recursos da produção para centros de trabalho em sistemas baseados em job shop, usando a abordagem heurística para garantir uma boa alocação dos recursos. São levados em conta a complexidade de um ambiente de produção, seus aspectos temporais e os modelos de Job Shop Scheduling atualmente em uso. Com isso são examinados os aspectos conceituais deste ambiente e proposto um modelo de alocação de recursos para auxiliar no planejamento operacional do mesmo. Pode-se definir os recursos como todos os elementos necessários à execução das diversas atividades de um processo produtivo, tais como equipamentos, máquinas, mão-de-obra, etc. Por sua vez, os recursos são limitados por natureza, quanto à quantidade de unidades disponíveis, às suas funcionalidades e à capacidade produtiva. O processo de alocação dos recursos pressupõe a designação dos recursos mais satisfatórios para a execução de cada uma das atividades que fazem parte de um projeto. O modelo proposto é baseado no uso de heurísticas para resolver o escalonamento nos centros de trabalho, também chamados de células de produção, usando restrições e regras entre as ordens de fabricação (peças) e as máquinas, para encontrar uma solução satisfatória ao problema. O resultado final é uma ferramenta de apoio à decisão no processo de manufatura, permitindo a visualização do melhor escalonamento de produção, visando a redução do ciclo e setup de produção no processo, com base nas informações locais do ambiente fabril. O sistema está implementado numa empresa de componentes hidráulicos, inicialmente no centro de trabalho de corte, composto por quatro máquinas que realizam o corte de diversos tipos de matérias-primas.|http://hdl.handle.net/10183/3741
Interface para um sistema gerenciador de transações longas de banco de dados|2003|Open Access|Dissertação|Banco : Dados;Interface gráfica;Transacoes longas|por|Existe uma certa gama de aplicações que não pode ser implementada através do modelo convencional de transações, são aplicações que tem um tempo de duração mais longo do que aquelas convencionalmente modeladas. Em uma transação Atômica, ou todo o trabalho é realizado por completo ou nada é feito, mas, quando se trata de atividades de longa duração, isto pode significar a perda de trabalho executado durante horas ou, até mesmo, dias. Pelo mesmo motivo, transações longas não devem executar isoladamente, porque isto impede que outras transações tenham acesso aos dados sendo manipulados. No âmbito do projeto TRANSCOOP, vêm sendo realizados vários estudos sobre modelos de transações não convencionais. Dentre eles, encontra-se o Modelo de Contratos, que prevê um mecanismo de controle seguro para gerenciar aplicações distribuídas que apresentam atividades de longa duração. Para experimentar e avaliar as idéias inseridas neste modelo está sendo desenvolvido um protótipo. Este sistema é provido de uma interface gráfica interativa, baseada em Manipulação Direta, e suporta a definição de transações longas de banco de dados de acordo com o Modelo de Contratos. O objetivo deste trabalho é descrever a arquitetura de um protótipo para o Modelo de Contratos, definindo a função de cada um de seus módulos, mais especificamente o módulo Interface, e a comunicação entre eles. Para a definição de uma interface adequada foram considerados aspectos de outras áreas da ciência, pois a área de interfaces homemmáquina é multidisciplinar.|http://hdl.handle.net/10183/3768
Avaliação de técnicas de interação egocêntricas em ambientes virtuais|2002|Open Access|Dissertação|Computação gráfica;Realidade virtual;3D|por|Navegação, seleção e manipulação de objetos em ambientes virtuais são baseadas em métodos (técnicas de interação) associados a dispositivos convencionais ou especiais. Existem várias técnicas e dispositivos à disposição dos que desenvolvem aplicações com tais ambientes, sendo a escolha da técnica mais adequada uma tarefa difícil. Neste trabalho, são apresentadas diversas técnicas de interação existentes, suas metáforas e taxonomias, para realizar tarefas de seleção e manipulação de objetos e navegação do usuário pelo AV. A metodologia adotada nos experimentos realizados, visando a avaliar técnicas de interação em AVs imersivos foi composta por critérios de avaliação de interfaces de IHC (interface humano-computador), com critérios utilizados por Bowman, Mine e Poupyrev em seus experimentos com interfaces 3D. Também são apresentados alguns experimentos realizados, tendo vista avaliarem-se técnicas de interação na realização de tarefas de navegação, seleção e manipulação de objetos em ambientes virtuais imersivos.  O objetivo deste trabalho foi a realização de experimentos para avaliar técnicas de interação para operações de seleção, manipulação e navegação em ambientes virtuais. Foram realizadas duas experiências: a primeira é uma avaliação de técnicas de seleção e manipulação em um ambiente semi-imersivo similar a um jogo de xadrez. As técnicas avaliadas são a mão virtual, associada a um mouse comum e a uma luva; e a ray-casting, associada à luva. Observou-se que o desempenho dos sujeitos é superior, quando a técnica utilizada é a mão virtual associada ao mouse comum. A segunda experiência apresenta uma avaliação experimental de dispositivos de RV em tarefas de navegação. Foi criado um ambiente, contendo um edifício com um elevador panorâmico e corredores de entrada e saída. Os testes foram realizados num ambiente imersivo, com capacete e luva, e, também em um ambiente semi-imersivo, com mouse comum e óculos com lentes de cristal líquido. Nesse segundo experimento, a preferência dos usuários pelos equipamentos utilizados foram o capacete e a luva. Observou-se que a existência ou não de objetos familiares no trajeto percorrido pelo usuário, não afeta a quantidade de colisões do sujeito com as mesmas.|http://hdl.handle.net/10183/3769
Manipulação simultânea de objetos em ambientes virtuais imersivos|2002|Open Access|Tese|Computação gráfica;Realidade virtual|por|Este trabalho trata do suporte à interação simultânea de mais de um usuário, sobre um mesmo objeto, em Ambientes Virtuais Colaborativos. Este suporte é obtido através do conceito de Metáfora Colaborativa e da arquitetura que materializa este conceito. Primeiramente, abordam-se os aspectos referentes às formas de interação usadas em ambientes virtuais imersivos monousuários. São apresentadas algumas técnicas interativas consideradas relevantes ao desenvolvimento do trabalho. No que diz respeito aos Ambientes Virtuais Colaborativos propriamente ditos apresentam-se suas principais características incluindo as semelhanças com os ambientes tradicionais de suporte computadorizado ao trabalho cooperativo. São abordados tanto aspectos referentes às facilidades providas por estes ambientes, quanto ao seu projeto e arquitetura. As principais abordagens existentes para suportar a interação entre usuários em ambientes virtuais colaborativos são apresentadas e é feita uma análise da execução de tarefas colaborativas em ambientes imersivos apontando-se as principais dificuldades existentes nos sistemas atuais.  A partir deste embasamento, faz-se o desenvolvimento da chamada Metáfora Colaborativa, que se constitui em um conjunto de regras que permitem combinar técnicas interativas comumente usadas na interação individual em ambientes virtuais imersivos. Esta combinação é baseada na definição de quais graus de liberdade cada usuário irá controlar durante a manipulação simultânea de um objeto. Para possibilitar a combinação das técnicas individuais na implementação de ambientes virtuais colaborativos, foi definida uma arquitetura que prevê sistemas de controle local sobre cada ambiente e a combinação dos comandos aplicados sobre os objetos. Com o objetivo de avaliar a Metáfora Colaborativa, foram modeladas e implementadas diversas técnicas de interação colaborativa e sua eficiência foi aferida através de tarefas. Foi definido um protocolo de testes aplicado a trinta duplas de usuários, comparando seu desempenho individual e colaborativo na execução das tarefas.|http://hdl.handle.net/10183/3770
Estudo de cicloestratigrafia nos depósitos eopermianos do Grupo Itararé, Bacia do Paraná, nos Estados de Santa Catarina e Rio Grande do Sul, baseado em dados de testemunho e de perfis de raios gama|2001|Open Access|Dissertação|Cicloestratigrafia;Grupo itarare;Ritmito;Eopermiano;Paraná, Bacia do|por|O final da Glaciação Neopaleozóica está representado hoje no registro sedimentar da Bacia do Paraná pelas rochas do Grupo Itararé. No Estado do Rio Grande do Sul e no sudeste do Estado de Santa Catarina seus depósitos possuem idade eopermiana, datados desde o Asseliano até o Artinskiano. A partir de dados de testemunhos e de perfis de raios gama de dois poços, um em Santa Catarina (7-RL-04- SC) e outro no Rio Grande do Sul (IB-93-RS), perfurados para pesquisa de carvão pela CPRM (Companhia de Pesquisa de Recursos Minerais), foram feitas análises cicloestratigráficas com o intuito de determinar a existência e a natureza da possível ciclicidade induzida por fenômenos astronômicos presente nesses sedimentos glaciais (basicamente folhelhos e ritmitos). A distância entre as locações originais dos poços (cerca de 380 km) possibilitou testar a influência da indução astronômica em localidades distintas da bacia. Dois métodos de amostragem foram utilizados no estudo, de acordo com a escala dos dados e com a possível indução: os perfis de raios gama (191 m para o 7-RL-04-SC e 71 m para o IB-93-RS) foram digitalizados e amostrados em intervalos de 1 cm, com o intuito de testar a presença de indução pelos ciclos orbitais na escala de 20 mil a 400 mil anos, ou outros fenômenos indutores na escala de 3 mil a 10 mil anos, e os testemunhos foram escaneados nos intervalos com ritmitos, (1,2 m para o 7-RL-04-SC e 38 cm para o IB-93-RS) e transformados em dados em escala de cinza equiespaçados (0,2538 mm), objetivando a busca por ciclos anuais a milenares  A análise harmônica pela transformada rápida de Fourier demonstrou a presença de ciclicidade em ambas as escalas: ciclos orbitais, com períodos de cerca de 17 mil a 100 mil anos, foram caracterizados em perfil e ciclos solares, com períodos de cerca de 22 a 1000 anos, foram evidenciados nos testemunhos. Os tempos de acumulação calculados para o poço 7-RL-04-SC nas duas escalas mostraram um alto grau de correlação (cerca de 9400 anos para o intervalo escaneado e aproximadamente 12600 para o mesmo intervalo nos dados do perfil), comprovando a eficiência dos métodos de obtenção dos dados e a utilidade da cicloestratigrafia como ferramenta de análise e refinamento cronoestratigráfico. Quanto às espessas seções de ritmitos, características do Grupo Itararé e presentes nos testemunhos, estas têm sido freqüentemente denominadas de varvitos ou referenciadas como semelhantes a varvitos na literatura.  Porém os resultados mostraram que cada par de ritmitos foi depositado em períodos de vinte e dois anos, relacionados aos ciclos solares de Hale. A análise permitiu ainda o estudo das relações existentes entre várias variáveis, como a taxa e o tempo de acumulação, e a definição, na seção do poço 7-RL-04-SC, de seqüências deposicionais de terceira e de quarta ordem. Essas últimas são associadas à indução pelos ciclos orbitais de excentricidade e comparáveis aos períodos glaciais do Pleistoceno, sendo que as taxas de acumulação calculadas para os dados do poço, variando entre 5,2 a 9,3 cm/ka, são muito similares às taxas de acumulação do Pleistoceno. A análise também mostrou que a seção completa do Grupo Itararé no poço IB-93-RS corresponde apenas à cerca de meio ciclo de precessão (12342 anos). Como os dois fenômenos de indução astronômica detectados, os ciclos solares e os orbitais, afetam o clima de maneira global, certamente influenciaram a sedimentação em outros pontos da bacia.|http://hdl.handle.net/10183/3779
Defeitos responsáveis pela isolação de GaAs irradiado com prótons|2003|Open Access|Dissertação|Gálio;Arsênio;Defeitos;Implantação de íons;Dopagem de semicondutores;Carbono;Magnésio;Isolantes;Recozimento|por|A Espectroscopia de Transientes de Níveis Profundos (DLTS – Deep Level Transient Spectroscopy) foi, detalhadamente, descrita e analisada. O processo de isolação por implantação em GaAs foi estudado. Sua dependência com a sub-rede, do As ou do Ga, em que o dopante é ativado foi investigada para material tipo-p. Semelhantes doses de implantação de prótons foram necessárias para se tornar semi-isolantes camadas de GaAs dopadas com C ou com Mg possuindo a mesma concentração de pico de lacunas livres. A estabilidade térmica da isolação nestas amostras foi medida. Diferenças no comportamento de recozimento destas apontaram a formação, provavelmente durante a referida etapa térmica, de uma estrutura diferente de defeitos em cada caso. Medidas de DLTS foram realizadas em amostras de GaAs tipo-n e tipo-p implantadas com prótons de 600 keV. A estrutura de picos observada apresentou, além de boa parte dos defeitos introduzidos para o caso de irradiação com elétrons, defeitos mais complexos. Um novo nível, com energia superior em ~0,64 eV ao valor correspondente ao topo da banda de valência, foi identificado nos espectros medidos em material tipo-p. A variação da concentração dos centros de captura introduzidos com diferentes etapas de recozimento foi estudada e comparada com o comportamento previamente observado para a resistência de folha em camadas de GaAs implantadas com prótons. Simulações foram feitas, indicando que a interpretação adotada anteriormente, associando o processo de isolação diretamente à formação de defeitos relacionados a anti-sítios, pode não estar completa.|http://hdl.handle.net/10183/3784
Transporte atômico e incorporação de oxigênio em filmes de HfSiO e HfSiON depositados sobre Si|2004|Open Access|Dissertação|Estabilidade térmica;Transporte atomico;Filmes finos dieletricos;Silício;Tratamento térmico;Oxigênio;Feixes de íons;Espectros de raios-x de fotoeletrons;Nitrogênio;Difusão|por|Esse trabalho estuda a estabilidade térmica e o transporte atômico em filmes dielétricos de HfSiO e HfSiON depositados por sputtering reativo sobre c-Si. Esses materiais possuem alta constante dielétrica (high- ) e são candidatos a substituir o óxido de silício como dielétrico de porta em dispositivos do tipo transistor de efeito de campo metal-óxido-semicondutor (MOSFETs). Esses filmes foram submetidos a diferentes seqüências de tratamentos térmicos em atmosferas inerte e de O2, e foram caracterizados através de análises de feixe de íons e espectroscopia de fotoelétrons induzidos por raios-X (XPS). Nesse estudo foi observado que a incorporação de oxigênio se dá através da troca com N ou O previamente existente nos filmes. Em filmes de aproximadamente 50 nm de espessura foi observado que a presença do N limita a difusão de oxigênio de forma que a frente de incorporação avança em direção ao interior do filme com o aumento do tempo de tratamento, enquanto nos filmes de HfSiO/Si o oxigênio é incorporado ao longo de todo o filme, mesmo para o tempo mais curto de tratamento. Diferentemente de outros materiais high- estudados, não foi possível observar migração de Si do substrato em direção a superfície dos filmes de HfSiON/Si com aproximadamente 2,5 nm de espessura. Os resultados obtidos nesse trabalho mostram que filmes de HfSiON/Si são mais estáveis termicamente quando comparados com outros filmes dielétricos depositados sobre Si anteriormente estudados, mas antes que ele se torne o dielético de porta é ainda necessário que se controle a difusão de N em direção ao substrato como observado nesse trabalho.|http://hdl.handle.net/10183/3785
Produção de novos materiais carbonáceos por altas pressões|2004|Open Access|Dissertação|Carbono;Diamante;Altas pressões;Altas temperaturas;Estabilidade térmica;Espectroscopia Raman;Cinética;Cristalização|por|Neste trabalho foram estudadas as condições de estabilidade e de eventuais transformações de fases em diferentes materiais carbonáceos processados em altas pressões e altas temperaturas. Além disso, foi explorada a possibilidade de produção de compactos de diamante policristalino sem o uso de fases metálicas como ligante. Diferentes materiais de partida (pó de diamante, açúcar, mistura de pó de diamante e açúcar -10 % em peso, e mistura de pó de diamante e negro de fumo - 10% em peso) foram processados a 7,7 GPa, durante 15 min a diferentes temperaturas (900°C, 1200°C, 1500°C e 1700°C), mas sempre na região de estabilidade termodinâmica do diamante. As amostras foram analisadas por espectroscopia Raman e difração de raios X. As amostras de diamante compactaram bem a altas temperaturas sem mostrar grafitização, mas mostraram indícios de formação de uma fase desordenada de C com hibridização sp3. Os compactos de negro de fumo e diamante são extremamente frágeis e foi identificada a formação de estruturas grafíticas.  A amostra de açúcar sofreu um grafitização acentuada a temperaturas elevadas, que levou a obtenção de grafite bem cristalizado. Os compactos obtidos com a mistura de açúcar e diamante são bastante homogêneos e possuem boa resistência mecânica. Para a amostra tratada a 1700°C, além de ser obtida uma fase diamante muito bem cristalizada, também são observados indícios de formação de uma fase desordenada de C com hibridização sp3. A origem do comportamento observado para as amostras com açúcar deve estar ligada à liberação de H2O associada à pirólise da sacarose, o que parece acelerar significativamente a cinética de cristalização de fases do carbono. A possível fase desordenada, obtida para amostras com pó de diamante processadas em altas temperaturas, pode desempenhar um papel importante como fase ligante para produção de compactos de diamante policristalino.|http://hdl.handle.net/10183/3788
Física de Pomeron a altas energias|2002|Open Access|Tese|Pomerons;Cromodinâmica quântica;Espalhamento;Altas energias;Dinâmica;Fenomenologia;Teoria de campos de Reggeons;Difração;Interacoes hadron-hadron;Teoria de perturbacao;Interações elétron-próton|por|Neste trabalho investigamos a física de Pomeron, tanto em sua formulação perturbativa dada pela Cromodinâmica Quântica como o aspecto não-perturbativo dado pela teoria de Regge. Estes formalismos teóricos levam em consideração o fato que o Pomeron porta os números quânticos do vácuo e deve dominar o espalhamento entre partículas em altas energias. Partindo do formalismo de dipolos de cor, estudamos a interação perturbativa usual corrigida por efeitos de unitariedade em altas energias para os processos DIS e Drell- Yan, determinando a importância destas correções para ambos processos. Referente ao Pomeron não-perturbativo, investigamos os aspectos do modelo de múltiplos espalhamentos, o qual também contribui também para a unitariedade no setor suave. Dado os resultados similares entre modelos QCD e de Regge na descrição de DIS difrativo, propomos um novo observável, o coeficiente angular difrativo da função de estrutura difrativa, que permitirá discriminar dinâmicas subjacentes ao processo. Finalmente estudamos a fenomenologia associada à truncagem da série assintótica advinda do formalismo BFKL, e a utilizamos para descrever os principais observáveis em colisões hadrônicas e elétron-próton.|http://hdl.handle.net/10183/3790
Uma Abordagem de escalonamento adaptativo no ambiente Real-Time CORBA|2002|Open Access|Dissertação|Tempo real : Computadores;Sistemas : Tempo real;Sistemas distribuídos;Corba;Escalonamento adaptativo|por|CORBA vem se tornando o middleware padrão no desenvolvimento de aplicações distribuídas, tornando-as independentes de plataforma e linguagem. Ele tem sido utilizado também em aplicações de tempo real através de sua extensão para tempo real, o RT-CORBA. Apesar desta extensão ter conseguido reduzir vários dos problemas do CORBA no que se refere ao não-determinismo e falta de garantias temporais, ainda há muito estudo na área de mecanismos de escalonamento utilizados. Assim, este trabalho tem por objetivo apresentar uma proposta de escalonamento adaptativo no ambiente Real-Time CORBA. Nesta proposta o período das tarefas é controlado, variando dentro de uma faixa pré-estabelecida com o propósito de reduzir o atraso médio das tarefas da aplicação.|http://hdl.handle.net/10183/3801
Personalização de interfaces web para sites institucionais com base em perfis de usuários|2002|Open Access|Dissertação|Informática : Artes gráficas;Home page;Interface homem-maquina;World Wide Web (WWW)|por|A crescente utilização de computadores e a popularização da Internet contribuem cada vez mais para que usuários não especialistas em informática tenham acesso a sistemas computacionais. Tais usuários possuem diferentes habilidades, formações e idades que devem ser levadas em consideração no projeto de uma boa interface. No entanto, no projeto de interfaces web é muito difícil identificar essas características do público alvo, tendo em vista que ele é bastante diversificado. Assim, construir uma interface que atenda os requisitos de todos os usuários de um site não é uma tarefa fácil. Se o projetista tivesse uma maneira de conhecer o seu público, grande parte dos problemas de interação entre usuário e interface poderiam ser resolvidos. Assim, a fim de auxiliar na tarefa de conhecer o público que um site possui, várias técnicas estão disponíveis. Independente das características dos usuários de sites web, a sua principal necessidade consiste em encontrar a informação desejada de modo fácil e rápido. Ainda que seja possível identificar o comportamento de todos os usuários em um site, torna-se difícil disponibilizar informações de forma clara e simples para todos. Para isto, um site adaptativo, que se ajusta automaticamente a cada usuário de acordo com seus padrões de comportamento, é muito útil.  Sites adaptativos são desenvolvidos com base em técnicas que auxiliam o projetista na tarefa de personalizar páginas web e, por este motivo, são chamadas de técnicas de personalização. O objetivo desta dissertação é investigar, desenvolver e experimentar uma técnica de personalização de interfaces web voltada especificamente para sites institucionais. Este tipo de site é voltado à divulgação de produtos e serviços de uma instituição e, normalmente, é composto por um grande número de páginas estruturadas de acordo com a estrutura organizacional da instituição. Essa característica permite o agrupamento de usuários por funções ou cargos. A técnica proposta neste trabalho é baseada em perfis de usuários, onde a personalização ocorre em dois níveis: nível de grupos de usuários e nível de usuários individuais, denominados respectivamente de macro perfil e micro perfil. Um estudo de caso realizado na intranet da Agência Nacional de Telecomunicações – ANATEL foi utilizado para aprimorar e validar a técnica. Foi desenvolvido um protótipo de site institucional personalizado, o qual foi disponibilizado, utilizado e avaliado por um grupo de usuários previamente selecionados. O processo de construção do protótipo assim como da avaliação e seus resultados são também descritos neste trabalho.|http://hdl.handle.net/10183/3802
Um Modelo de replicação em ambientes que suportam mobilidade|2001|Open Access|Dissertação|Programação;Sistemas distribuídos;Mobilidade : Sistemas distribuidos;Replicacao : Objetos|por|Os estudos sobre mobilidade intensificaram-se com o uso em grande escala da Internet, pois esta trouxe a possibilidade de explorar mobilidade através de redes heterogêneas, conectadas por diferentes links de comunicação e distantes umas das outras. A replicação de componentes em sistemas distribuídos normalmente é utilizada para torná-los mais confiáveis e seguros ou para aumentar o desempenho da aplicação, uma vez que acessos remotos podem ser evitados através da localidade da réplica. Em qualquer um dos casos, a replicação implica a manutenção na consistência entre as múltiplas cópias, isto é, é preciso garantir que as cópias estejam com seus estados consistentes em um determinado momento. Em sistemas que permitem mobilidade e replicação, a principal preocupação é com a consistência e o gerenciamento das réplicas do objeto móvel. Isto é, dependendo da técnica de replicação utilizada, como gerenciar um objeto e suas réplicas se estes podem mudar sua localização? Como garantir um bom desempenho do sistema? Estas perguntas esta proposta procura responder. Este trabalho apresenta um modelo de replicação em ambientes de objetos distribuídos que permitem mobilidade chamado ReMMoS - Replication Model in Mobility Systems. O objetivo deste modelo é prover um ambiente de execução para suporte ao desenvolvimento de aplicações envolvendo mobilidade explicita e replicação implicita. Assim, o programador não necessita preocupar-se com o gerenciamento e a consistencia das cópias.|http://hdl.handle.net/10183/3803
Integração materializada na web : um estudo de caso|2002|Open Access|Dissertação|Armazenamento : Dados;Recuperacao : Informacao;World Wide Web (WWW);Integração : Dados;Dados semi-estruturados|por|A World Wide Web em poucos anos de existência se tornou uma importante e essencial fonte de informação e a localização e recuperação de informações na Internet passou a ser um grande problema a ser resolvido. Isto porque a falta de padronização e estrutura adequada para representação dos dados, que é resultado da liberdade de criação e manipulação dos documentos, compromete a eficácia dos modelos de recuperação de informação tradicionais. Muitos modelos foram então desenvolvidos para melhorar o desempenho dos sistemas de recuperação de informação. Com o passar dos anos surge assim uma nova área de pesquisa a extração de dados da web que, ao contrário dos sistemas de recuperação, extrai informações dos documentos relevantes e não documentos relevantes de conjunto de documentos.  Tais estudos viabilizaram a integração de informações de documentos distribuídos e heterogêneos, que foram baseados nos mesmos modelos aplicados a banco de dados distribuídos. Neste trabalho é apresentado um estudo que tem como objetivo materializar informações contidas em documentos HTML de modo que se possa melhorar o desempenho das consultas em relação ao tempo de execução quanto à qualidade dos resultados obtidos. Para isso são estudados o ambiente web e as características dos dados contidos neste ambiente, como por exemplo, a distribuição e a heterogeneidade, aspectos relacionados à maneira pela qual as informações estão disponibilizadas e como estas podem ser recuperadas e extraídas através de regras sintáticas. Finalizando o estudo são apresentados vários tipos de classificação para os modelos de integração de dados e é monstrado em detalhes um estudo de caso, que tem como objetivo demonstrar a aplicação das técnicas apresentadas ao longo desta pesquisa.|http://hdl.handle.net/10183/3804
LAR : laboratório de automação residencial para análise comparativa entre Jini e UPnP|2003|Open Access|Dissertação|Automação industrial;Automação predial e residencial|por|A Automação Residencial é uma área em crescimento no mercado mundial. À medida que os avanços tecnológicos são incorporados aos dispositivos pertencentes ao ambiente residencial, e que estes se tornam mais disseminados, surgem necessidades para fabricantes e usuários. Para os fabricantes, é desejável maior capacidade de conexão e intercâmbio de dados entre os dispositivos e, conseqüentemente, padronização. Para os usuários, é desejável maior facilidade na utilização dos dispositivos e, principalmente, na configuração dos mesmos em ambientes dinâmicos, de forma a não requerer interação do usuário para a conexão e desconexão, característica das redes espontâneas. Grandes empresas têm unido esforços no desenvolvimento de novos padrões e uma das tendências observadas é o isolamento destas novas necessidades numa camada entre a aplicação e o sistema operacional, denominada middleware. Este trabalho compara duas tecnologias que se enquadram nos critérios acima: Jini, da Sun Microsystems e Universal Plug and Play (UPnP), de um consórcio de empresas liderado pela Microsoft. O estudo é feito através do desenvolvimento de um protótipo, da definição de um conjunto de métricas qualitativas e quantitativas, e da implementação de um conjunto de workloads para a análise comparativa entre elas. Os resultados obtidos são apresentados e analisados, contribuindo com subsídios para os interessados em conhecer melhor ou optar por uma delas. Por fim, são registradas algumas necessidades observadas que poderão servir como base para trabalhos futuros.|http://hdl.handle.net/10183/3813
Avaliação de abordagens para captura de informações da aplicação|2002|Open Access|Dissertação|Confiabilidade : Computadores;Tolerancia : Falhas;Sistemas distribuídos;Linux|por|Numerosas pesquisas estão introduzindo o conceito de grupo em padrões abertos para programação distribuída. Nestas, o suporte a grupo de objetos por meio de middlewares, apresentam diferentes abordagens de interligação com a aplicação. Segundo princípios defendidos na tese de Felber, essas abordagens vão ao encontro do objetivo de facilitar o desenvolvimento e proporcionar confiabilidade e desempenho. Neste contexto, localizou-se três enfoques básicos para a interligação com a aplicação, denominados integração, serviço, e interceptação, que utilizam a captura de mensagens para obtenção de informações ou como meio para adicionar novas funcionalidades às aplicações. A utilização dessas informações pode auxiliar no ajuste de parâmetros funcionais de serviços relacionados, na escolha de mecanismos, influindo em aspectos como, desempenho e segurança. Ao longo do estudo dessas abordagens, sentiu-se a necessidade de estudar detalhes e testar aspectos de implementação, suas premissas de uso e as conseqüências advindas da incorporação de seus mecanismos junto à aplicação. Este trabalho visa apresentar uma análise do comportamento das referidas abordagens por meio da implementação de protótipos, possibilitando assim, investigar problemas relacionados ao emprego da técnica e suas conseqüências quando integradas à aplicação. Os objetivos específicos reúnem a busca de informações qualitativas, tais como: modularidade, transparência, facilidade de uso e portabilidade; e informações quantitativas, fundamentalmente traduzidas pelo grau de interferência no desempenho da aplicação.  O desenvolvimento dos protótipos teve como início a busca por um ambiente que ofereceria suporte as condições necessárias para a implementação das diferentes abordagens. Percebeu-se que definir os mecanismos diretamente sobre uma linguagem de programação, como C ou C++, não era viável. As versões padrões dessas linguagens não oferecem mecanismos capazes de suportar algumas características de implementação como, por exemplo, a captura de mensagens na abordagem de interceptação. A possibilidade é introduzida apenas por extensões dessas linguagens. Assim, a investigação de um ambiente de implementação voltou-se para mecanismos disponíveis em sistemas operacionais. A opção pela utilização do Linux visou atender alguns requisitos importantes para o desenvolvimento dos protótipos tais como: facilidade de instalação, boa documentação e código aberto.  Este último é um ponto essencial, pois a construção de parte dos protótipos explora a programação em nível do sistema operacional. A linguagem de programação C foi escolhida como base para a implementação, já que as diferentes abordagens exploram tanto o nível do kernel como o nível do usuário, e é compatível com o Linux. A etapa de desenvolvimento dos protótipos possibilitou a coleta de informações sobre aspectos qualitativos. As demais informações que fazem parte do perfil levantado por este trabalho sobre as abordagens, foram obtidas através da utilização dos protótipos em experimentos com duas aplicações distribuídas denominadas de “Ping-Pong” e “Escolha de Líderes”, que têm como característica geral à troca de mensagens, utilizando comunicação através de sockets. A realização de medidas em múltiplas execuções, avaliadas após o tratamento estatístico necessário, permitiu definir um perfil das diferentes abordagens.|http://hdl.handle.net/10183/3815
Linguagem de consulta temporal : definição e implementação|2002|Open Access|Dissertação|Banco : Dados;Banco : Dados temporais;Linguagens : Consulta;OASIS|por|Até hoje, não existem implementações de SGBDs Temporais disponíveis no mercado de software. A tradução de linguagens de consulta temporais para o padrão SQL é uma alternativa para implementação de sistemas temporais com base em SGBDs comerciais, os quais não possuem linguagem e estrutura de dados temporais. OASIS (Open and Active Specification of Information Systems) é uma linguagem que serve como repositório de alto nível para especificação formal orientada a objetos e geração automática de software, em diversas linguagens, através da ferramenta CASE OO-Method. As aplicações geradas desta forma utilizam, como meio de persistˆencia de objetos, SGBDs comerciais baseados na abordagem relacional. A linguagem OASIS foi estendida com aspectos temporais. A extensão de OASIS com aspectos temporais requer a especificação de um modelo de dados e de uma linguagem de consulta temporais que possam ser utilizados em SGBDs convencionais. Há duas abordagens para resolver o problema. A primeira baseia-se em extensões da linguagem e/ou do modelo de dados de modo que o modelo não-temporal é preservado. A segunda, abordagem de generalização temporal, é mais radical e não preserva o modelo não-temporal. A linguagem ATSQL2 fornece recursos adequados aos conceitos encontrados na abordagem de generalização temporal. Neste trabalho utiliza-se os conceitos de generalização temporal preservando o modelo não-temporal.  A presente dissertação tem por finalidade propor um modelo de dados para suporte à extensão temporal da linguagem OASIS, bem como estender a linguagem ATSQL2 para facilitar as consultas a eventos temporais. O sistema de tradução da linguagem de consulta temporal para SQL é também adaptado ao modelo de dados proposto.|http://hdl.handle.net/10183/3816
Descrição e geração de ambientes para simulações com sistemas multiagentes|2003|Open Access|Dissertação|Inteligência artificial;Inteligencia artificial distribuida;Sistemas multiagentes;Simulação|por|Este trabalho situa-se na área de Sistemas Multiagente, que é uma sub-área da Inteligência Artificial Distribuída. Em particular, o problema abordado nesta dissertação é o da modelagem de ambientes, um aspecto importante na criação de simulações baseadas em sociedades de agentes cognitivos, no entanto pouco tratado na literatura da área. A principal contribuição deste trabalho é a concepção de uma linguagem, chamada ELMS, própria para a definição de ambientes multiagente, e a implementação de um protótipo de interpretador para esta linguagem. O resultado da interpretação é um processo que simula o ambiente descrito em alto nível, e é apropriado para a interação com os agentes cognitivos que irão compartilhar o ambiente. Esta linguagem foi desenvolvida no contexto do projeto MASSOC, que tem como objetivo a criação de simulações sociais com agentes cognitivos. A abordagem deste projeto dá ênfase ao uso da arquitetura BDI para agentes cognitivos, a comunicação inter-agente de alto nível (ou seja, baseada em atos de fala) e a modelagem de ambientes com a linguagem ELMS, que é proposta neste trabalho. Os ambientes e agentes que podem ser usados na criação de simulaçõpes, bem como a comunicação entre eles utilizando a ferramenta SACI, são definidos ou gerenciados a partir de uma interface gráfica, que facilita a criação e controle de simulações com a plataforma MASSOC. Além de apresentar a linguagem ELMS e seu interpretador, esta dissertação menciona ainda, como breve estudo de caso, uma simulação de aspectos sociais do crescimento urbano. Esta simulação social auxiliou na concepção e avaliação da linguagem ELMS.|http://hdl.handle.net/10183/3818
Navegação exploratória baseada em problemas de valores de contorno|2003|Open Access|Tese|Robótica;Redes neurais;Construção : Mapas|por|Este trabalho apresenta e discute uma estratégia e discute uma estratégia inédita para o problema de exploração e mapeamento de ambientes desconhecidos usandoo robô NOMAD 200. Esta estratégia tem como base a solução numéricqa de problemas de valores de contorno (PVC) e corresponde ao núcleo da arquitetura de controle do robô. Esta arquitetura é similar à arquitetura blackboard, comumente conhecida no campo da Inteligência Artificial, e é responsável pelo controle e gerenciamento das tarefas realizadas pelo robô através de um programa cleinte. Estas tarefas podem ser a exploração e o mapeamento de um ambiente desconhecido, o planejamento de caminhos baseado em um mapa previamente conhecido ou localização de um objeto no ambiente. Uma características marcante e importante é que embora estas tarefas pareçam diferentes, elas têm em comum o mesmo princípio: solução de problemas de valores de contorno. Para dar sustentabilidade a nossa proposta, a validamos através de inúmeros experimentos, realizados e simulação e diretamente no robô NOMAD 200, em diversos tipos de ambientes internos. Os ambientes testados variam desde labirintos formados por paredes ortogonais entre si até ambientes esparsos. Juntamente com isso, introduzimos ao longo do desenvolvimento desta tese uma série de melhorias que lidam com aspectos relacionados ao tempo de processamento do campo potencial oriundo do PVC e os ruídos inseridos na leitura dos sensores. Além disso, apresentamos um conjunto de idéias para trabalhos futuros.|http://hdl.handle.net/10183/3819
Um método para abordar todo o ciclo de desenvolvimento de aplicações tempo real|2003|Open Access|Tese|Sistemas : Tempo real;Orientacao : Objetos;Uml;Requisitos temporais|por|Neste trabalho apresenta-se um método de desenvolvimento integrado baseado no paradigma de orientação a objetos, que visa abordar todo o ciclo de desenvolvimento de uma aplicação tempo real. Na fase de especificação o método proposto baseia-se no uso de restrições temporais padronizadas pelo perfil da UML-TR, sendo que uma alternativa de mapeamento destas restrições para o nível de programação é apresentada. Este mapeamento serve para guiar a fase de projeto, onde utilizou-se como alvo a interface de programação orientada a objetos denominada TAFT-API, a qual foi projetada para atuar junto ao ambiente de execução desenvolvido no âmbito desta tese. Esta API é baseada na especificação padronizada para o Java-TR. Este trabalho também discute o ambiente de execução para aplicações tempo real desenvolvido. Este ambiente faz uso da política de escalonamento tolerante a falhas denominada TAFT (Time-Aware Fault- Tolerant). O presente trabalho apresenta uma estratégia eficiente para a implementação dos conceitos presentes no escalonador TAFT, que garante o atendimento a todos os deadlines mesmo em situações de sobrecarga transiente. A estratégia elaborada combina algoritmos baseados no Earliest Deadline, sendo que um escalonador de dois níveis é utilizado para suportar o escalonamento combinado das entidades envolvidas. Adicionalmente, também se apresenta uma alternativa de validação dos requisitos temporais especificados. Esta alternativa sugere o uso de uma ferramenta que permite uma análise qualitativa dos dados a partir de informações obtidas através de monitoração da aplicação. Um estudo de caso baseado em uma aplicação real é usado para demonstrar o uso da metodologia proposta.|http://hdl.handle.net/10183/3823
Transformações de ordem-desordem no sistema ixiolita-columbita-wodginita|2000|Open Access|Dissertação|Minerais;Óxidos;Transformacoes de ordem-desordem;Tratamento térmico;Difração de raios X;Espectroscopia mossbauer;Cristais;Pós;Gradiente de campo elétrico|por|O mineral columbita-tantalita (muitas vezes chamado simplesmente columbita,para Nb>Ta,ou tantalita,paraNb<Ta) é uma solução sólida do tipo AB206 (A=Fe,Co,Mn,Mg, Ni e B=Ta, Nb), pertencente ao grupo espacial Pbcn (estruturaortorrômbica).Na fase ordenada o sistema apresenta uma superestrutura do tipo a-Pb02. Relacionados com este sistema temos os minerais wodginita, com a fórmula genérica ABC20s (A=Mn, Fe2+B= Sn, Ti, Fe3+,Ta e C=Ta, Nb), e ixiolita M02 (M=Mn, Sn, Fe, Ta, Nb) que é uma subestrutura da columbita. Fenômenos de ordem-desordem envolvendo estes minerais ainda apresentam questões em aberto. Sabe-se, por exemplo, que o ordenamento dos cátions na estrutura M02 é possível se a célula unitária for aumentada: triplicando-a tem-se a estabilização da estrutura AB206 quadruplicando-a tem-se a estrutura ABC2Os. Todavia, não se sabe se a ocorrência de uma ou outra rota depende de fatores geoquímicos ou de fatores cristaloquímicos. Evolução térmica e fenômenos de ordem-desordem, investigados em uma manganotantalitanatural parcialmente ordenada (Mn,Fe)(Ta,Nb)206e em uma ferrocolumbita sintética Fe(Nb,Tah06, são relatados na presente dissertação. As amostras foram caracterizadas com o uso da difração de raios-X (DRX), através de refinamento estrutural com o programa FullProf. As propriedades químicas e magnéticas foram investigadas com o uso da EspectroscopiaMõssbauer (EM)  Tratamento térmico em ar na amostra natural parcialmente ordenada leva a diferentes resultados dependendo da forma da amostra. Para amostra em pó a transformação manganotantalita-wodginita foi observada, com pequenas contribuições de (Fe,Mn)(Nb,Ta)04 e (Nb,TahOs. Para um fragmento de cristal o tratamento térmico produz a mistura de quatro fases, sendo a maior contribuição devido ao ordenamento da fase (Mn,Fe)(Ta,Nb)206 presente no interior do cristal. (Mn,Fe)(Ta,Nbh06 presente na superficie do cristal oxida-se, como no caso observado na amostra em pó. O tratamento térmico em ar, aplicado na ferrocolumbita sintética, gerou a transformação ferrocolumbita-ixiolita.|http://hdl.handle.net/10183/3833
Estudo da difusão de Ag e Al em [alfa]-Ti utilizando as técnicas de espectrometria de retroespalhamento Rutherford e reação nuclear|2000|Open Access|Dissertação|Prata;Alumínio;Titânio;Difusão;Impurezas;Temperatura;Valência;Solubilidade;Retroespalhamento rutherford;Reacoes nucleares|por|O objetivo da presente dissertação é o estudo da difusão de Ag e Al em uma matriz de α-Ti. Sua motivação principal se origina do fato de haver na literatura resultados contraditórios sobre o comportamento difusional desses elementos. Além disso, este estudo é necessário para dar continuidade à investigação sistemática da difusão de impurezas substitucionais em α-Ti, que vem sendo realizada pelo grupo de implantação iônica, a fim de estabelecer uma relação entre tamanho, valência e solubilidade dos elementos e seus coeficientes de difusão. A dependência do coeficiente de difusão com a temperatura para ambos elementos foi estudada nos intervalos de temperatura de 823 a 1073 K para a Ag e 948 a 1073 K para o Al. No caso da Ag foi usada a técnica de RBS para determinar os perfis de concentração, a qual tem uma alta resolução em profundidade (tipicamente 10nm), o que permite a determinação das baixas difusividades esperadas no presente experimento ( D ≤ 10-7 m2/s). No caso do Al foi usada a técnica de NRA, que preenche os mesmos requisitos citados anteriormente, com uma resolução em profundidade de aproximadamente 10 Å. As medidas realizadas mostraram que, para ambos os casos, os coeficientes de difusão seguem uma representação linear de Arrhenius. Foram encontrados os seguintes parâmetros característicos de difusão: Do = (1 ± 0,75) x10-4 m2s-1 e Q = 279 ± 6 kJ/mol para a Ag e Do = (1,4 ± 1,2) x10-2 m2s-1 e Q = 326 ± 10 kJ/mol para o Al, os quais são típicos de um comportamento substitucional normal. A comparação desse trabalho com trabalhos prévios não mostra evidência de relacionamento entre tamanho, valência e solubilidade dos elementos e seus coeficientes de difusão.|http://hdl.handle.net/10183/3842
Migração de sistemas legados|2003|Open Access|Dissertação|Reengenharia;Engenharia reversa;Engenharia : Software|por|Mesmo depois de todas as novidades tecnológicas nos últimos anos, ainda existem muitos sistemas desenvolvidos com tecnologias antigas, muitas vezes ultrapassadas e obsoletas denominados sistemas legados. O problema do bug do ano 2000 funcionou como um excelente despertador para o fato de que não podemos nos esquecer do grande número de sistemas ainda em produção, e que são importantes para a empresa. Não se pode simplesmente descartar estes sistemas e é muito difícil migrar sistemas legados rapidamente para novas plataformas. Mais ainda, as regras de negócio que regem qualquer empresa são muito complexas para poderem ser modeladas e remodeladas em poucos meses e em seguida automatizadas porque a maior dificuldade em desenvolver sistemas não é escrever código nesta ou naquela linguagem, mas entender o que o sistema deve fazer. Este trabalho enfoca uma solução possível para o problema referente à migração de sistemas legados: a tradução destes sistemas legados da forma mais automatizada possível para que possam se beneficiar das novas tecnologias existentes deve ser o resultado final produzido. Assim, o objetivo desta dissertação é a investigação do problema de migração de sistemas legados e suas soluções assim como o desenvolvimento de uma ferramenta que traduz um sistema legado escrito na linguagem COBOL para PROGRESS, visando o aproveitamento do código e principalmente o aproveitamento de soluções de análise e projeto, que exigiram bastante esforço para serem elaboradas e poderiam ser reutilizadas em novos desenvolvimentos.|http://hdl.handle.net/10183/3861
Business Intelligence: comparação de ferramentas|2003|Open Access|Dissertação|Banco : Dados;Armazem : Dados;Oracle;Sql|por|Cada vez mais o tempo acaba sendo o diferencial de uma empresa para outra. As empresas, para serem bem sucedidas, precisam da informação certa, no momento certo e para as pessoas certas. Os dados outrora considerados importantes para a sobrevivência das empresas hoje precisam estar em formato de informações para serem utilizados. Essa é a função das ferramentas de “Business Intelligence”, cuja finalidade é modelar os dados para obter informações, de forma que diferencie as ações das empresas e essas consigam ser mais promissoras que as demais. “Business Intelligence” é um processo de coleta, análise e distribuição de dados para melhorar a decisão de negócios, que leva a informação a um número bem maior de usuários dentro da corporação. Existem vários tipos de ferramentas que se propõe a essa finalidade. Esse trabalho tem como objetivo comparar ferramentas através do estudo das técnicas de modelagem dimensional, fundamentais nos projetos de estruturas informacionais, suporte a “Data Warehouses”, “Data Marts”, “Data Mining” e outros, bem como o mercado, suas vantagens e desvantagens e a arquitetura tecnológica utilizada por estes produtos. Assim sendo, foram selecionados os conjuntos de ferramentas de “Business Intelligence” das empresas Microsoft Corporation e Oracle Corporation, visto as suas magnitudes no mundo da informática.|http://hdl.handle.net/10183/3865
Abstrações para uma linguagem de programação visando aplicações móveis em um ambiente de Pervasive Computing|2004|Open Access|Tese|Computação móvel;Processamento distribuído|por|Computação Móvel é um termo genérico, ainda em definição, ao redor do qual se delineia um espectro de cenários possíveis, desde a Computação Pessoal, com o uso de computadores de mão, até a visão futurista da Computação Ubíqua. O foco do projeto ISAM (Infra-estrutura de Suporte às Aplicações Móveis Distribuída), em desenvolvimento no II/UFRGS, é a Pervasive Computing. Esta desenha um cenário onde o usuário é livre para se deslocar mantendo o acesso aos recursos da rede e ao seu ambiente computacional, todo tempo em qualquer lugar. Esse novo cenário apresenta muitos desafios para o projeto e execução de aplicações. Nesse escopo, esta tese aprofunda a discussão sobre questões relativas à adaptação ao contexto em um ambiente pervasivo sob a ótica de uma Linguagem de Programação, e define uma linguagem chamada ISAMadapt.  A definição da linguagem ISAMadapt baseia-se em quatro abstrações: contexto, adaptadores, políticas e comandos de adaptação. Essas abstrações foram concretizadas em duas visões: (1) em tempo de programação, através de comandos da linguagem e arquivos de configuração, descritos com o auxílio do Ambiente de Desenvolvimento de Aplicações; (2) em tempo de execução, através de serviços e APIs fornecidos pelos componentes que integram o ambiente de execução pervasiva (ISAMpe). Deste, os principais componentes que implementam a semântica de execução da aplicação ISAMadapt são: o serviço de reconhecimento de contexto, ISAMcontextService, e a máquina de execução da adaptação dinâmica, ISAMadaptEngine.As principais contribuições desta tese são: (a) primeira linguagem para a codificação de aplicações pervasivas; (b) sintaxe e semântica de comandos para expressar sensibilidade ao contexto pervasivo; (c) fonte para o desenvolvimento de uma metodologia de projeto de aplicações pervasivas; (d) projeto ISAM e o projeto contextS (www.inf.ufrgs.br/~isam) que fornecem suporte para o ciclo de vida das aplicações, desde o desenvolvimento até a execução de aplicações pervasivas.|http://hdl.handle.net/10183/3866
Sistema de gerência de energia para redes locais|2002|Open Access|Dissertação|Engenharia elétrica;Gerencia : Energia eletrica;Gerencia : Redes : Computadores;Tolerancia : Falhas;Redes locais : Computadores|por|Este trabalho apresenta a proposta e a implementação de um sistema de gerência de energia para redes locais de computadores (Local Area Networks ou LANs). Desde sua introdução, no início dos anos 90, os mecanismos de gerência de energia para computadores têm contribuído de maneira significativa para a redução do consumo nos períodos de inatividade, mas podem ter seu efeito minimizado por uma série de fatores, dentre os quais destaca-se a conexão do equipamento a uma rede. Em linhas gerais, o objetivo do sistema proposto é, justamente, facilitar a gerência de energia em ambientes de rede. O funcionamento do sistema é baseado na aplicação de políticas de consumo definidas pelo administrador da rede. As políticas podem ser aplicadas em duas situações distintas: em horários pré-determinados (p. ex. depois do horário comercial), quando podem ajudar a reduzir o desperdício de energia, ou em resposta a alterações no fornecimento de energia, caso a rede seja protegida por no-breaks, quando a redução no consumo resulta em maior tempo de autonomia da fonte reserva (banco de baterias). As políticas são configuradas através de um mecanismo flexível, que permite não apenas desligar os equipamentos, mas colocá-los em estados intermediários de consumo e executar outros tipos de ações.  A arquitetura do sistema é baseada no modelo SNMP (Simple Network Management Protocol) de gerência de redes. É composta, basicamente, de agentes, elementos de software que residem nos equipamentos da rede e detêm o conhecimento específico sobre suas características de consumo de eletricidade, e de um gerente, elemento central que contém a configuração das políticas de consumo e que é responsável pelo monitoramento e controle dos agentes. Gerente e agentes comunicam-se através do protocolo SNMP, trocando informações segundo uma base de informações (MIB) projetada especificamente para a gerência de energia. A ênfase da parte prática do trabalho está no gerente, que foi inteiramente implementado através da linguagem Java, utilizando bibliotecas disponíveis gratuitamente. Adicionalmente, foi implementado um agente-protótipo para a plataforma Windows, o que permitiu observar o sistema completo em execução. Os testes permitiram validar a praticabilidade da arquitetura e estimar o ganho potencial proporcionado pela utilização do sistema. São apresentadas medições que demonstram um aumento de até 6 vezes na autonomia do banco de baterias do no-break para uma configuração de rede pequena, utilizando o sistema para desligar automaticamente 90% dos computadores durante um corte no fornecimento externo. A economia decorrente da redução de consumo em horários de inatividade foi estimada em até R$0,63 por computador no período de um ano (tomando por base a tarifa média praticada no Brasil entre janeiro e maio de 2002).|http://hdl.handle.net/10183/3887
GURU : uma ferramenta para administrar banco de dados através da web|2002|Open Access|Dissertação|Banco : Dados;Gerencia : Banco : Dados;World Wide Web (WWW)|por|Antigamente as informações que as organizações utilizavam durante a sua gestão eram suficientemente armazenadas em arquivos. A própria aplicação era responsável pela manipulação dos dados e pela função de guardá-los de maneira segura. No entanto, a sociedade evoluiu com tamanha rapidez que as organizações começaram a gerar uma quantidade cada vez maior de informação e, também, a rapidez de acesso às informações armazenadas tornou-se cada vez mais importante. Os antigos sistemas de arquivos tornaram-se complexos sistemas de armazenamento de informações responsáveis por gerir grandes volumes de dados, chamados Sistemas Gerenciadores de Banco de Dados - SGBD’s. Devido à complexidade dos bancos de dados e à necessidade de sua operação ininterrupta surge a tarefa do Administrador, cuja função é assegurar que os bancos de dados permaneçam operantes, íntegros e rápidos. Para realizar suas tarefas o Administrador precisa contar com boas ferramentas de modo a tornar as intervenções no banco de dados rápidas e seguras. Existem no mercado, boas ferramentas para administração de banco de dados. No entanto, são todas proprietárias, possuem custo elevado e apresentam deficiências quando o DBA e o BD estão localizados logicamente em redes de dados distintas. Para tentar resolver este problema, este trabalho se propõe a desenvolver uma ferramenta de administração de banco de dados que o DBA possa utilizar para gerenciar os bancos de dados, utilizando a Web como instrumento.|http://hdl.handle.net/10183/3888
Diretrizes e critérios de cobertura de teste a partir de especificações UML|2001|Open Access|Dissertação|Engenharia : Software;Testes : Software;Uml|por|As maiores dificuldades encontradas no teste de software estão relacionadas à definição dos dados de teste e a decisão de quando encerrar os testes. Uma das formas encontradas para minimizar tais dificuldades está centrada na utilização de critérios de cobertura. O principal objetivo dos critérios de cobertura é tornar o processo de testes mais rápido e preciso, fornecendo informações que determinem o que testar em um software para garantir sua qualidade. A modelagem é um dos elementos de maior importância nas atividades relacionadas ao desenvolvimento de software. Os modelos são construídos principalmente para melhor se entender o sistema, descrever a estrutura e comportamento desejados, visualizar a arquitetura e documentar as decisões tomadas durante o seu desenvolvimento. Atualmente, o sistema de notação mais utilizado para a modelagem de sistemas baseados nos conceitos de orientação a objetos é a Linguagem de Modelagem Unificada – UML [LAR 99]. Nesta notação, um sistema é descrito por um conjunto de diagramas que apresentam diferentes aspectos do sistema.  As informações disponibilizadas por estes diagramas propiciam, já nas fases iniciais do desenvolvimento da aplicação (análise e projeto), o planejamento dos casos de teste e a definição de critérios de cobertura. Observa-se que nestas fases a maioria das informações necessárias para o teste já estão disponíveis, como por exemplo, a definição das classes com seus atributos, métodos e relacionamentos, a representação da interação existente entre objetos para a realização de um cenário e a descrição dos possíveis estados e transições de um objeto em resposta a eventos externos e internos. Este trabalho propõe um conjunto de diretrizes e critérios de cobertura de teste, tendo como base as especificações diagramáticas UML. As diretrizes estabelecem um conjunto de instruções para que o teste seja feito e os critérios de cobertura identificam os pontos principais e serem considerados durante o teste. Na definição das diretrizes e dos critérios foram avaliadas as informações disponibilizadas pelos diagramas de classes, seqüência, colaboração e estados.|http://hdl.handle.net/10183/3889
Calibração e aplicação do modelo numérico genesis nas praias de Tramandaí e Imbé-RS|2004|Open Access|Dissertação|Geologia marinha;Modelo Numérico Genesis;Tramandaí, Praia de (RS);Imbé, Praia de (RS)|por|O modelo numérico GENESIS (Generalized Model for Simulating Shoreline Change) é parte de um sistema de modelagem de linha de praia, o SMS (Shoreline Modeling System), desenvolvido pelo CERC (Coastal Engineering Research Center), U.S.A. É um modelo genérico, determinístico e bidimensional, com grande flexibilidade para ser adaptado a costas abertas, arenosas e sujeitas a intervenção humana. Utilizado na previsão da resposta da linha praia as diversas obras costeiras que podem ser implantadas na mesma. Características estas, que fazem dele uma ferramenta indicada para a o estudo costa do Rio Grande do Sul e para o objetivo deste estudo. A aplicação do modelo de evolução de linha praia – GENESIS neste trabalho, tem como objetivos: calibrar o modelo numérico GENESIS para a costa centro norte do Rio Grande do Sul e avaliar seu uso como ferramenta na previsão de impactos ambientais gerados por obras costeiras, Alem de reproduzir as condições do modelo físico reduzido de 1965 e comparar os resultados entre as simulações matemática e física. O modelo foi aplicado num trecho de linha de praia da região centro norte do Rio Grande do Sul, nas praias de Tramandaí e Imbé.  As quais já foram alvo de estudos anteriores através de modelo físico reduzido, em função do desejo deste município em construir molhes na desembocadura do canal da Laguna de Tramandaí. Para implementação do modelo numérico GENESIS foram utilizados dados das posições da linha de praia em três diferentes anos, coletados pelo CECO/UFRGS, dados de onda coletados pelo ondógrafo do IPH/UFRGS, e diversos dados sobre as praias e sua história, retirados da extensa bibliografia publicada sobre a região de estudo. A calibração do modelo foi realizada através das linhas de praia medidas em 1997 e em 2000. O modelo foi considerado calibrado quando o mesmo consegui reproduzir a linha de praia do ano 2000 a partir da linha de 1997, obtendo um erro máximo de 15 m. Foram realizadas simulações que reproduziam as simulações feitas em modelo físico reduzido do IPH em 1965. Através da comparação dos dados de onda utilizados no modelo físico reduzido de 1965 e dos dados de onda coletados pelo ondógrafo em 1996, pudemos observar a importância do uso de um série de dados de onda neste tipo de estudo, bem como, a desenvoltura e limitações do modelo numérico GENESIS na situações geradas.|http://hdl.handle.net/10183/3898
Processos e componentes mantélicos no norte da Patagônia (Argentina) e relações com a subducção Andina : Evidências petrográficas, geoquímicas e isotópicas em xenólitos ultramáficos mantélicos|2004|Open Access|Dissertação|Geoquímica;Metassomatismo;Xenólitos;Patagônia (Argentina e Chile);Argentina|por|Xenólitos ultramáficos, carregados até a superfície da Terra por magmatismo básico alcalino intraplaca, fornecem evidências diretas da natureza e processos envolvidos em modificações do manto litosférico subcontinental, como fusão parcial e metassomatismo. Estes xenólitos têm sido utilizados para identificar processos relacionados a evolução da litosfera continental, estimar a composição original do manto e a escala das heterogeneidades mantélicas. Raramente xenólitos ultramáficos mantélicos são encontrados em ambientes convergentes, no entanto na Patagônia (sul da América do Sul), diversas ocorrências são identificadas em basaltos alcalinos na região de arco e back arc da Cordilheira do Andes. Estes xenólitos oportunizam o estudo dos processos de interação entre a cunha mantélica, a placa oceânica subductada e a astenosfera. Nesta dissertação são apresentados dados petrográficos, mineralógicos, geoquímicos e de isótopos de Sr e O em 22 xenólitos ultramáficos de dois centros vulcânicos Mioceno-Holoceno distintos: Cerro del Mojon (41°06’S-70°13’W) e Estancia Alvarez (40°46’S-68°46’W), localizados na borda NW do Platô de Somuncura, norte de Patagônia (Argentina). A suíte de xenólitos ultramáficos do Cerro del Mojon consiste de espinélio dunitos e harzburgitos mantélicos anidros (Grupo1), espinélio lherzolitos mantélicos anidros (Grupo 2a) e hidratados (Grupo 2b) e espinélio clinopiroxenitos crustais (Grupo 3). Os xenólitos mantélicos do Grupo 1 são depletados (empobrecidos em ETR pesados e HFSE, com baixas razões 87Sr/86Sr em Cpx – 0,7028-0.7037), de alta PT (16-19 kbar, 950-1078 ºC), e têm evidências de metassomatismo críptico (enriquecimento em K, Na ETR leves) atribuído a componentes derivados de sedimentos da placa oceânica subductada, EM 2 (87Sr/86Sr em RT até 0,7126, 87Rb/86Sr até 1,66 e δ18O até +6.78‰).  Estes valores anomalamente altos foram obtidos em amostras com bolsões de reação ao redor do espinélio, induzidos pela percolação de fluidos metassomáticos sob altas pressões seguida por descompressão (Sp+fluido→ Cpx+Ol+Sp+melt-andesítico-traquítico). Os xenólitos do Grupo 2 são moderadamente depletados (empobrecidos em HFSE, com baixas a altas razões 87Sr/86Sr em RT e Cpx – 0,7031-0,7045 e δ18O +5-6.2‰), de baixa PT (14-15 kbar, 936-942 ºC), e têm evidências de metassomatismo modal e críptico (enriquecimento em ETR leves, Na, K, Ti, Sr, Hf e anfibólio modal). O metassomatismo críptico parece estar relacionado ao mesmo agente metassomatizante do Grupo 1, mas ocorre em menor intensidade. O metassomatismo modal, no entanto, tem outra origem, podendo ser derivado de fontes mantélicas profundas. A quebra do anfibólio metassomático durante a descompressão e ascensão dos xenólitos até a superfície formou bolsões de reação ao redor do anfibólio (Anf→Cpx+Ol+Sp+melts-basálticos & andesíticos). A suíte de xenólitos ultramáficos de Estancia Alvarez consiste de espinélio harzburgitos anidros mantélicos (Grupo 1a) e espinélio dunitos anidros crustais (Grupo 1b), ambos com veios de serpentina. Os xenólitos mantélicos do Grupo 1a são depletados (empobrecidos em ETR leves e HFSE), de profundidades variadas (P entre 11-18 kbar) e de baixa T (877-961 ºC). Estes xenólitos têm enriquecimento em ETR leves, B, Rb e K, que pode estar relacionado a percolação de fluidos ricos em H2O e LILE gerados pela desidratação de filossilicatos dos sedimentos da placa oceânica (EM 2), como evidenciado por veios de serpentina e altas razões 87Sr/86Sr (0,7046-0,7298), 87Rb/86Sr (0,07-5,63) e valores de δ18O (até +7,88‰).|http://hdl.handle.net/10183/3909
Organização e armazenamento de conteúdo instrucional no ambiente AdaptWeb utilizando XML|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Hiperdocumento;Ensino-aprendizagem;Internet;Armazenamento : Dados;XML (Linguagem de marcação)|por|O uso da Internet como ferramenta de ensino tem se tornado cada vez mais freqüente. A recente popularização da Internet vem permitindo o desenvolvimento de ambientes de ensino-aprendizagem baseados na Web. Os principais recursos explorados para fins educacionais são hipertexto e hipermídia, que proporcionam uma grande gama de elementos para o instrutor que pretende utilizar a WWW. Este trabalho está inserido no desenvolvimento do ambiente AdaptWeb (Ambiente de Ensino e Aprendizagem Adaptativo para a Web), que visa o desenvolvimento de um ambiente de educação a distância. A arquitetura do ambiente é composta por quatro módulos entre eles o módulo de Armazenamento de dados que armazena todos os dados provenientes da fase de Autoria utilizando XML (Extensible Markup Language). Na etapa de Autoria é feita a inserção de todos os dados relativos a disciplina que deseja disponibilizar, estes dados serão armazenados temporariamente em uma representação matricial em memória. A entrada de dados do módulo de Armazenamento de Dados é esta representação matricial que serve então como base para a geração dos arquivos XML, que são utilizados nas demais etapas do ambiente. Para a validação dos arquivos XML foram desenvolvidas DTD (Document Type Definition) e também foi implementado um analisador de documentos XML, utilizando a API (Application Programming Interface) DOM (Document Object Model), para efetuar a validação sintática destes documentos. Para conversão da representação matricial em memória foi especificado e implementado um algoritmo que funciona em conformidade com as DTD especificadas e com a sintaxe da linguagem XML.|http://hdl.handle.net/10183/3952
Um Mecanismo de realimentação de relevâncias para recuperação de informações visuais utilizando feições a partir de imagens JPEG|2002|Open Access|Dissertação|Armazenamento : Dados;Recuperação : Informação visual|por|Esta dissertação propõe e discute um mecanismo de realimentação de relevâncias (i. e. “Relevance Feedback”). A técnica de realimentação de relevâncias foi introduzida inicialmente em meados dos anos 60, como uma estratégia para refinamento de consultas para a recuperação de informações. Como uma técnica de refinamento de consultas, foi aplicada inicialmente em sistemas de recuperação de informações textuais. Neste caso, os termos ou expressões consideradas importantes, são utilizados na formulação de uma nova consulta. Ao surgirem os sistemas de recuperação de informação visual baseada em conteúdo (CBVIR), houve a necessidade de serem introduzidos novos elementos associados a esse processo de reformulação de consultas, de tal forma que fossem utilizados não apenas as informações de alto nível, como os termos e expressões. Esses novos elementos passaram a considerar também a subjetividade de percepção humana em relação ao conteúdo visual. Neste trabalho, apresenta-se um processo de extração e representação desse conteúdo, através da utilização de feições (conteúdo) de cor e textura, extraídos de imagens JPEG, uma vez que no processo de compressão de imagens nesse formato, utiliza-se coeficientes da Transformada Discreta do Cosseno (DCT), sendo, portanto esses coeficientes utilizados como elementos que possuem as informações associadas a cor e textura na imagem. Além da DCTé utilizada a Transformação Mandala [YSH 83] no processo de agrupamento de somente 10 coeficientes, com o objetivo de produzir 10 imagens com resoluça menor que a imagem original, mas que representam cada uma, o conteúdo de uma frequência particular da imagem original.  A escolha por uma representação como essa,é a garantia de uma redução significativa na quantidade de dados a serem processados. Entretanto, a representação obtida nesse formato para as imagens,é com base em conteúdo global de cor e textura, o que pode produzir resultados insatisfatórios. A introdução de um mecanismo de realimentação de relevâncias, associado à representação utilizada, permite contornar a dificuldade apontada acima, através da obtenção de consultas subsequentes, selecionando os objetos mais relevantes, assim como menos objetos não relevantes, utilizando o conhecimento do usuário de forma interativa no refinamento de consultas para recuperação de informações visuais.|http://hdl.handle.net/10183/3954
Um Modelo formal e executável de agentes BDI|1999|Open Access|Tese|Inteligência artificial;Agentes inteligentes;Sistemas multiagentes;Programacao em logica|por|Modelos BDI (ou seja, modelos Beliefs-Desires-Intentions models) de agentes têm sido utilizados já há algum tempo. O objetivo destes modelos é permitir a caracterização de agentes utilizando noções antropomórficas, tais como estados mentais e ações. Usualmente, estas noções e suas propriedades são formalmente definidas utilizandos formalismos lógicos que permitem aos teóricos analisar, especificar e verificar agentes racionais. No entanto, apesar de diversos sistemas já terem sido desenvolvidos baseados nestes modelos, é geralmente aceito que existe uma distância significativa entre esta lógicas BDI poderosas e sistemas reais. Este trabalho defende que a principal razão para a existência desta distância é que os formalismos lógicos utilizados para definir os modelos de agentes não possuem uma semântica operacional que os suporte. Por “semântica operacional” entende-se tanto procedimentos de prova que sejam corretos e completos em relação à semântica da lógica, bem como mecanismos que realizem os diferentes tipos de raciocínio necessários para se modelar agentes. Há, pelo menos, duas abordagens que podem ser utilizadas para superar esta limitação dos modelos BDI. Uma é estender as lógicas BDI existentes com a semântica operacional apropriada de maneira que as teorias de agentes se tornem computacionais. Isto pode ser alcançado através da definição daqueles procedimentos de prova para as lógicas usadas na definição dos estados mentais. A outra abordagem é definir os modelos BDI utilizando formalismos lógicos apropriados que sejam, ao mesmo tempo, suficientemente poderosos para representar estados mentais e que possuam procedimentos operacionais que permitam a utilizaçao da lógica como um formalismo para representação do conhecimento, ao se construir os agentes. Esta é a abordagem seguida neste trabalho.  Assim, o propósito deste trabalho é apresentar um modelo BDI que, além de ser um modelo formal de agente, seja também adequado para ser utilizado para implementar agentes. Ao invés de definir um novo formalismo lógico, ou de estender um formalismo existente com uma semântica operacional, define-se as noções de crenças, desejos e intenções utilizando um formalismo lógico que seja, ao mesmo tempo, formalmente bem-definido e computacional. O formalismo escolhido é a Programação em Lógica Estendida com Negação Explícita (ELP) com a semântica dada pelaWFSX (Well-Founded Semantics with Explicit Negation - Semântica Bem-Fundada com Negação Explícita). ELP com a WFSX (referida apenas por ELP daqui para frente) estende programas em lógica ditos normais com uma segunda negação, a negação explícita1. Esta extensão permite que informação negativa seja explicitamente representada (como uma crença que uma propriedade P não se verifica, que uma intenção I não deva se verificar) e aumenta a expressividade da linguagem. No entanto, quando se introduz informação negativa, pode ser necessário ter que se lidar com programas contraditórios. A ELP, além de fornecer os procedimentos de prova necessários para as teorias expressas na sua linguagem, também fornece um mecanismo para determinar como alterar minimamente o programa em lógica de forma a remover as possíveis contradições. O modelo aqui proposto se beneficia destas características fornecidas pelo formalismo lógico.  Como é usual neste tipo de contexto, este trabalho foca na definição formal dos estados mentais em como o agente se comporta, dados tais estados mentais. Mas, constrastando com as abordagens até hoje utilizadas, o modelo apresentanto não é apenas uma especificação de agente, mas pode tanto ser executado de forma a verificar o comportamento de um agente real, como ser utilizado como mecanismo de raciocínio pelo agente durante sua execução. Para construir este modelo, parte-se da análise tradicional realizada na psicologia de senso comum, onde além de crenças e desejos, intenções também é considerada como um estado mental fundamental. Assim, inicialmente define-se estes três estados mentais e as relações estáticas entre eles, notadamente restrições sobre a consistência entre estes estados mentais. Em seguida, parte-se para a definição de aspectos dinâmicos dos estados mentais, especificamente como um agente escolhe estas intenções, e quando e como ele revisa estas intenções. Em resumo, o modelo resultante possui duas características fundamentais:(1) ele pode ser usado como um ambiente para a especificação de agentes, onde é possível definir formalmente agentes utilizando estados mentais, definir formalmente propriedades para os agentes e verificar se estas propriedades são satifeitas pelos agentes; e (2) também como ambientes para implementar agentes.|http://hdl.handle.net/10183/3955
ONNIS-GI: uma rede neural oscilatória para segmentação de imagens implementada em arquitetura maciçamente paralela|2004|Open Access|Tese|Computação gráfica;Processamento de imagens;Redes neurais;Segmentacao : Imagem;Visão computacional;Processamento paralelo|por|A presente tese apresenta a concepção de uma rede neural oscilatória e sua realização em arquitetura maciçamente paralela, a qual é adequada à implementação de chips de visão digitais para segmentação de imagens. A rede proposta, em sua versão final, foi denominada ONNIS-GI (Oscillatory Neural Network for Image Segmentation with Global Inhibition) e foi inspirada em uma rede denominada LEGION (Locally Excitatory Globally Inhibitory Oscillator Network), também de concepção recente. Inicialmente, é apresentada uma introdução aos procedimentos de segmentação de imagens, cujo objetivo é o de situar e enfatizar a importância do tema abordado dentro de um contexto abrangente, o qual inclui aplicações de visão artificial em geral. Outro aspecto abordado diz respeito à utilização de redes neurais artificiais em segmentação de imagens, enfatizando as denominadas redes neurais oscilatórias, as quais têm apresentado resultados estimulantes nesta área. A implementação de chips de visão, integrando sensores de imagens e redes maciçamente paralelas de processadores, é também abordada no texto, ressaltando o objetivo prático da nova rede neural proposta. No estudo da rede LEGION, são apresentados resultados de aplicações originais desenvolvidas em segmentação de imagens, nos quais é verificada sua propriedade de separação temporal dos segmentos. A versão contínua da rede, um arranjo paralelo de neurônios baseados em equações diferenciais, apresenta elevada complexidade computacional para implementação em hardware digital e muitos parâmetros, com procedimento de ajuste pouco prático. Por outro lado, sua arquitetura maciçamente paralela apresenta-se particularmente adequada à implementação de chips de visão analógicos com capacidade de segmentação de imagens.  Com base nos bons resultados obtidos nas aplicações desenvolvidas, é proposta uma nova rede neural, em duas versões, ONNIS e ONNIS-GI, as quais suplantam a rede LEGION em diversos aspectos relativos à implementação prática. A estrutura dos elementos de processamento das duas versões da rede, sua implementação em arquitetura maciçamente paralela e resultados de simulações e implementações em FPGA são apresentados, demonstrando a viabilidade da proposta. Como resultado final, conclui-se que a rede ONNIS-GI apresenta maior apelo de ordem prática, sendo uma abordagem inovadora e promissora na solução de problemas de segmentação de imagens, possuindo capacidade para separar temporalmente os segmentos encontrados e facilitando a posterior identificação dos mesmos. Sob o ponto de vista prático, a nova rede pode ser utilizada para implementar chips de visão digitais com arquitetura maciçamente paralela, explorando a velocidade de tais topologias e apresentando também flexibilidade para implementação de procedimentos de segmentação de imagens mais sofisticados.|http://hdl.handle.net/10183/3956
Ocorrência de trihalometanos e ácidos haloacéticos na desinfecção de efluentes tratados biologicamente|2004|Open Access|Dissertação|Tratamento de efluentes;Subprodutos da desinfecção;Desinfetantes hipocloritos;Ferrato(vi);Trihalometano;Ácido haloacético;Química ambiental|por|Desinfecção é um processo empregado no tratamento de água potável e de efluente líquido. Cloro e seus derivados (desinfetantes mais utilizados) apresentam o inconveniente de formar produtos perigosos à saúde, resultantes de reações com compostos orgânicos. Os compostos Trihalometanos (THMs) e os ácidos haloacéticos (HAAs) estão entre os principais grupos de subprodutos encontrados. Vários fatores interferem na formação de tais produtos, como por exemplo: pH, temperatura, tempo reacional, nitrogênio amoniacal etc. O presente trabalho apresenta um estudo da geração de subprodutos provindos da desinfecção de quatro distintos efluentes de estações de tratamento biológico. Hipoclorito de sódio e ferrato(VI) de sódio foram usados como desinfetantes em concentrações e tempos reacionais variados. Análises de pH, demanda química de oxigênio, nitrogênio amoniacal, cloro combinado, THMs e HAAs foram realizadas. A concentração dos subprodutos foi proporcional à concentração e ao tempo reacional do desinfetante hipoclorito de sódio. Para a mais alta concentração empregada de hipoclorito (20 mg L-1) e maior tempo reacional (168 h), o total de THMs não excedeu ao valor máximo de descarga permitido para efluentes tratados (1 mg L-1 de clorofórmio). Os THMs e os HAAs apresentaram-se inversamente correlacionados com a concentração de nitrogênio amoniacal presente no efluente. Para o desinfectante ferrato(VI) não houve formação de subprodutos halogenados, uma vez que este desinfetante não contribui com átomos de cloro.|http://hdl.handle.net/10183/3959
Estudo de difusão de In e Pd em [alfa]-Ti utilizando a técnica de espectrometria de retroespalhamento de Rutherford e canalização|1998|Open Access|Dissertação|Difusão;Retroespalhamento rutherford;Canalização;Implantacao ionica;Valência;Solubilidade|por|O objetivo da presente dissertação é o estudo de difusão de In e Pd na matriz de a-Ti. O interesse deste estudo baseia-se na investigação sistemática, realizada pelo grupo de Implantação Iônica, de difusão de impurezas substitucionais em a-Ti com o fim de determinar uma relação entre tamanho, valência e solubilidade e os respectivos coeficientes de difusão. Considerações de tamanho, diferença de valência e solubilidade indicam que o In deverá difundir substitucionalmente. Por outro lado, não está claro qual será o mecanismo de difusão do Pd, pois a sua solubilidade não é muito alta e seu raio atômico é menor que o da matriz. A dependência do coeficiente de difusão desses dois elementos com a temperatura foi estudada entre 823 e 1073 K no caso do In, e entre 673 e 1073 K no caso do Pd. Em ambos os casos, a técnica de RBS foi utilizada para determinar os perfis de concentração. Esta técnica tem uma alta resolução em profundidade (tipicamente 10 nm), o que permite a determinação das baixas difusividades esperadas no presente experimento (D ≤ 10-17 m² s-¹). Adicionalmente, utilizamos para o caso do Pd a técnica de canalização, com a finalidade de determinar o grau de substitucionalidade dos átomos de Pd, tanto imediatamente após a implantação, quanto depois de efetuados os recozimentos e, portanto, determinar o mecanismo de difusão do mesmo em α - Ti.  No caso do In, os coeficientes de difusão seguem uma lei de Arrhenius com parâmetros de difusão Q = (260 ± 10) kJ/mol e Do = (2,0 ± 1,3) x 10-6 m² s-¹ , valores estes, típicos de um comportamento difusivo substitucional. No entanto, para o Pd, os coeficientes de difusão também seguem um comportamento tipo Arrhenius, com Q = (264 ± 9) kJ/mol e Do = (2,8 ± 1,3) x 10-3 m² s-¹. Quando se comparam os resultados do Pd com outros substitucionais, tais como Pb, Au, Sn e In, os resultados indicam que o Pd é um difusor rápido, porém mais lento que os intersticiais do tipo Fe, Co ou Cu. Por outro lado, os experimentos de canalização indicam que no intervalo de temperatura estudado, 30% dos átomos de Pd ficam em posição intersticial na rede. Isto é um forte indício de que o mecanismo de difusão é do tipo misto.|http://hdl.handle.net/10183/3984
Estudo de difusão de impurezas introduzidas por implantação iônica em polímeros|2003|Open Access|Tese|Filmes de polímeros;Difusão;Impurezas;Níveis profundos;Implantação de íons;Retroespalhamento rutherford;Feixes de íons;Tratamento térmico;Ouro;Prata;Bismuto;Európio;Erbio;Boro;Xenonio;Criptonio|por|No presente trabalho estudamos de forma sistemática a difusão de impurezas em filmes poliméricos usando as técnicas de implantação iônica e análise por feixe de íons, retroespalhamento Rutherford e de perfil de profundidade por nêutrons. Com este propósito foram implantadas e realizadas medidas em diferentes intervalos de temperatura para diferentes sistemas como (Au, Ag) no fotoresiste AZ1350, (Bi, Eu, Er, B) no fotoresiste S1813 e (Xe, Kr) no termoplástico Poliestireno. Como resultado mostrou-se que para implantações em baixas energias e fluências Au, Ag seguem uma difusão regular. Os valores obtidos para os parâmetros de difusão são semelhantes indicando assim um mecanismo de difusão verdadeiramente atômico. É mostrado também que com o aumento da fluência, devido aos danos gerados pelo processo de implantação, átomos são aprisionados na região implantada levando a um mecanismo de difusão por aprisionamento e liberação. Contudo, mostrou-se que a energia de ativação indica que este processo de difusão ainda é de caráter atômico. Da análise dos valores de D observamos um efeito de massa associado onde D(T)Au < D(T)Ag, pois a massa de Ag é duas vezes menor que a de Au.  Para os elementos como Bi, Eu e Er, considerados quimicamente mais ativos que Au, não foram observados efeitos de possíveis ligações químicas nestes sistemas. Valores de energia de ativação de Bi apresentaram-se próximos aos de Au para as fluências de implantação aplicadas, o mesmo ocorrendo para a difusão de Er e Eu. O B mostrou que depois de implantado difunde durante ou imediatamente após a implantação. Difusão esta dada na presença de armadilhas saturáveis induzidas por radiação, indo além da difusão térmica, por ordem de magnitude de ≈10-12, contra ≈10-13 cm2s-1, respectivamente. Quanto a Xe e Kr, observou-se que estes também difundem durante ou imediatamente após a implantação, e que a fração do gás retido no pico depende da fluência implantada. Implantações em baixa temperatura (80 K) e posteriores análises foram determinados in situ por RBS na faixa de 80 a 300 K. Verificou-se que a difusão segue um perfil regular. Em cada caso mostrou-se que a dependência dos valores de D como função da temperatura segue um comportamento tipo Arrhenius, com valores de energia de ativação para os metais (Au, Ag, Bi) entre 580 e 680 meV, para os lantanídeos (Er, Eu) valores entre 525 a 530 meV, para o semi-metal (B) 100 meV, e finalmente para os gases nobres Kr e Xe entre 40 e 67 meV.|http://hdl.handle.net/10183/3985
Influência de efeitos não-locais na dispersão de poluentes na camada linite planetária|2004|Open Access|Dissertação|Poluição do ar;Camada limite planetária|por|A equação de difusão-advecção é muito utilizada no campo de estudos da poluição atmosférica na determinação da concentração de poluentes. Uma maneira de solucionar o problema de fechamento desta equação está baseada na hipótese de transporte por gradiente que, em analogia com a difusão molecular, assume que o fluxo turbulento de concentração é proporcional à magnitude do gradiente de concentração média. Neste trabalho, diferentemente do modo tradicional, utiliza-se uma equação genérica para a difusão turbulenta considerando-se que o fluxo mais a sua derivada são proporcionais ao gradiente médio. Desta forma, obtém-se uma equação que leva em conta a assimetria no processo de dispersão de poluentes atmosféricos. Portanto, a proposta do presente trabalho é a obtenção da solução analítica desta nova equação utilizando-se a técnica da Transformada de Laplace, considerando-se a Camada Limite Planetária (CLP) como um sistema multicamadas. Os parâmetros que encerram a turbulência sâo derivados da teoria de difusão estatística de Taylor combinada com a teoria de similaridade convectiva válidos para grandes tempos de difusão. Finalmente, na avaliação da performance deste modelo que considera a assimetria no processo de dispersão de poluentes atmosféricos, utilizam-se os dados experimentais de Copenhagen e Prairie Grass.|http://hdl.handle.net/10183/4020
Graphical models and point set matching;Modelos Gráficos e Casamento de Padrões de Pontos |2004|Open Access|Tese|Point pattern matching;Weighted graph matching;Probabilistic graphical models;Hidden Markov random fields;Pattern recognition;Computação gráfica;Reconhecimento : Padroes|eng|Point pattern matching in Euclidean Spaces is one of the fundamental problems in Pattern Recognition, having applications ranging from Computer Vision to Computational Chemistry. Whenever two complex patterns are encoded by two sets of points identifying their key features, their comparison can be seen as a point pattern matching problem. This work proposes a single approach to both exact and inexact point set matching in Euclidean Spaces of arbitrary dimension. In the case of exact matching, it is assured to find an optimal solution. For inexact matching (when noise is involved), experimental results confirm the validity of the approach. We start by regarding point pattern matching as a weighted graph matching problem. We then formulate the weighted graph matching problem as one of Bayesian inference in a probabilistic graphical model. By exploiting the existence of fundamental constraints in patterns embedded in Euclidean Spaces, we prove that for exact point set matching a simple graphical model is equivalent to the full model. It is possible to show that exact probabilistic inference in this simple model has polynomial time complexity with respect to the number of elements in the patterns to be matched. This gives rise to a technique that for exact matching provably finds a global optimum in polynomial time for any dimensionality of the underlying Euclidean Space. Computational experiments comparing this technique with well-known probabilistic relaxation labeling show significant performance improvement for inexact matching. The proposed approach is significantly more robust under augmentation of the sizes of the involved patterns. In the absence of noise, the results are always perfect.;Casamento de padrões de pontos em Espaços Euclidianos é um dos problemas fundamentais em reconhecimento de padrões, tendo aplicações que vão desde Visão Computacional até Química Computacional. Sempre que dois padrões complexos estão codi- ficados em termos de dois conjuntos de pontos que identificam suas características fundamentais, sua comparação pode ser vista como um problema de casamento de padrões de pontos. Este trabalho propõe uma abordagem unificada para os problemas de casamento exato e inexato de padrões de pontos em Espaços Euclidianos de dimensão arbitrária. No caso de casamento exato, é garantida a obtenção de uma solução ótima. Para casamento inexato (quando ruído está presente), resultados experimentais confirmam a validade da abordagem. Inicialmente, considera-se o problema de casamento de padrões de pontos como um problema de casamento de grafos ponderados. O problema de casamento de grafos ponderados é então formulado como um problema de inferência Bayesiana em um modelo gráfico probabilístico. Ao explorar certos vínculos fundamentais existentes em padrões de pontos imersos em Espaços Euclidianos, provamos que, para o casamento exato de padrões de pontos, um modelo gráfico simples é equivalente ao modelo completo. É possível mostrar que inferência probabilística exata neste modelo simples tem complexidade polinomial para qualquer dimensionalidade do Espaço Euclidiano em consideração. Experimentos computacionais comparando esta técnica com a bem conhecida baseada em relaxamento probabilístico evidenciam uma melhora significativa de desempenho para casamento inexato de padrões de pontos. A abordagem proposta é signi- ficativamente mais robusta diante do aumento do tamanho dos padrões envolvidos. Na ausência de ruído, os resultados são sempre perfeitos.|http://hdl.handle.net/10183/4041
Adicionando qualidade de serviço para um ambiente de colaboração visual baseado em H.323.|2003|Open Access|Dissertação|Redes;Computadores;Trabalho cooperativo;Multimídia|por|A colaboração visual, recurso que permite a troca de informações de forma remota, é construída em cima de uma combinação de diversas ferramentas, na qual estão incluídos: videoconferência, “streaming de vídeo”, compartilhamento e transferência de informações e imagens (colaboração em cima de dados) entre outros. Estas soluções, vêm utilizando cada vez mais, o protocolo IP e a Internet para o transporte dos sinais. Com este objetivo, o ITU-T lançou a recomendação H.323, que definiu um padrão confiável, que permite a troca de sinais multimídia em redes de pacotes sem qualidade de serviço. Entretanto, com o passar dos anos percebeu-se que aplicações que manipulam voz e vídeo, precisam que as redes de pacotes tenham capacidade de prover características semelhantes às oferecidas por redes de comutação por circuito, para o transporte dos sinais multimídia. Neste sentido, redes IP podem utilizar mecanismos de qualidade de serviço como o DiffServ, para prover tratamento adequado dos sinais de áudio e vídeo e assim, aumentar a qualidade percebida pelos usuários. As aplicações de colaboração visual são notáveis candidatas a utilização de mecanismos de QoS da rede. Neste caso é desejável que estas aplicações estejam aptas a especificar o nível de qualidade de serviço desejado e requisitem este nível de serviço para a rede.  Neste contexto, o trabalho apresenta um modelo para o desenvolvimento de um terminal H.323 capaz de requisitar qualidade de serviço para a rede IP, visando aumentar a qualidade percebida pelo usuário em relação aos sinais de mídia. Neste terminal foi incluída uma entidade chamada de EPQoSE, responsável pela sinalização de QoS em benefício do terminal. Além disso, o modelo proposto neste texto apresenta um sistema de gerenciamento baseado em políticas, responsável por controlar as requisições de QoS dos terminais H.323, dentro de um domínio. Como o terminal precisa se comunicar com estas entidades, apresentamos no trabalho, a maneira como ele faz isso e definimos um conjunto de funções que devem ser implementadas pelo QoSM no terminal.|http://hdl.handle.net/10183/4060
Estudo da diastereosseletividade simples e facial envolvendo íons imínio e N-acilimínio|2003|Open Access|Dissertação|Diastereosseletividade;Reacao de mannich;Síntese orgânica|por|Na primeira parte deste trabalho foi investigada a diastereosseletividade simples da reação de Mannich envolvendo silil enol éteres 34, 50 e 85 com faces enantiotópicas e iminas aromáticas 47, 53 e 81 ativadas in situ por quantidades catalíticas de ln(0Tf)₃ e lnCl₃. Nesta etapa, foi realizado um estudo comparativo da ação dos dois ácidos de Lewis como catalisadores e as β-aminocetonas 51, 86-92 com dois centros estereogênicos foram obtidas em rendimentos que variaram entre 50-98%. Foi possível constatar a habilidade de lnCl₃ e ln(OTf)₃ como ácidos de Lewis para promover a formação in situ de cátions imínio e posterior obtenção de sistemas β-aminocarbonílicos 51, 86-92. Não foi observado um controle significativo da diastereosseletividade simples na formação dos isômeros syn e anti para os compostos β-aminocarbonílicos 51, 86-92, nas condições empregadas neste estudo. No segundo capítulo deste trabalho, investigou-se as reações de α-amidoalquilação envolvendo os nucleófilos 1-trimetilsiloxi-1-metoxi-2,2-dimetil eteno 63, aliltrimetilsilano 154, 1-fenil-1-(trimetilsililoxi) eteno 54 e os íons N-acilimínio endocíclicos gerados a partir de N-benzil-5-acetoxilactamas 149-153 com diferentes grupos protetores no oxigênio em C-4. Este estudo foi realizado com o intuito de avaliar a influência da natureza desses grupos substituintes sobre a diastereosseletividade facial nestas reações. Os grupos protetores utilizados foram: acetila (Ac), benzoila (Bz), terc-butildimetilsilila (TBS), trimetilsilila (TMS), p-toluenossulfonila (Ts) e metanosufonila (Ms). Os precursores 149-153 foram submetidos à ação de JnCl₃ (ácido de Lewis) que demonstrou também eficiência na geração in situ de cátions N-acilimínios, entretanto em quantidades subestequiométricas, ou TMSOTf (agente sililante} a fim de gerar in situ os cátions correspondentes, seguido da adição dos nucleófilos. Através destas reações, foi possível obter os compostos aminocarbonílicos 155-165, na maioria dos casos, como misturas diastereoisoméricas, tendo como produtos majoritários os adutos com estereoquímica relativa trans. Nas reações de adição do silil ceteno acetal 63 aos cátions N-acilimínio derivados dos compostos 149-152, observou-se preferência na formação dos isômeros trans com boas seletividades em todos os casos e com rendimentos na faixa de 65%, sendo que quando os precursores 150 (R=Bz) e 151 (R=TBS) foram utilizados, apenas o diastereoisômero trans foi observado. Nas reações que envolveram o nucleófilo aliltrimetilsilano 154 e os íons N-acilimínio, gerados a partir dos precursores 149-153, obteve-se as respectivas misturas diastereoisoméricas dos produtos aminocarbonílicos 160-164 em rendimentos que variaram de 20-75%. Observou-se na maioria dos casos uma preferência pela formação dos isômeros trans em moderadas seletividades, exceto quando o precursor 151 foi utilizado. Neste caso, o composto 162 foi obtido com seletividade facial (70:30) em favor do diastereoisômero cis com um rendimento em tomo de 70%. Finalmente, nas reações envolvendo o nucleófilo silil enol éter 54, foi possível observar a formação do respectivo produto aminocarbonílico apenas quando o precursor 149 foi utilizado, neste caso o produto aminocarbonílico 165 trans foi obtido como majoritário na razão de 91:9.;In Chapter I of this work, the simple diastereoselectivity of Mannich reaction between the silyl enol ethers 34, 50 e 85 with enantiotopic faces and the aromatic aldimines 47, 53 and 81 promoted by catalytic amounts of lnCl₃ and ln(OTf)₃ was investigated. The corresponding diastereoisomeric (syn and anti) β-aminoketonas 51 and 86-92 were isolated in 50-98% yeld. A comparativa study of the ability of both indium compounds as a Lewis acid catalyst was performed and the in situ formation of na activated imine-indium complex (na iminium ion) was postulated as the reactive specie in the Mannich reaction. In the reaction conditions used in this work, poor levels of simple diastereoselectivity were observed in the formation of the syn and anti isomers. In Chapter II, the α-amidoalkilation reaction between the silylated carbon nucleophiles 54, 63 and 154, with the endocyclic N-acyliminium íons promoted by lnCl₃ or TMSOTf was investigated. The N-acyliminium ions were generated in situ from the correspondi 4-O-protected-N-benzyl-5- acetoxylactams 149-153 and the influence of a set of O-protecting groups, acetyl (Ac), benzoyl (Bz), Tertbutyldimethylsilyl (TBS), trimethilsilyl (TMS), p-toluenesulfonyl (Ts) and methanesulfonyl (Ms) at C-4 position on the facial diastereoselectivity was evaluated. For the reaction with the silylated nucleophile 63, the aminocarbonyl compounds 155-165 was isolated in approximately 65% yeld as a mixture of cis and trans isomers with the predominance of the trans isomer in all cases. To the Bz and TBS O-protecting groups, only trans isomers were formed. The reactions with the nucleophile 154 and the N-acyliminium íons afforded the corresponding aminocarbonyl compounds 160-164 in 20-75% yeld. The prefrential formation of the trans isomers also has been found in this case, excepting the compound 162 (TBS O-protecting group) which leads to unexpected cis stereochemistry. Finally, the nuclephile 54 showed itself a low reactivity in the reactions with the N-acyliminium cations and only the aminocarbonyl compound 165 (Ac O-protecting group) was isolated in good yeld in a 91:9 ration, in favour of the trans isomer.|http://hdl.handle.net/10183/4104
Simulação computacional por dinâmica molecular de propriedades de equilíbrio e espectroscópicas de misturas líquidas CS2/C6H6|2003|Open Access|Tese|Dinâmica molecular : Líquidos;Simulação computacional|por|Propriedades de equilíbrio e espectroscópicas são determinadas para a mistura líquida CS2/C6H6 na temperatura de 298K e nas frações molares de benzeno iguais a 0,25, 0,50 e 0,75, usando simulação computacional por Dinâmica Molecular. As interações intermoleculares são descritas em relaçãao a todos os átomos por um potencial Lennard-Jones (12/6), com as interações eletrostáticas representadas por quadrupolos pontuais, localizados nos centros de massa das mol´eculas. Um segundo potencial também é considerado, onde a distribuição de cargas na molécula é usada para descrever as interações eletrostáticas. Dados termodinâmicos e propriedades estruturais, descritas através de funções de distribuição radial e de correlação angular, são calculados. Os resultados obtidos com os dois potenciais mostram boa concordância com os dados experimentais. A análise das frações molares locais e das energias internas indica que a mistura líquida CS2/C6H6 apresenta comportamento aproximadamente ideal. As funções de distribuição radial e de correlação angular evidenciam uma configuração preferencialmente paralela entre as moléculas de CS2 e C6H6 a distâncias menores em relação a distância do máximo da função de distribução , correspondente a primeira camada de coordenação . Nesta região, a configuração perpendicularé fracamente favorecida em relação as demais orientações.  Funções de correlação temporal da polarizabilidade coletiva e da polarizabilidade molecular, correspondentes aos espalhamentos Rayleigh e Raman, respectivamente, são calculadas a partir do modelo de interação dipolo induzido por dipolo de 1a ordem. As funções de correlação são divididas nas contribuições orientacional, induzida por colisão e cruzada, e nas contribuições dos componentes. Propriedades de equilíbrio como anisotropia efetiva e intensidades integradas são determinadas para as misturas e líquidos puros. Os resultados indicam que a correlação orientacional entre as moléculas de CS2 é a maior responsável pelos espectros Rayleigh e espectros Raman deste componente. A maior participação do benzeno ocorre no espalhamento Raman com uma maior correlação na contribuição induzida por colisão. A separação na escala temporal entre a dinâmica reorientacional e induzida por colisão determinada na simulação é menor em relação ao experimento. Tempos de correlação da contribuição induzida por colisão são maiores nas misturas em relação aos líquidos puros no espalhamento Rayleigh, conforme o experimento, e estão relacionados a contribuições significativas das funções de correlação entre os componentes CS2 e benzeno a tempo longo.  Funções de correlação temporal do momento dipolar coletivo são determinadas a partir do modelo dipolo induzido por quadrupolo e analisadas em termos de contribuições isotrópicas, anisotrópicas e cruzadas, como também de contribuições dos componentes. A relação entre estes termos é concordante com os parâmetros moleculares de anisotropia e quadrupolo dos componentes. Os resultados da simulação indicam absorbância de excesso para as misturas em uma ordem semelhante a do experimento. O tempo de correlação maior na mistura de fração molar de benzeno igual a 0,25, também encontrado no experimento, está relacionado a elevada contribuição a tempo longo da função de correlação entre dipolos induzidos em moléculas de CS2 por moléculas de C6H6. Os espectros calculados na simulação e os espectros experimentais do espalhamento de luz despolarizado Rayleigh apresentam boa concordância, validando os modelos de potenciais de interação e de indução usados. No entanto, diferenças são encontradas no infravermelho longínquo. Modi cações no modelo simplificado de indução são propostas.|http://hdl.handle.net/10183/4107
Sistema de detecção de intrusão baseado em métodos estatísticos para análise de comportamento|2003|Open Access|Tese|Redes : Computadores;Gerencia : Redes : Computadores;Seguranca : Redes : Computadores|por|A segurança no ambiente de redes de computadores é um elemento essencial para a proteção dos recursos da rede, dos sistemas e das informações. Os mecanismos de segurança normalmente empregados são criptografia de dados, firewalls, mecanismos de controle de acesso e sistemas de detecção de intrusão. Os sistemas de detecção de intrusão têm sido alvo de várias pesquisas, pois é um mecanismo muito importante para monitoração e detecção de eventos suspeitos em um ambiente de redes de computadores. As pesquisas nessa área visam aprimorar os mecanismos de detecção de forma a aumentar a sua eficiência. Este trabalho está focado na área de detecção de anomalias baseada na utilização de métodos estatísticos para identificar desvios de comportamento e controlar o acesso aos recursos da rede. O principal objetivo é criar um mecanismo de controle de usuários da rede, de forma a reconhecer a legitimidade do usuário através de suas ações. O sistema proposto utilizou média e desvio padrão para detecção de desvios no comportamento dos usuários. Os resultados obtidos através da monitoração do comportamento dos usuários e aplicação das medidas estatísticas, permitiram verificar a sua validade para o reconhecimento dos desvios de comportamento dos usuários. Portanto, confirmou-se a hipótese de que estas medidas podem ser utilizadas para determinar a legitimidade de um usuário, bem como detectar anomalias de comportamento.  As análises dos resultados de média e desvio padrão permitiram concluir que, além de observar os seus valores estanques, é necessário observar o seu comportamento, ou seja, verificar se os valores de média e desvio crescem ou decrescem. Além da média e do desvio padrão, identificou-se também a necessidade de utilização de outra medida para refletir o quanto não se sabe sobre o comportamento de um usuário. Esta medida é necessária, pois a média e o desvio padrão são calculados com base apenas nas informações conhecidas, ou seja, informações registradas no perfil do usuário. Quando o usuário faz acessos a hosts e serviços desconhecidos, ou seja, não registrados, eles não são representados através destas medidas. Assim sendo, este trabalho propõe a utilização de uma medida denominada de grau de desconhecimento, utilizada para medir quantos acessos diferentes do seu perfil o usuário está realizando. O sistema de detecção de anomalias necessita combinar as medidas acima descritas e decidir se deve tomar uma ação no sistema. Pra este fim, propõe-se a utilização de sistemas de regras de produção e lógica fuzzy, que permitem a análise das medidas resultantes e execução do processo de decisão que irá desencadear uma ação no sistema. O trabalho também discute a integração do sistema de detecção de intrusão proposto à aplicação de gerenciamento SNMP e ao gerenciamento baseado em políticas.|http://hdl.handle.net/10183/4112
Gerência de mudanças de requisitos: uma proposta de aplicação a um estudo de caso|2004|Open Access|Dissertação|Requisitos : Software;Desenvolvimento : Software;Qualidade : Software|por|As organizações desenvolvedoras de software, na sua maioria, têm grande dificuldade de identificar e adotar um processo adequado de gestão de mudanças de requisitos. Durante todo o ciclo de vida de desenvolvimento de um software existem inúmeras solicitações de mudanças de escopo e de requisitos (técnicos ou não). Isso provoca muitos transtornos aos projetos e aos envolvidos. Por isso, há uma necessidade determinante de que essas organizações utilizem um processo adequado de acompanhamento e de controle de requisitos. Com base nisso, este trabalho procura apresentar uma metodologia de gerenciamento de mudanças dos requisitos, desde a base conceitual, que envolve os requisitos ( tipos, problemas, técnicas de elicitação e visão geral sobre modelo de gerência de requisitos), até a aplicação da metodologia proposta em um estudo de caso. O desenvolvimento deste trabalho teve como objetivo principal desenvolver a estrutura de uma metodologia que fosse de fácil aplicação nas organizações, dando uma noção de como devem ser gerenciadas as mudanças de requisitos, sua documentação, os modelos de documentos a serem utilizados e um exemplo prático de aplicação da metodologia.|http://hdl.handle.net/10183/4118
APSEE-Monitor: um mecanismo de apoio a visualização de modelos de processos de software|2004|Open Access|Dissertação|Reutilizacao : Software;Processo : Software|por|A tecnologia de processos de desenvolvimento de software ´e uma importante área de estudo e pesquisas na Engenharia de Software que envolve a construção de ferramentas e ambientes para modelagem, execução, simulação e evolução de processos de desenvolvimento de software, conhecidos como PSEEs (do inglês: Process-Centered Software Engineering Environments). Um modelo de processo de software é uma estrutura complexa que relaciona elementos gerenciáveis (i.e. artefatos, agentes, e atividades) que constituem o processo de software. Esta complexidade, geralmente, dificulta a percepção e entendimento do processo por parte dos profissionais envolvidos, principalmente quando estes profissionais têm acesso apenas a uma visão geral do modelo. Desta forma, há necessidade de mecanismos para visualização e acompanhamento dos processos, fornecendo informações adequadas aos diferentes estados, abstraindo as informações relevantes tanto as fases presentes no processo de desenvolvimento quanto ao agente envolvido, além de facilitar a interação e o entendimento humano sobre os elementos do processo. Estudos afirmam que a maneira como são apresentadas as informações do modelo de processo pode influenciar no sucesso ou não do desenvolvimento do software, assim como facilitar a adoção da tecnologia pela indústria de software.  Este trabalho visa contribuir nas pesquisas que buscam mecanismos e cientes para a visualização de processos de software apresentando a abordagem APSEE-Monitor destinada ao apoio a visualização de processos de software durante a sua execução. O principal objetivo desta pesquisa é apresentar um modelo formal de apoio a visualização de processos capaz de extrair dados de processos e organizá-los em sub-domínios de informações de interesse do gerente de processos. Neste trabalho aplica-se o conceito de múltiplas perspectivas como uma estratégia viável para a abstração e organização das informações presentes no modelo de processos. A solução proposta destaca-se por estender a definção original de perspectivas e fornecer uma estratégia de extração dos dados através de uma especificação formal utilizando o paradigma PROSOFT-Algébrico. Além disso, o trabalho apresenta um conjunto de requisitos relativos a interação entre gerentes de processos e PSEEs, a definição formal das perspectivas, uma gramática que define a linguagem de consulta aos processos, e um protótipo da aplicação.|http://hdl.handle.net/10183/4119
GROA: um gerenciador de repositórios de objetos de aprendizagem|2004|Open Access|Dissertação|Informática : Educação;Ensino a distância;Internet|por|Um conceito recente, relacionado à tecnologia educacional, baseia-se na idéia de objetos de aprendizagem (OAs), entendidos como pequenos componentes que podem ser usados, reusados ou referenciados durante a aprendizagem suportada pela tecnologia. Paralelo a isto, várias organizações estão envolvidas num trabalho de desenvolvimento de padrões de metadados para estes objetos, a fim de facilitar a catalogação e recuperação dos mesmos. Desta forma, os OAs podem ser localizados mais facilmente e utilizados em diferentes contextos e plataformas e por diferentes pessoas. O que se propõe para atingir esta facilidade de uso dos OAs é que os objetos sejam armazenados em bases de dados que são também conhecidas como repositórios, que fornecem ao usuário vários benefícios em termos de recuperação de informações. Neste contexto, este trabalho apresenta o GROA - Gerenciador de Repositórios de Objetos de Aprendizagem, que disponibiliza recursos de criação de repositórios capazes de armazenamento, gerenciamento, indexação e estruturação de objetos de aprendizagem, e capazes de operar como serviços deWeb, na internet. Este sistema foi implementado no Zope, que utiliza um banco de dados orientado a objetos, integrado a um servidor web. O texto analisa o conceito de OA e o contextualiza em relação a questões como a educação a distância, ambientes de apoio ao ensino e reusabilidade de conteúdos. Também, detalha os padrões de metadados que permitem a inserção dos OAs como componentes da Web Semântica. Em particular, apresenta-se o mecanismo de mapas de tópicos utilizado para estruturar os repositórios de OAs gerenciados pelo GROA. Finalmente, o texto discorre sobre os detalhes da implementação do GROA.|http://hdl.handle.net/10183/4120
Variabilidade espaço-temporal dos deslocamentos da linha de costa no Rio Grande do Sul|2004|Open Access|Tese|Geologia marinha;Erosao|por| O comportamento antagônico dos deslocamentos anuais da linha de costa coincide com os eventos de ENSO. Foi observado também que a linha de costa tende a retornar a sua forma e posição anteriores, sazonalmente no litoral sul, anualmente no litoral médio e a cada 19 meses no litoral norte. A variabilidade espacial na resposta da linha de costa às mudanças sazonais e interanuais deve-se a uma combinação de fatores, incluindo granulometria, orientação da linha de costa e transporte sedimentar ao longo da costa. A análise regional da costa do RS permitiu classificá-la em quatro classes de manejo: (1) áreas de manejo crítico, ocorrem em 177 km ou 29% da costa do RS e consistem basicamente nas áreas urbanizadas, concentradas principalmente no litoral norte, (2) áreas prioritárias, ocorrem em 198 km ao longo do litoral médio, ocupando 32% da costa do RS, (3) áreas latentes ocorrem em 65 km ou 10% da costa, localizados no litoral sul entre o Hermenegildo e o Albardão e (4) áreas naturais, ao longo de 178 km ou 29% da costa gaúcha, encontradas no litoral central e sul.|http://hdl.handle.net/10183/4128
Extensão do framework geoframe para modelagem de processos de análise geográfica|2003|Open Access|Dissertação|Banco : Dados geograficos;Sistemas : Informacao geografica;Framework;Geoinformática|por|As particularidades encontradas na modelagem de bancos de dados geográficos tornam necessário o desenvolvimento de modelos específicos. A totalidade dos modelos desenvolvidos oferece recursos para a modelagem de aspectos estáticos. Alguns dos modelos apresentam soluções parciais para a modelagem de aspectos dinâmicos. A possibilidade de executar processos de análise geográfica, que alteram o estado dos componentes do banco de dados geográficos é, de forma geral, a maior motivação para justificar os investimentos necessários para a sua construção. A formalização desses processos em um modelo conceitual, na fase de projeto, torna desnecessário o uso da terminologia específica que cada software de SIG emprega. A solução desenvolvida estende um framework conceitual (GeoFrame) com uma semântica que suporta a expressão de processos de análise geográfica, mantendo compatibilidade com a linguagem UML.  Para utilizar de forma adequada os recursos do framework, uma metodologia para a elaboração do modelo do usuário é indicada. Nessa metodologia, os processos são identificados a partir da elaboração de diagramas de casos de uso e atividades, incorporados no diagrama de classes e detalhados através de diagramas de atividades contendo ações. Um levantamento sobre operações utilizadas em processos de análise geográfica abrangendo a visão conceitual, lógica e de implementação de vários autores levou à construção de um catálogo de operações geográficas. Essas operações foram modeladas utilizando os elementos de modelagem de comportamento da especificação da UML, versão 2.0. O conjunto de recursos oferecidos nesse trabalho proporciona ao projetista de bancos de dados geográficos o desenvolvimento de uma especificação em alto nível e abrangente, utilizando a linguagem UML, reconhecida como padrão em modelagem de sistemas.|http://hdl.handle.net/10183/4131
ALOI : um agente para a localização e organização de informações|2003|Open Access|Dissertação|Inteligência artificial;Agentes inteligentes;Internet|por|Este trabalho é um estudo sobre agentes inteligentes e suas aplicações na Internet. São apresentados e comparados alguns exemplos de software com funcionalidades para extrair, selecionar e auxiliar no consumo de informações da Internet, com base no perfil de interesse de cada usuário. O objetivo principal deste trabalho é a proposição de um modelo geral e amplo de agente para a obtenção e manutenção de um repositório de links para documentos que satisfaçam o interesse de um ou mais usuários. O modelo proposto baseia-se na obtenção do perfil do usuário a partir de documentos indicados como modelos positivos ou negativos. O ponto forte do modelo são os módulos responsáveis pela extração de informações da Internet, seleção quanto a importância e armazenamento em banco de dados das URLs obtidas, classificadas quanto a usuário, categoria de assunto e assunto. Além disso, o modelo prevê a realização de freqüentes verificações de integridade e pertinência dos links armazenados no repositório. Com base no modelo proposto foi implementado um protótipo parcial. Tal protótipo contempla os módulos responsáveis pela obtenção de informações, seleção das informações pertinentes e classificação e armazenamento dos links de acordo com o assunto. Finalmente, o protótipo implementado permaneceu em execução por um determinado período, gerando alguns resultados preliminares que viabilizaram uma avaliação do modelo.|http://hdl.handle.net/10183/4132
E-Avalia : um agente para avaliação de ensino-aprendizagem em educação a distância|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância|por|Este trabalho tem por objetivo a construção e a prototipação de um modelo de avaliação de alunos no ensino a distância, utilizando a tecnologia de agentes. Para tal, foram utilizados os conceitos de avaliação formativa descritos por Bloom em [BLO83]. O agente para avaliação desenvolvido neste trabalho foi denominado E-Avalia, no intuito de expressar a avaliação do Ensino-Aprendizagem. O agente E-Avalia permite a criação de uma estrutura de competências e habilidades que compõe um curso. Desta forma, os alunos são avaliados qualitativamente de acordo com as habilidades pré-definidas pelo professor para cada competência do curso. As avaliações são realizadas ao término de cada unidade de ensino, que corresponde a uma competência do curso, essas por sua vez são compostas por questões de vários tipos, cada qual com uma habilidade associada, ou seja, um objetivo específico. O relatório entregue ao aluno como resultado da sua avaliação é descritivo e contém as habilidades atingidas e as habilidades não atingidas acompanhadas das prescrições necessárias para atingi-las. Já os relatórios disponíveis para o professor mostram além dos dados qualitativos e dados quantitativos permitindo que sejam tomadas decisões gerenciais em função dos resultados como um todo. Percebe-se, nos sistemas de ensino a distância existentes, que os modelos de avaliação tem como objetivo principal fornecer dados classificatórios dos alunos, através de avaliações somativas.  A avaliação formativa ao contrário da avaliação somativa tem o propósito de fornecer dados qualitativos em relação à aprendizagem dos alunos, constatar se os objetivos estabelecidos foram alcançados pelos alunos e fornecer dados para aperfeiçoar o processo ensino-aprendizagem. O agente E-Avalia foi desenvolvido para ser integrado ao ambiente SEMEAI – SistEma Multiagente de Ensino Aprendizagem na Internet. O processo de integração dos agentes do SEMEAI é de responsabilidade do agente Tutor, que tem como função gerenciar os eventos ocorridos dentro do ambiente e encaminhar as ações aos agentes responsáveis. O agente Seleciona Estratégia, desenvolvido em [PER99] e utilizado no SEMEAI, tem a função de selecionar uma estratégia de ensino baseado no perfil do aluno. A avaliação formativa foi utilizada com o intuito de fornecer resultados que provoquem a mudança da estratégia utilizada no ambiente de ensino, a fim de melhorar o ensino e a aprendizagem dos alunos. Foram realizados dois estudos de caso para validar o agente E-Avalia. O primeiro estudo de caso foi realizado com empregados da instituição Emater através do ambiente TelEduc, no qual foi proposta a avaliação dos objetivos de uma unidade do curso StarOffice. O segundo estudo de caso foi realizado com alunos de ensino médio e técnico do Centro Federal de Educação Tecnológica de Pelotas – CEFET-RS, no qual foi proposta a avaliação dos objetivos de uma unidade do curso de Informática Básica. A principal diferença entre os dois estudos de caso é que o primeiro foi realizado totalmente a distância, sem contato com os alunos, e o segundo foi realizado com o 12 auxílio do professor da disciplina em sala de aula. Os resultados dos estudos de caso estão no capítulo cinco deste trabalho.|http://hdl.handle.net/10183/4134
Monitoramento personalizado de hiperdocumentos como apoio à avaliação em um ambiente de ensino-aprendizagem na Web|2003|Open Access|Dissertação|Informática : Educação;Ensino : Aprendizagem;Ensino a distância;Hiperdocumento|por|Este trabalho realiza um estudo sobre a avaliação no processo de ensino-aprendizagem, aplicados pelos ambientes de educação a distância na Web. Foram analisados ambientes disponíveis no mercado e buscado subsídios em pesquisas já formuladas. Alguns ambientes são exclusivamente direcionados para a avaliação enquanto outros abrangem todo o processo de ensino-aprendizagem. Com base nestas análises foi proposto um modelo de monitoramento sobre os documentos dispostos em um ambiente de ensino-aprendizagem na Web. Para viabilizar a proposta foi implementado a integração e adaptação de ambientes de ensino-aprendizagem e avaliação, para que através deste ambiente final fosse possível avaliar o modelo proposto. A ênfase do processo de acompanhamento do aluno foi sobre o conteúdo que as ferramentas de avaliação formal e informal dispõem no ambiente. Para detectar possíveis problemas no processo de ensino-aprendizagem é necessário monitorar uma grande quantidade de dados. A análise dos dados para a avaliação exige um considerado esforço, para reunião e consolidação desses dados. Neste processo nota-se uma grande sobrecarga de trabalho para os professores, devido a grande quantidade de dados a serem acompanhados. Este fato é um dos grandes problemas do processo de avaliação em ambientes de ensino, seja presencial ou à distância. Este trabalho visa minimizar e flexibilizar este problema.|http://hdl.handle.net/10183/4135
Investigação e aplicação de operações categoriais entre atores de animações baseadas em autômatos finitos com saída|2004|Open Access|Dissertação|Automatos finitos;Teoria : Categorias|por|O presente trabalho apresenta uma investigação sobre algumas operações categoriais baseadas em grafos e a aplicação das mesmas a uma área específica da Ciência da Computação, a saber, animações computacionais baseadas em autômatos finitos com saída. As operações categoriais estudadas neste trabalho são: Produto, Coproduto, Soma Amalgamada e Produto Fibrado. O modelo AGA (Animação Gráfica baseada em Autômatos finitos) foi o escolhido para ser utilizado como base desta dissertação. Inspirado nestes estudos, o trabalho contém uma proposta de como aplicar tais operações com o objetivo de definir animações aparentemente complexas, de forma simples, precisa e de fácil implementação. O enfoque está baseado em J. Stoy que diz que um dos objetivos para o uso da semântica formal de teoria das categorias é “sugerir meios ou formas para o projetista desenvolver sistemas melhores, mais elegantes (“limpos”) e com descrições formais mais simples”. Entretanto, não é objetivo deste trabalho verificar se a utilização destas operações é ou não melhor do que a utilização de qualquer outra solução para criar novas animações.  Esta dissertação traz uma nova versão do modelo AGA, denominada AGANd (Animação Gráfica baseada em Autômatos finitos Não determinísticos), sendo que o AGA utiliza apenas autômatos finitos determinísticos para criar os atores de uma animação. Com a utilização do AGANd obtém-se animações mais realistas e mais flexíveis. A aplicação destas operações se dá nos dois modelos, os resultados obtidos a partir de cada uma das operações sobre os mesmos são apresentados de forma detalhada e ilustrados com os autômatos resultantes no decorrer do trabalho. É apresentada uma sugestão de implementação para cada uma das operações, visando estender o protótipo já implementado do modelo AGA. Isso faz com que o leitor seja estimulado a aplicar estas e outras operações categoriais em novas animações baseadas ou não nos modelos apresentados, despertando até mesmo para seu uso em outras áreas da Ciência da Computação.|http://hdl.handle.net/10183/4137
Uma Arquitetura reflexiva baseada na web para ambiente de suporte a processo|2002|Open Access|Tese|Engenharia : Software;Sistemas : Informacao distribuida;Internet;Reflexao computacional;Arquitetura reflexiva;Desenvolvimento : Software|por|A presente tese visa contribuir na construção de ambientes de desenvolvimento de software através da proposição de uma arquitetura reflexiva para ambiente de suporte a processo, nomeada WRAPPER (Webbased Reflective Architecture for Process suPport EnviRonment). O objetivo desta arquitetura é prover uma infra-estrutura para um ambiente de suporte a processo de software, integrando tecnologias da World Wide Web, objetos distribuídos e reflexão computacional. A motivação principal para esta arquitetura vem da necessidade de se obter maior flexibilidade na gerência de processo de software. Esta flexibilidade é obtida através do uso de objetos reflexivos que permitem a um gerente de processo obter informações e também alterar o processo de software de forma dinâmica. Para se obter um ambiente integrado, a arquitetura provê facilidades para a agregação de ferramentas CASE de plataformas e fabricantes diversos, mesmo disponibilizadas em locais remotos. A integração de ferramentas heterogêneas e distribuídas é obtida através do uso de tecnologias Web e de objetos distribuídos. Reflexão computacional é usada no ambiente tanto para extrair dados da execução do processo, quanto para permitir a adaptação do mesmo. Isto é feito através da introdução e controle de meta-objetos, no metanível da arquitetura, que podem monitorar e mesmo alterar os objetos do nível base.  Como resultado, a arquitetura provê as seguintes características: flexibilidade na gerência de processo, permitindo o controle e adaptação do processo; distribuição do ambiente na Web, permitindo a distribuição de tarefas do processo de software e a integração de ferramentas em locais remotos; e heterogeneidade para agregar componentes ao ambiente, permitindo o uso de ferramentas de plataformas e fornecedores diversos. Neste contexto, o presente trabalho apresenta a estrutura da arquitetura reflexiva, bem como os mecanismos usados (e suas interações) para a modelagem e execução de processo dentro do ambiente de suporte ao processo de software.|http://hdl.handle.net/10183/4138
A relação do espaço na evolução morfodinâmica do Manguezal do Itacorubi, Florianópolis,SC.|2004|Open Access|Tese|Geologia marinha;Morfodinâmica : Evolução;Manguezais;Itacorubi (Florianópolis, SC)|por|Este trabalho aborda o estudo de gerenciamento costeiro feito na área do Manguezal do Itacorubi, na Ilha de Santa Catarina, Estado de Santa Catarina, sul do Brasil. São apresentados dois enfoques, o primeiro discute o comportamento do ecossistema ao longo do século XX diante de diferentes fatores de estresse, concluindo que o manguezal, em 1998, havia perdido 13,32% de área em relação a 1938, porém, nesse mesmo período, progradou aproximadamente 80 m sobre a plataforma continental adjacente, o que indica que o manguezal, apesar de sitiado por uma vizinhança densamente povoada, tem capacidade regenerativa e de avançar em direção do oceano. O segundo enfoque aborda os aspectos geológicos, relacionando os depósitos sedimentares encontrados nesse embaiamento costeiro e as variações holocênicas do nível do mar, constatando que o mar já cobriu essa área em tempos recentes, deixando como testemunho espessos pacotes sedimentares que se apóiam diretamente sobre o embasamento cristalino da região.  Dados de testemunhos de sondagem auxiliaram no mapeamento de subsuperfície, indicando a ocorrência de um vale inciso no embasamento, o qual teria servido de acesso preferencial durante o evento transgressivo, guardando os sedimentos de maior granulometria, associados a uma região de maior dinâmica, enquanto nas áreas adjacentes, mais rasas, as granulometrias tendem para as frações mais finas. Sobre esse pacote essencialmente clástico estabeleceu-se a sedimentação carbonosa do manguezal. As variações sedimentares laterais são muito notáveis mesmo naqueles furos relativamente próximos (menos de 20 m), evidenciando a dificuldade que seria a construção de um mapa defácies de uma área de pântano parálico. O bosque de mangue existe nessa área há pelo menos 4,5 Ka, afirmação apoiada sobre os resultados obtidos com a datação de 14C sobre amostra de sedimentos carbonosos.|http://hdl.handle.net/10183/4139
Uma fundamentação teórica para a complexidade estrutural de problemas de otimização|2002|Open Access|Tese|Teoria : Ciência : Computação;Teoria : Categorias;Teoria : Complexidade|por|Com o objetivo de desenvolver uma fundamentação teórica para o estudo formal de problemas de otimização NP-difíceis, focalizando sobre as propriedades estruturais desses problemas relacionadas à questão da aproximabilidade, este trabalho apresenta uma abordagem semântica para tratar algumas questões originalmente estudadas dentro da Teoria da Complexidade Computacional, especificamente no contexto da Complexidade Estrutural. Procede-se a uma investigação de interesse essencialmente teórico, buscando obter uma formalização para a teoria dos algoritmos aproximativos em dois sentidos. Por um lado, considera-se um algoritmo aproximativo para um problema de otimização genérico como o principal objeto de estudo, estruturando-se matematicamente o conjunto de algoritmos aproximativos para tal problema como uma ordem parcial, no enfoque da Teoria dos Domínios de Scott. Por outro lado, focaliza-se sobre as reduções entre problemas de otimização, consideradas como morfismos numa abordagem dentro da Teoria das Categorias, onde problemas de otimização e problemas aproximáveis são os objetos das novas categorias introduzidas. Dentro de cada abordagem, procura-se identificar aqueles elementos universais, tais como elementos finitos, objetos totais, problemas completos para uma classe, apresentando ainda um sistema que modela a hierarquia de aproximação para um problema de otimização NP-difícil, com base na teoria categorial da forma.  Cada uma destas estruturas matemáticas fornecem fundamentação teórica em aspectos que se complementam. A primeira providencia uma estruturação interna para os objetos, caracterizando as classes de problemas em relação às propriedades de aproximabilidade de seus membros, no sentido da Teoria dos Domínios, enquanto que a segunda caracteriza-se por relacionar os objetos entre si, em termos de reduções preservando aproximação entre problemas, num ponto de vista externo, essencialmente categorial.|http://hdl.handle.net/10183/4144
Geração de modelos de co-simulação distribuída para a arquitetura DCB|2003|Open Access|Dissertação|Sistemas digitais;Sistemas embarcados;Simulacao distribuida|por|O aumento na complexidade dos sistemas embarcados, compostos por partes de hardware e software, aliado às pressões do mercado que exige novos produtos em prazos cada vez menores, tem levado projetistas a considerar a possibilidade de construir sistemas a partir da integração de componentes já existentes e previamente validados. Esses componentes podem ter sido desenvolvidos por diferentes equipes ou por terceiros e muitas vezes são projetados utilizando diferentes metodologias, linguagens e/ou níveis de abstração. Essa heterogeneidade torna complexo o processo de integração e validação de componentes, que normalmente é realizado através de simulação. O presente trabalho especifica mecanismos genéricos e extensíveis que oferecem suporte à cooperação entre componentes heterogêneos em um ambiente de simulação distribuída, sem impor padrões proprietários para formatos de dados e para a descrição do comportamento e interface dos componentes. Esses mecanismos são baseados na arquitetura DCB (Distributed Co-Simulation Backbone), voltada para co-simulação distribuída e heterogênea e inspirada nos conceitos de federado (componente de simulação) e federação (conjunto de componentes) que são definidos pelo HLA (High Level Architecture), um padrão de interoperabilidade para simulações distribuídas.  Para dar suporte à co-simulação distribuída e heterogênea, esse trabalho descreve mecanismos que são responsáveis pelas tarefas de cooperação e distribuição, chamados de embaixadores, assim como o mecanismo gateway, que é responsável pela interoperabilidade entre linguagens e conversão de tipos de dados. Também é apresentada uma ferramenta de suporte à geração das interfaces de co-simulação, que são constituídas de dois embaixadores configuráveis e um gateway para cada federado, gerado a partir de templates pré-definidos.|http://hdl.handle.net/10183/4161
Geração de regras de extração de dados em páginas HTML|2003|Open Access|Dissertação|Armazenamento : Dados;Recuperacao : Informacao;Dados semi-estruturados;HTML (Linguagem de marcação);Extracao : Dados|por|Existem vários trabalhos na área de extração de dados semi-estruturados, usando diferentes técnicas. As soluções de extração disponibilizadas pelos trabalhos existentes são direcionadas para atenderem a dados de certos domínios, considerando-se domínio o conjunto de elementos pertencentes à mesma área de interesse. Dada a complexidade e a grande quantidade dos dados semi-estruturados, principalmente dos disponíveis na World Wide Web (WWW), é que existem ainda muitos domínios a serem explorados. A maior parte das informações disponíveis em sites da Web está em páginas HTML. Muitas dessas páginas contêm dados de certos domínios (por exemplo, remédios). Em alguns casos, sites de organizações diferentes apresentam dados referentes a um mesmo domínio (por exemplo, farmácias diferentes oferecem remédios). O conhecimento de um determinado domínio, expresso em um modelo conceitual, serve para definir a estrutura de um documento. Nesta pesquisa, são consideradas exclusivamente tabelas de páginas HTML. A razão de se trabalhar somente com tabelas está baseada no fato de que parte dos dados de páginas HTML encontra-se nelas, e, como conseqüência, elimina-se o processamento dos outros dados, concentrando-se os esforços para que sejam processadas automaticamente.  A pesquisa aborda o tratamento exclusivo de tabelas de páginas HTML na geração das regras de extração, na utilização das regras e do modelo conceitual para o reconhecimento de dados em páginas semelhantes. Para essa técnica, foi implementado o protótipo de uma ferramenta visual denominado Gerador de Regras de Extração e Modelo Conceitual (GREMO). GREMO foi desenvolvido em linguagem de programação visual Delphi 6.0. O processo de extração ocorre em quatro etapas: identificação e análise das tabelas de informações úteis em páginas HTML; identificação de conceitos para os elementos dos modelos conceituais; geração dos modelos conceituais correspondentes à página, ou utilização de modelo conceitual existente no repositório que satisfaça a página em questão; construção das regras de extração, extração dos dados da página, geração de arquivo XML correspondente aos dados extraídos e, finalmente, realimentação do repositório. A pesquisa apresenta as técnicas para geração e extração de dados semi-estruturados, as representações de domínio exclusivo de tabelas de páginas HTML por meio de modelo conceitual, as formas de geração e uso das regras de extração e de modelo conceitual.|http://hdl.handle.net/10183/4163
Réplicas para alta disponibilidade em arquiteturas orientadas a componentes com suporte de comunicação de grupo|2003|Open Access|Tese|Confiabilidade : Computadores;Tolerancia : Falhas;Comunicacao : Grupos;Sistemas distribuídos;Replicacao : Dados|por|Alta disponibilidade é uma das propriedades mais desejáveis em sistemas computacionais, principalmente em aplicações comerciais que, tipicamente, envolvem acesso a banco de dados e usam transações. Essas aplicações compreendem sistemas bancários e de comércio eletrônico, onde a indisponibilidade de um serviço pode representar substanciais perdas financeiras. Alta disponibilidade pode ser alcançada através de replicação. Se uma das réplicas não está operacional, outra possibilita que determinado serviço seja oferecido. No entanto, réplicas requerem protocolos que assegurem consistência de estado. Comunicação de grupo é uma abstração que tem sido aplicada com eficiência a sistemas distribuídos para implementar protocolos de replicação. Sua aplicação a sistemas práticos com transações e com banco de dados não é comum. Tipicamente, sistemas transacionais usam soluções ad hoc e sincronizam réplicas com protocolos centralizados, que são bloqueantes e, por isso, não asseguram alta disponibilidade. A tecnologia baseada em componentes Enterprise JavaBeans (EJB) é um exemplo de sistema prático que integra distribuição, transações e bancos de dados. Em uma aplicação EJB, o desenvolvedor codifica o serviço funcional que é dependente da aplicação, e os serviços não–funcionais são inseridos automaticamente. A especificação EJB descreve serviços não–funcionais de segurança, de transações e de persistência para bancos de dados, mas não descreve serviços que garantam alta disponibilidade. Neste trabalho, alta disponibilidade é oferecida como uma nova propriedade através da adição de serviços não–funcionais na tecnologia EJB usando abstrações de comunicação de grupo.  Os serviços para alta disponibilidade são oferecidos através da arquitetura HA (highly-available architecture) que possui múltiplas camadas. Esses serviços incluem replicação, chaveamento de servidor, gerenciamento de membros do grupo e detecção de membros falhos do grupo. A arquitetura HA baseia-se nos serviços já descritos pela especificação EJB e preserva os serviços EJB existentes. O protocolo de replicação corresponde a uma subcamada, invisível para o usuário final. O serviço EJB é executado por membros em um grupo de réplicas, permitindo a existência de múltiplos bancos de dados idênticos. Conflitos de acesso aos múltiplos bancos de dados são tratados estabelecendo–se uma ordem total para aplicação das atualizações das transações. Esse grupo é modelado como um único componente e gerenciado por um sistema de comunicação de grupo. A combinação de conceitos de bancos de dados com comunicação de grupo demonstra uma interessante solução para aplicações com requisitos de alta disponibilidade, como as aplicações EJB. Os serviços adicionais da arquitetura HA foram implementados em protótipo. A validação através de um protótipo possibilita que experimentos sejam realizados dentro de um ambiente controlado, usando diferentes cargas de trabalho sintéticas. O protótipo combina dois sistemas de código aberto. Essa característica permitiu acesso à implementação e não somente à interface dos componentes dos sistemas em questão. Um dos sistemas implementa a especificação EJB e outro implementa o sistema de comunicação de grupos. Os resultados dos testes realizados com o protótipo mostraram a eficiência da solução proposta. A degradação de desempenho pelo uso de réplicas e da comunicação de grupo é mantida em valores adequados.|http://hdl.handle.net/10183/4165
Gerenciamento de processos em controladores programáveis usando XML|2003|Open Access|Dissertação|XML (Linguagem de marcação);Controlador programável;Automação industrial|por|Os controladores programáveis tornaram-se fator decisivo para o controle de processos em ambientes industriais. Permitir o gerenciamento desses, eleva-os ao mesmo grupo de outros equipamentos da rede. Gerenciar esses dispositivos e o processo controlado tende a facilitar a identificação de falhas, as intervenções no processo e demais vantagens trazidas por um bom esquema de gerência. Uma maneira de realizar esse gerenciamento é por meio de programas conhecidos como supervisórios. Além das aplicações de supervisão, uma nova classe de ferramentas, que também possibilita o gerenciamento tanto do controlador quanto do processo a esse submetido, vem sendo disponibilizada. A presença de protocolos de gerenciamento, tal qual o SNMP, já é uma realidade em alguns dos modelos de equipamentos oferecidos por vários fabricantes, promovendo uma integração com plataformas de gerência já existentes no mercado. A proposta deste trabalho inclui a elaboração de um modelo de gerenciamento usando XML, atualmente em ampla ascensão e aceitação. Está, também, previsto a construção de um protótipo capaz de realizar as funções de um agente. A criação de um modelo de gerenciamento baseado em tecnologias e especificações abertas, tais como XML e Web, possibilita desde a utilização de um navegador, até o desenvolvimento de novas ferramentas para a requisição/aquisição de dados em controladores.|http://hdl.handle.net/10183/4176
Redes-em-Chip : arquiteturas e modelos para avaliação de área e desempenho|2003|Open Access|Tese|Microeletrônica;Circuitos integrados|por|Com o advento dos processos submicrônicos, a capacidade de integração de transistores tem atingido níveis que possibilitam a construção de um sistema completo em uma única pastilha de silício. Esses sistemas, denominados sistemas integrados, baseiam-se no reuso de blocos previamente projetados e verificados, os quais são chamados de núcleos ou blocos de propriedade intelectual. Os sistemas integrados atuais incluem algumas poucas dezenas de núcleos, os quais são interconectados por meio de arquiteturas de comunicação baseadas em estruturas dedicadas de canais ponto-a-ponto ou em estruturas reutilizáveis constituídas por canais multiponto, denominadas barramentos. Os futuros sistemas integrados irão incluir de dezenas a centenas de núcleos em um mesmo chip com até alguns bilhões de transistores, sendo que, para atender às pressões do mercado e amortizar os custos de projeto entre vários sistemas, é importante que todos os seus componentes sejam reutilizáveis, incluindo a arquitetura de comunicação. Das arquiteturas utilizadas atualmente, o barramento é a única que oferece reusabilidade. Porém, o seu desempenho em comunicação e o seu consumo de energia degradam com o crescimento do sistema. Para atender aos requisitos dos futuros sistemas integrados, uma nova alternativa de arquitetura de comunicação tem sido proposta na comunidade acadêmica.  Essa arquitetura, denominada rede-em-chip, baseia-se nos conceitos utilizados nas redes de interconexão para computadores paralelos. Esta tese se situa nesse contexto e apresenta uma arquitetura de rede-em-chip e um conjunto de modelos para a avaliação de área e desempenho de arquiteturas de comunicação para sistemas integrados. A arquitetura apresentada é denominada SoCIN (System-on-Chip Interconnection Network) e apresenta como diferencial o fato de poder ser dimensionada de modo a atender a requisitos de custo e desempenho da aplicação alvo. Os modelos desenvolvidos permitem a estimativa em alto nível da área em silício e do desempenho de arquiteturas de comunicação do tipo barramento e rede-em-chip. São apresentados resultados que demonstram a efetividade das redes-em-chip e indicam as condições que definem a aplicabilidade das mesmas.|http://hdl.handle.net/10183/4179
Reuse-based test planning for core-based systems-on-chip;Planejamento de teste para sistemas de hardware integrados baseados em componentes virtuais |2003|Open Access|Tese|SoC testing;Testing of embedded cores;Design for test;Design space exploration;Network-on-chip;Microeletrônica;Testes : Circuitos integrados|eng|Electronic applications are currently developed under the reuse-based paradigm. This design methodology presents several advantages for the reduction of the design complexity, but brings new challenges for the test of the final circuit. The access to embedded cores, the integration of several test methods, and the optimization of the several cost factors are just a few of the several problems that need to be tackled during test planning. Within this context, this thesis proposes two test planning approaches that aim at reducing the test costs of a core-based system by means of hardware reuse and integration of the test planning into the design flow. The first approach considers systems whose cores are connected directly or through a functional bus. The test planning method consists of a comprehensive model that includes the definition of a multi-mode access mechanism inside the chip and a search algorithm for the exploration of the design space. The access mechanism model considers the reuse of functional connections as well as partial test buses, cores transparency, and other bypass modes. The test schedule is defined in conjunction with the access mechanism so that good trade-offs among the costs of pins, area, and test time can be sought. Furthermore, system power constraints are also considered. This expansion of concerns makes it possible an efficient, yet fine-grained search, in the huge design space of a reuse-based environment. Experimental results clearly show the variety of trade-offs that can be explored using the proposed model, and its effectiveness on optimizing the system test plan.  Networks-on-chip are likely to become the main communication platform of systemson- chip. Thus, the second approach presented in this work proposes the reuse of the on-chip network for the test of the cores embedded into the systems that use this communication platform. A power-aware test scheduling algorithm aiming at exploiting the network characteristics to minimize the system test time is presented. The reuse strategy is evaluated considering a number of system configurations, such as different positions of the cores in the network, power consumption constraints and number of interfaces with the tester. Experimental results show that the parallelization capability of the network can be exploited to reduce the system test time, whereas area and pin overhead are strongly minimized. In this manuscript, the main problems of the test of core-based systems are firstly identified and the current solutions are discussed. The problems being tackled by this thesis are then listed and the test planning approaches are detailed. Both test planning techniques are validated for the recently released ITC’02 SoC Test Benchmarks, and further compared to other test planning methods of the literature. This comparison confirms the efficiency of the proposed methods.;O projeto de sistemas eletrônicos atuais segue o paradigma do reuso de componentes de hardware. Este paradigma reduz a complexidade do projeto de um chip, mas cria novos desafios para o projetista do sistema em relação ao teste do produto final. O acesso aos núcleos profundamente embutidos no sistema, a integração dos diversos métodos de teste e a otimização dos diversos fatores de custo do sistema são alguns dos problemas que precisam ser resolvidos durante o planejamento do teste de produção do novo circuito. Neste contexto, esta tese propõe duas abordagens para o planejamento de teste de sistemas integrados. As abordagens propostas têm como principal objetivo a redução dos custos de teste através do reuso dos recursos de hardware disponíveis no sistema e da integração do planejamento de teste no fluxo de projeto do circuito. A primeira abordagem considera os sistemas cujos componentes se comunicam através de conexões dedicadas ou barramentos funcionais. O método proposto consiste na definição de um mecanismo de acesso aos componentes do circuito e de um algoritmo para exploração do espaço de projeto. O mecanismo de acesso prevê o reuso das conexões funcionais, o uso de barramentos de teste locais, núcleos transparentes e outros modos de passagem do sinal de teste. O algoritmo de escalonamento de teste é definido juntamente com o mecanismo de acesso, de forma que diferentes combinações de custos sejam exploradas. Além disso, restrições de consumo de potência do sistema podem ser consideradas durante o escalonamento dos testes. Os resultados experimentais apresentados para este método mostram claramente a variedade de soluções que podem ser exploradas e a efi- ciência desta abordagem na otimização do teste de um sistema complexo.  A segunda abordagem de planejamento de teste propõe o reuso de redes em-chip como mecanismo de acesso aos componentes dos sistemas construídos sobre esta plataforma de comunicação. Um algoritmo de escalonamento de teste que considera as restrições de potência da aplicação é apresentado e a estratégia de teste é avaliada para diferentes configurações do sistema. Os resultados experimentais mostram que a capacidade de paralelização da rede em-chip pode ser explorada para reduzir o tempo de teste do sistema, enquanto os custos de área e pinos de teste são drasticamente minimizados. Neste manuscrito, os principais problemas relacionados ao teste dos sistemas integrados baseados em componentes virtuais são identificados e as soluções já apresentadas na literatura são discutidas. Em seguida, os problemas tratados por este traballho são listados e as abordagens propostas são detalhadas. Ambas as técnicas são validadas através dos sistemas disponíveis no ITC’02 SoC Test Benchmarks. As técnicas propostas são ainda comparadas com outras abordagens de teste apresentadas recentemente. Esta comparação confirma a eficácia dos métodos desenvolvidos nesta tese.|http://hdl.handle.net/10183/4180
Designing single event upset mitigation techniques for large SRAM-Based FPGA components;Desenvolvimento de técnicas de tolerância a falhas transientes em componentes programáveis por SRAM |2003|Open Access|Tese|Microeletrônica;FPGA;Tolerancia : Falhas;Fault tolerance;Single event upset;Fault injection;Time and hardware redundancy|eng|This thesis presents the study and development of fault-tolerant techniques for programmable architectures, the well-known Field Programmable Gate Arrays (FPGAs), customizable by SRAM. FPGAs are becoming more valuable for space applications because of the high density, high performance, reduced development cost and re-programmability. In particular, SRAM-based FPGAs are very valuable for remote missions because of the possibility of being reprogrammed by the user as many times as necessary in a very short period. SRAM-based FPGA and micro-controllers represent a wide range of components in space applications, and as a result will be the focus of this work, more specifically the Virtex® family from Xilinx and the architecture of the 8051 micro-controller from Intel. The Triple Modular Redundancy (TMR) with voters is a common high-level technique to protect ASICs against single event upset (SEU) and it can also be applied to FPGAs. The TMR technique was first tested in the Virtex® FPGA architecture by using a small design based on counters. Faults were injected in all sensitive parts of the FPGA and a detailed analysis of the effect of a fault in a TMR design synthesized in the Virtex® platform was performed. Results from fault injection and from a radiation ground test facility showed the efficiency of the TMR for the related case study circuit. Although TMR has showed a high reliability, this technique presents some limitations, such as area overhead, three times more input and output pins and, consequently, a significant increase in power dissipation.  Aiming to reduce TMR costs and improve reliability, an innovative high-level technique for designing fault-tolerant systems in SRAM-based FPGAs was developed, without modification in the FPGA architecture. This technique combines time and hardware redundancy to reduce overhead and to ensure reliability. It is based on duplication with comparison and concurrent error detection. The new technique proposed in this work was specifically developed for FPGAs to cope with transient faults in the user combinational and sequential logic, while also reducing pin count, area and power dissipation. The methodology was validated by fault injection experiments in an emulation board. The thesis presents comparison results in fault coverage, area and performance between the discussed techniques.;Esse trabalho consiste no estudo e desenvolvimento de técnicas de proteção a falhas transientes, também chamadas single event upset (SEU), em circuitos programáveis customizáveis por células SRAM. Os projetistas de circuitos eletrônicos estão cada vez mais predispostos a utilizar circuitos programáveis, conhecidos como Field Programmable Gate Array (FPGA), para aplicações espaciais devido a sua alta flexibilidade lógica, alto desempenho, baixo custo no desenvolvimento, rapidez na prototipação e principalmente pela reconfigurabilidade. Em particular, FPGAs customizados por SRAM são muito importantes para missões espaciais pois podem ser rapidamente reprogramados à distância quantas vezes for necessário. A técnica de proteção baseada em redundância tripla, conhecida como TMR, é comumente utilizada em circuitos integrados de aplicações específicas e pode também ser aplicada em circuitos programáveis como FPGAs. A técnica TMR foi testada no FPGA Virtex® da Xilinx em aplicações como contadores e micro-controladores. Falhas foram injetadas em todos as partes sensíveis da arquitetura e seus efeitos foram detalhadamente analisados. Os resultados de injeção de falhas e dos experimentos sob radiação em laboratório comprovaram a eficácia do TMR em proteger circuitos sintetizados em FPGAs customizados por SRAM. Todavia, essa técnica possui algumas limitações como aumento em área, uso de três vezes mais pinos de entrada e saída (E/S) e conseqüentemente, aumento na dissipação de potência.  Com o objetivo de reduzir custos no TMR e melhorar a confiabilidade, uma técnica inovadora de tolerância a falhas para FPGAs customizados por SRAM foi desenvolvida para ser implementada em alto nível, sem modificações na arquitetura do componente. Essa técnica combina redundância espacial e temporal para reduzir custos e assegurar confiabilidade. Ela é baseada em duplicação com um circuito comparador e um bloco de detecção concorrente de falhas. Esta nova técnica proposta neste trabalho foi especificamente projetada para tratar o efeito de falhas transientes em blocos combinacionais e seqüenciais na arquitetura reconfigurável, reduzir o uso de pinos de E/S, área e dissipação de potência. A metodologia foi validada por injeção de falhas emuladas em uma placa de prototipação. O trabalho mostra uma comparação nos resultados de cobertura de falhas, área e desempenho entre as técnicas apresentadas.|http://hdl.handle.net/10183/4181
Modelo de transmissão da dengue com competição larval uniforme|2004|Open Access|Dissertação|Matemática Aplicada : Ecologia;Epidemiologia : Modelos matematicos|por|A dengue representa um sério problema de saúde pública no Brasil, que apresenta condições climáticas favoráveis ao desenvolvimento e proliferação do Aedes aegypti, vetor transmissor da doeça. O mosquito Aedes aegypti passa por diferentes estágios de desenvolvi- mento com características distintas, logo, para uma descrição mais aproximada da história de vida desta espécie e, consequentemente, do comportamento da epidemia, considera-se neste trabalho um modelo com distribuição etária, fundamental para a determinação das propriedades de estabilidade de populações que têm fases distintas de desenvolvimento. O modelo com distribuição etária para a população de mosquitos é descrito por um conjunto de equações diferenciais ordinárias com retardo de difícil análise e implementação. Inicialmente, apresenta-se o modelo SEIR com dinâmica vital, onde a população de mosquitos estabiliza rapidamente e, após, incorpora-se a ele um modelo com competição larval uniforme para população de insetos, com distribuição etária, o que provoca um período de instabilidade nas populações de larvas e adultos, estágios de desenvolvimento considerados para o vetor. A análise realizada investiga as consequências que este período de instabilidade provoca na evolução da epidemia.|http://hdl.handle.net/10183/4204
Plataforma de comunicação tempo real sobre clusters SCI|2002|Open Access|Dissertação|Arquitetura de computadores;Cluster;Sistemas distribuídos;Sistemas : Tempo real;Sci|por|Devido a sua baixa latência de banda, os clusters equipados com o adaptador SCI são uma alternativa para sistemas de tempo real distribuídos. Esse trabalho apresenta o projeto e implementação de uma plataforma de comunicação de tempo real sobre clusters SCI. O hardware padrão do SCI não se mostra adequado para a transmissão de tráfego de tempo real devido ao problema da contenção de acesso ao meio que causa inversão de prioridade. Por isso uma disciplina de acesso ao meio é implementada como parte da plataforma. Através da arquitetura implementada é possível o estabelecimento de canais de comunicação com garantia de banda. Assim, aplicações multimídias, por exemplo, podem trocar com taxa constante de conunicação. Cada mensagem é enviada somente uma vez. Assim, mensagens som a semântica de eventos podem ser enviadas. Além disso, a ordem e o tamanho das mensagens são garantidos. Além do tráfego com largura de banda garantida, a plataforma possibilita a troca de pacotes IP entre diferentes máquinas do cluster. Esses pacotes são inseridos no campo de dados dos pacotes próprios da plataforma e após são enviados através do uso de pacotes IP. Além disso, essa funcionalidade da plataforma permite também a execução de bibliotecas de comunicação baseadas em TCP/IP como o MPI sobre o cluster SCI. A plataforma de comunicação é implementada como modulos do sistema operacional Linux com a execução de tempo real RTAI. A valiação da plataforma mostrou que mesmo em cenários com muita comunicação entre todos os nodos correndo, a largura de banda reservada para cada canal foi mantida.|http://hdl.handle.net/10183/4205
Particionamento de grafos de aplicações e mapeamento em grafos de arquiteturas heterogêneas|2002|Open Access|Dissertação|Arquitetura de computadores;Cluster;Teoria : Grafos|por|Esta pesquisa visa a modelagem de clusters de computadores, utilizando um modelo analítico simples que é representado por um grafo valorado denominado grafo da arquitetura. Para ilustrar tal metodologia, exemplificou-se a modelagem do cluster Myrinet/SCI do Instituto de Informática da UFRGS, que é do tipo heterogêneo e multiprocessado. A pesquisa visa também o estudo de métodos e tecnologias de software para o particionamento de grafos de aplicações e seu respectivo mapeamento sobre grafos de arquiteturas. Encontrar boas partições de grafos pode contribuir com a redução da comunicação entre processadores em uma máquina paralela. Para tal, utilizou-se o grafo da aplicação HIDRA, um dos trabalhos do GMCPAD, que modela o transporte de substâncias no Lago Guaíba. Um fator importante é o crescente avanço da oferta de recursos de alto desempenho como os clusters de computadores. Os clusters podem ser homogêneos, quando possuem um arquitetura com nós de mesma característica como: velocidade de processamento, quantidade de memória RAM e possuem a mesma rede de interconexão interligando-os. Eles também podem ser heterogêneos, quando alguns dos componentes dos nós diferem em capacidade ou tecnologia. A tendência é de clusters homogêneos se tornarem em clusters heterogêneos, como conseqüência das expansões e atualizações. Efetuar um particionamento que distribua a carga em clusters heterogêneos de acordo com o poder computacional de cada nó não é uma tarefa fácil, pois nenhum processador deve ficar ocioso e, tampouco, outros devem ficar sobrecarregados  Vários métodos de particionamento e mapeamento de grafos foram estudados e três ferramentas (Chaco, Jostle e o Scotch) foram testadas com a aplicação e com a arquitetura modeladas. Foram realizados, ainda, vários experimentos modificando parâmetros de entrada das ferramentas e os resultados foram analisados. Foram considerados melhores resultados aqueles que apresentaram o menor número de corte de arestas, uma vez que esse parâmetro pode representar a comunicação entre os processadores de uma máquina paralela, e executaram o particionamento/mapeamento no menor tempo. O software Chaco e o software Jostle foram eficientes no balanceamento de carga por gerarem partições com praticamente o mesmo tamanho, sendo os resultados adequados para arquiteturas homogêneas. O software Scotch foi o único que permitiu o mapeamento do grafo da aplicação sobre o grafo da arquitetura com fidelidade, destacando-se também por executar particionamento com melhor qualidade e pela execução dos experimentos em um tempo significativamente menor que as outras ferramentas pesquisadas.|http://hdl.handle.net/10183/4206
Uma Ferramenta para auxiliar na avaliação de textos construídos colaborativamente em ambientes de ensino-aprendizagem|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Trabalho cooperativo;Ensino-aprendizagem|por|Este trabalho está relacionado ao Ambiente Multiagente de Ensino-Aprendizagem (AME-A), no qual os agentes que o compõem preocupam-se em ensinar e/ou aprender. A ferramenta descrita neste trabalho baseia-se, em parte, na idéia do agente Promove_Interação, que tem por objetivo possibilitar que diversos aprendizes e professores se comuniquem, através da Internet e discutam assuntos determinados por um professor. Procurando auxiliar a tarefa do professor em determinar se os aprendizes estão realmente adquirindo conhecimento, desenvolveu-se uma ferramenta para analisar as interações dos aprendizes. O algoritmo desenvolvido utiliza um dicionário de palavras/frases-chaves relacionadas ao assunto em questão, referentes a tópicos que deveriam ser discutidos e/ou fazer parte das conclusões dos alunos. Ao ser ativado, o software identifica os aprendizes e suas respectivas interações e as armazena em uma base de dados; em seguida, avalia as interações de cada aprendiz, verificando a freqüência com que este utiliza as palavras-chave através de dois métodos de avaliação, baseados em algumas técnicas de KDT. O software permite também a classificação de todas as palavras/frases empregadas durante a reunião.|http://hdl.handle.net/10183/4207
Independência entre aplicações e processos modelados|2002|Open Access|Dissertação|Sistemas : Informação;Sistemas : Workflow|por|O presente trabalho expõe uma análise sobre duas arquiteturas disponíveis para o desenvolvimento de sistemas que utilizam a tecnologia de workflow. Estas arquiteturas são: a Arquitetura Baseada em Modelagem e Execução em um ùnico Ambiente e a Arquitetura Baseada em Modelagem e Execução e Ambientes Distintos. São expostas: características, vantagens e desvantagens destas arquiteturas. A Aquitetura Baseada em Modelagem e Expressões em Ambientes Distintos é analisada em seu principal ponto negativo: a falta de independência existente entre as apliciações e os processos modelados, bem como são discutidos os problemas que esta falta de independência gera no desenvolvimemto e manutenção das aplicações. Uma alternativa à dependência entre o motor de workflow (engine) e as aplicações é proposta para a Arquitetura Baseada na Modelagem e Execução em Ambientes Distintos. Esta proposta é baseada em um modelo que consiste de uma Camada Intermediária, armazenada em um banco de dados relacional, capaz de extender as funcionalidades do motor de workflow. Este banco de dados armazena toda a estrrutura dos processos modelados, sendo responsável pela função que atualmente é repassada às aplicações: o controle da lógica dos processos. Estes trabalho produziu de uma Camada Intermediária, dividida em Camada de Independência (suportando a independência) e Camada de Integrgação (responsável pela comunicação com o motor de workflow). O estudo apresentada as estruturas do banco de dados, as funções disponibilizadas pela API da Camada Intermediária e um pequeno protótipo para dedmonstrar a arquitetura proposta.|http://hdl.handle.net/10183/4254
Anomalia na densidade em um gás de rede com interações competitivas|2004|Open Access|Dissertação|Água;Densidade;Gás de rede;Diagramas de fase;Teoria de campo médio;Simulação;Líquidos;Gás|por|Apesar de ser um líquido comum na natureza, muitas dúvidas ainda pairam sobre várias características da água. A existência de uma relação comum entre o tipo de potencial intermolecular, criticalidade e as várias formas de anomalia existentes nessa substância ainda é uma questão em aberto, apesar da intensa pesquisa que têm-se feito ao longo dos anos sobre esse assunto. Nesta dissertação, propomos a hipótese de que a anomalia na densidade esta correlacionada à presença de multicriticalidade e que ambos os fenômenos surgem de um potencial de duas escalas. Para dar suporte a esta hipótese, além de trabalhos anteriores, usamos e estudamos um gás de rede com interações que competem (primeiros vizinhos Vi atrativos e segundos vizinhos V2repulsivos). Construímos para este sistema um diagrama de fases J.lV8. T usando dois métodos: aproximação de campo médio e simulações. Encontramos na aproximação de campo médio duas linhas críticas, uma das quais encontra a linha de 1~ ordem separando duas fases líquidas, e um ponto tricrítico, dado que V2/V1 < -0.5. Se V2/V1 > -0.5, a transição líquido-líquido desaparece, dando lugar a apenas duas fases, uma líquido e uma gás, separadas por uma linha de coexistência terminada em um ponto crítico  Com a aproximação de campo médio não encontramos anomalia na densidade. Os resultados obtidos com as simulações alteram qualitativamente o diagrama de fases. Tanto as linhas críticas quanto os pontos tricríticos tem suas posições modificadas com relação ao campo médio. Neste caso encontramos um comportamento anômalo na densidade se V2/Vl < -0.5. Concluímos que o potencial de duas escalas competitivas é um ingrediente necessário ao aparecimento de anomalia na densidade e coexistência entre duas fases líquidas. Ainda, mostramos que essa anomalia pode estar associada não apenas a dois pontos críticos, como se espera para a água, mas a uma multicriticalidade em geral, tal como linhas críticas.|http://hdl.handle.net/10183/4256
Um estudo da fatoração incompleta LU e Cholesky como pré-condicionadores nos métodos iterativos|2002|Open Access|Dissertação|Métodos iterativos;Métodos de fatoração|por|Neste trabalho procuramos analisar alguns métodos iterativos e os processos de aceleração na solução lineares grandes e esparsos, associando o uso de alguns pré-condicionadores, tais como os métodos de fatoração incompleta. De forma mais específica, nos detivemos no estudo deos métodos de fatoração incompleta LU, ou ILU, e o método de Cholesky incompleto. Para isso procuramos antes definir algumas especificidades sobre esses métodos, tais como, crtérios de existência, limitação. Alguns fatores analisam tais problemas e sugerem algumas técnicas de conserto, ou seja, algumas maneiras de eliminar tais falhas para que os métodos de iteração possam ser utlizados para determinar soluções mais próximas da solução real. Procedemos a uma revisão teórica de alguns dos métodos iterativos, dos pré-condicionadores. Jacobi, fatoração incompleta LU e fatoração incompleta de Cholesky e a sua associação com os métodos iterativos GMRES e Gradiente Conjugado. Utilizando os pré-condionadores associados aos métodos iterativos citados e fixando alguns parâmetros de parada, aplicamos algusn testes. Os resultados e a análise dos mesmos encontram-se neste trabalho.|http://hdl.handle.net/10183/4257
Efeitos de ordenamento chiral em supercondutores e sistemas magnéticos desordenados|2004|Open Access|Tese|Materiais granulares;Materiais supercondutores;Compostos de terras raras;Vidros de spin;Sistemas magneticos;Temperatura crítica supercondutora;Propriedades magnéticas;Efeito hall;Magnetorresistência;Magnetização;Transicao de ordem-desordem|por|Este trabalho apresenta uma investigação experimental sobre as propriedades magnéticas e de transporte elétrico de sistemas caracterizados por desordem e frustração. Tais sistemas são amostras granulares do supercondutor de alta temperatura crítica YBa2Cu3O7-δ e amostras do tipo vidro-de-spin e reentrantes das ligas magnéticas diluídas AuMn 8at% e AuFe xat% (x = 8, 12, 15, 18 e 21). No supercondutor granular foram estudados os efeitos de flutuações termodinâmicas na magnetocondutividade nas proximidades da transição supercondutora e a linha de irreversibilidades magnéticas. A transição para o estado de resistência nula é um processo que ocorre em duas etapas. Inicialmente, a transição de pareamento estabiliza a supercondutividade no interior dos grãos. A transição de coerência ocorre em temperaturas inferiores e ativa ligações fracas entre os grãos através de um processo do tipo percolação. O regime que antecede a transição de coerência é caracterizado por flutuações na fase do parâmetro de ordem. A linha de irreversibilidades magnéticas, estudada a partir da magnetoresistência e magnetização DC, revela um comportamento do tipo Almeida-Thouless em baixos campos magnéticos aplicados, seguido de um crossover, em µ0H = 0.1 T, para um comportamento do tipo Gabay-Toulouse. A linha de irreversibilidades é interpretada como sendo uma manifestação experimental de uma transição de vidro chiral.  As ligas magnéticas diluídas foram estudadas através do efeito Hall extraordinário. Os resultados mostram claramente que dois termos de sinais contrários contribuem para este efeito nestes sistemas. Em particular, é observada uma anomalia no coeficiente de Hall extraordinário em temperaturas próximas à temperatura de ordenamento dos vidros-de-spin e de “canting” dos reentrantes que não é prevista por nenhuma teoria convencional de efeito Hall. A interpretação dos resultados é feita em termos do modelo de vidro chiral, mostrando a importância de uma contribuição de origem puramente chiral. Esta é a primeira vez que a chiralidade, uma propriedade intrínseca de sistemas desordenados e frustrados, é observada experimentalmente de modo direto.|http://hdl.handle.net/10183/4267
Um Middleware reflexivo para apoiar o desenvolvimento de aplicações com requisitos de segurança|2004|Open Access|Dissertação|Engenharia : Software;Reflexao computacional;Seguranca : Software;Java (Linguagem de programação);Middleware|por|Muitos aplicativos atuais, envolvendo diversos domínios de conhecimento, são estruturados como arquiteturas de software que incorporam, além dos requisitos funcionais, requisitos não funcionais, como segurança, por exemplo. Tais requisitos podem constituir um domínio próprio, e, portanto, serem comuns a várias outras arquiteturas de software. Tecnologias como Programação Orientada a Aspectos, Reflexão Computacional e Padrões de Projeto colaboram no desenvolvimento de arquiteturas que provêem a separação de requisitos não funcionais. Porém, sua experimentação e adoção no domínio da segurança computacional ainda é incipiente. O foco deste trabalho é a elaboração de um padrão de projeto voltado à segurança, utilizando como arquitetura conceitual programação orientada a aspectos, e como arquitetura de implementação, reflexão computacional. A composição destas tecnologias resulta em um middleware orientado à segurança, voltado a aplicações desenvolvidas em Java. Estuda-se as tecnologias, seus relacionamentos com a área de segurança, seguido da proposta de uma arquitetura de referência, a partir da qual é extraído um protótipo do middleware de segurança. Este, por sua vez, provê mecanismos de segurança tão transparentes quanto possível para as aplicações que suporta.  Com o objetivo de realizar a implementação do middleware de segurança, também são estudadas os mecanismos de segurança da plataforma Java, porém limitado ao escopo deste trabalho. Segue-se o estudo da base conceitual das tecnologias de Reflexão Computacional, o modelo de implementação, seguido de Programação Orientada a Aspectos, o modelo conceitual, e, por fim, têm-se os Padrões de Projeto, a arquitetura de referência. Integrando as três tecnologias apresentadas, propõe-se um modelo, que estabelece a composição de um Padrão Proxy, estruturado de acordo com a arquitetura reflexiva. Este modelo de arquitetura objetiva implementar o aspecto de segurança de acesso a componentes Java, de forma não intrusiva,. Baseado no modelo, descreve-se a implementação dos diversos elementos do middleware, estruturados de forma a ilustrar os conceitos propostos. Ao final, apresenta-se resultados obtidos durante a elaboração deste trabalho, bem como críticas e sugestões de trabalhos futuros.|http://hdl.handle.net/10183/4268
Uma Proposta de modelo fisiológico de emoções|2004|Open Access|Dissertação|Inteligência artificial;Emoções|por|Este trabalho apresenta o Modelo Fisiológico de Emoções. Este modelo trata a inteligência através de um ponto de vista biológico. O comportamento de cada componente é avaliado de forma independente e evitando abstrações que não estão de acordo com o funcionamento do corpo. O Modelo Fisiológico de Emoções contém um organismo simplificado incluindo apenas um restrito grupo de órgãos e tecidos constantemente gerando diferentes estímulos a agindo como geradores de intenção. O modelo também difere de abordagens cognitivas e considera um restrito grupo de estados emocionais com manifestações fisiológicas diferentes influenciando a tomada de decisão. O pequeno grupo de órgãos pode produzir diferentes estados fisiológicos quando o organismo está comendo, correndo ou mostrando algum estado emocional específico. O trabalho ainda mostra a implementação de um agente construído com base no modelo.|http://hdl.handle.net/10183/4271
Investigação de técnicas de visualização para representação de autômatos finitos com saída|2003|Open Access|Dissertação|Computação gráfica;Grafos;Visualizacao : Informacao|por|Atualmente, a World Wide Web (WWW) já se estabeleceu como um dos meios de divulgação mais difundidos. Sendo um meio de publicação de custo relativamente baixo, muitas iniciativas foram desenvolvidas no sentido de estendê-la e transformá-la também numa ferramenta de apoio. Assim, uma série de pesquisas foi realizada no sentido de promover e facilitar o gerenciamento das informações da WWW, que são estruturadas, em sua maioria, como conjuntos de documentos inter-relacionados. Grafos são estruturas utilizadas para a representação de objetos e seus múltiplos relacionamentos. Nesse sentido, pode-se afirmar que hiperdocumentos podem ser modelados através de grafos, onde uma página representa um nodo e um link para outra página é representado por uma aresta. Considerando estas características, e dada a crescente complexidade dos materiais publicados na WWW, desenvolveu-se, ao longo da última década, o uso de técnicas e recursos de Visualização de Grafos com larga aplicação na visualização da estrutura e da navegação na WWW. Técnicas de visualização de grafos são aplicáveis especificamente para representar visualmente estruturas que possam ser modeladas por meio de objetos relacionados, sendo investigadas técnicas para a abstração de modo a facilitar tanto o processo de compreensão do contexto da informação, quanto a apreensão dos dados relacionados.  Este trabalho tem como objetivo a investigação de técnicas de Visualização de Grafos aplicadas a autômatos finitos com saída. Este direcionamento se deve ao fato de alguns autores utilizar a abordagem de autômatos finitos com saída para as estruturas de hiperdocumentos. Se for considerado que um documento da WWW (ou o estado de um autômato) é composto por fragmentos de informação (ou saídas) tais como trechos de texto, imagens, animações, etc e que este documento é relacionado a outros por meio de links (ou transições), tem-se a verificação de sua representatividade por meio destas estruturas. Em trabalho anterior, no âmbito do PPGC da UFRGS, a ferramenta Hyper-Automaton foi desenvolvida com o objetivo de estender o uso da Internet no sentido de prover uma ferramenta de apoio à publicação de materiais instrucionais. Por adotar a notação de autômatos finitos com saída, possibilita, além da criação e gerenciamento de hiperdocumentos, a reutilização de fragmentos de informação sem que haja qualquer interferência de um autômato que utilize este fragmento sobre outro. O Hyper-Automaton foi selecionado como caso de estudo motivador deste trabalho. As técnicas aqui desenvolvidas têm como intuito diminuir a complexidade visual da informação, assim como permitir a navegação através dos autômatos finitos com saída de forma que seja possível visualizar detalhes como as saídas e informações relacionadas a cada uma delas, mantendo a visualização do contexto da informação. Foram analisadas técnicas de agrupamento como forma de redução da complexidade visual, e técnicas do tipo foco+contexto, como alternativa para prover a visualização simultânea do contexto e dos detalhes da informação.|http://hdl.handle.net/10183/4273
Estudo da utilização de uma ferramenta para construção de programas em português em disciplinas básicas de ensino de programação|2001|Open Access|Dissertação|Compiladores;Informática : Educação|por|Este trabalho tem por objetivo apresentar e estudar a aplicação de uma ferramenta chamada PCP – Pseudo-Compilador Portugol, criada para auxiliar estudantes de programação a aprimorar o raciocínio lógico e a criar programas estruturados, sem que precisem se preocupar com comandos e instruções em Inglês ou tenham conhecimento de uma linguagem de programação específica. Por ser uma ferramenta que usa somente palavras do nosso idioma, os alunos podem direcionar todo o seu raciocínio no entendimento e resolução do problema em forma de algoritmo. O estudo experimental realizado neste trabalho pretende analisar e comparar o aprendizado entre grupos de alunos de disciplinas de programação utilizando e não utilizando esta ferramenta. Além de acompanhar o desempenho dos alunos, pretende também coletar informações durante as baterias de testes e obter as opiniões dos mesmos em relação ao PCP, no que se refere às facilidades, dificuldades, pontos positivos e falhas apresentadas. Este estudo é apresentado em duas etapas, com oito baterias de teste em cada uma. Na primeira etapa foram selecionados alunos do Curso de Ciência da Computação da UNIGRAN, em Dourados-MS; na segunda etapa foram selecionados alunos da Escola Anglo Decisivo. Estas duas etapas possibilitam a análise do aprendizado proporcionado pela ferramenta com alunos que já têm alguma noção de programação e com alunos que não tiveram nenhum contato com o desenvolvimento de programas.|http://hdl.handle.net/10183/4274
Vínculos ao histórico de formação estelar da LMC|2003|Open Access|Dissertação|Grande Nuvem de Magalhães;Fotometria estelar;Luminosidade;Formacao de estrelas;Modelo de cores;Magnitude estelar|por|Apresentamos a fotometria de 6 campos estelares profundos distribuídos na Grande Nuvem de Magalhães obtidos com a Wide Field and Planetary Camera 2/Hubble Space Telescope em duas bandas fotométricas: F814W (~I) e F555W (~V). Foram selecionadas ao todo 15997 estrelas pela nossa amostra, que somadas às estrelas da amostra de Castro et aI. 2001 [9] contabilizaram 22239 estrelas, dentro de limites de magnitude típicos que estão no intervalo 18.5<-V <-26. Investigamos o comportamento do erro fotométrico através da tarefa PHOT/IRAF, bem como via medidas independentes de magnitude de um mesmo objeto obtidas com diferentes exposições. Implementamos um método de correção para a completeza fotométrica como função da posição no diagrama cor-magnitude, isto é, com função tanto da magnitude como da cor e analisamos a sensibilidade das funções de luminosidade obtidas para diferentes métodos de correção. Foram obtidos os diagramas cor-magnitude, com os quais ajustamos isócronas de Pádova com idades T ;S 16 Gano e metalicidades 0.001 < Z < 0.004 ou -1.3 < [Fe/H] < -0.7. A população mais velha (r~12 Gano) pode ser encontrada através do ponto de turn-off em V~22. Estrelas de idade intermediária (r~1 - 8 Gano) podem ser localizadas pela presença de um clump. Os campos próximos aos aglomerados jovens NGC1805 e NGC1818 são os campos mais populosos, apresentando estrelas com r~1 Gano. Construímos funções de luminosidade para 18.5<-V <-25, utilizando os dados das duas amostras; não foram encontradas diferenças significativas entre os campos  Desenvolvemos um método para geração de diagramas cor-magnitude (CMDs) artificiais a partir de um modelo de histórico de formação estelar. Este método incorpora os efeitos de erros fotométricos, binarismo não resolvido, avermelhamento e permite o uso de formas variadas de função de massa inicial e do próprio histórico. Implementamos o Método dos Modelos Parciais para modelamento de CMDs, incluindo experimentos controlados para a comprovação da validade deste método na busca de vínculos ao histórico de formação estelar da Grande Nuvem de Magalhães em dife!entes regiões. Recuperamos SFHs confiáveis para cada um dos 6 campos da LMC. Observamos variações na formação estelar de um campo para outro. Em todos os campos encontramos uma lacuna na formação estelar com 7 rv 700 Mano. Os dois campos próximos à barra (NGC1805 e NGC1818) apresentam alguns pequenos surtos, tendo formado em sua maioria, estrelas velhas ou relativamente jovens. Já os campos próximos a NGC1831 e NGC1868 apresentam formação estelar que se aproxima de um histórico de formação estelar uniforme, com alguns pequenos surtos. Os campos NGC2209 e Hodge 11 apresentam três períodos de formação estelar muitos semelhantes.|http://hdl.handle.net/10183/4278
Um estudo comparativo de ferramentas de descoberta de conhecimento em texto: a análise da Amazônia|2002|Open Access|Dissertação|Recuperacao : Informacao;Descoberta : Conhecimento|por|Este trabalho faz avaliação de ferramentas que utilizam técnica de Descoberta de Conhecimento em Texto (agrupamento ou “clustering”). As duas ferramentas são: Eurekha e Umap. O Eurekha é baseado na hipótese de agrupamento, que afirma que documentos similares e relevantes ao mesmo assunto tendem a permanecer em um mesmo grupo. O Umap, por sua vez, é baseado na árvore do conhecimento. A mesma coleção de documentos submetida às ferramentas foi lida por um especialista humano, que agrupou textos similares, a fim de que seus resultados fossem comparados aos das ferramentas. Com isso, pretende-se responder a seguinte questão: a recuperação automática é equivalente à recuperação humana? A coleção de teste é composta por matérias do jornal Folha de São Paulo, cujo tema central é a Amazônia. Com os resultados, pretende-se verificar a validade das ferramentas, os conhecimentos obtidos sobre a região e o tratamento que o jornal dá em relação à mesma.|http://hdl.handle.net/10183/4279
Apresentação de documentos XML através de exemplos|2002|Open Access|Dissertação|XML (Linguagem de marcação);Banco de dados|por|A linguagem XSLT transforma documentos XML não apenas em novos documentos XML, mas também em documentos HTML, PDF e outros formatos, tornando-se bastante útil. Entretanto, como um ambiente de programação, XSLT apresenta algumas deficiências. Não apresenta um ambiente gráfico de programação e exige conhecimento prévio sobre manipulação de estrutura de dados em árvores, o que compromete a produtividade do programador e limita o uso da linguagem a especialistas. Assim, várias propostas têm sido apresentadas na tentativa de suprir estas deficiências, utilizando recursos variados como geração automática de script XSLT e reuso de transformações. Este trabalho apresenta a ferramenta X2H que visa auxiliar a apresentação de documentos XML em HTML, aumentando a produtividade de programadores que utilizam a linguagem XSLT. Para facilitar a sua utilização, a X2H possui uma interface gráfica com abordagem baseada em exemplos, na qual o usuário compõe um documento exemplo HTML a partir de um documento fonte XML. Estes documentos são visualizados como árvores hierárquicas, nas quais é vinculado um conjunto de operações dependente do contexto, que permitem a composição do documento exemplo. Este documento serve de entrada para um gerador de regras, que gera um script na linguagem XSLT que, se executado, apresenta o documento HTML resultado desejado.|http://hdl.handle.net/10183/4280
Editor distribuído de manipulação colaborativa de documentos diagramáticos|2001|Open Access|Dissertação|Modelagem : Sistemas;Editor colaborativo|por|Este trabalho apresenta um framework para editores distribuídos de manipulação colaborativa de documentos diagramáticos (FreDoc). FreDoc fornece um conjunto de classes que propicia um ambiente distribuído para edição colaborativa de diagramas de duas dimensões. Ele é constituído de 4 pacotes de classes: 1) de controle de acesso; 2) de controle de edição; 3) formas geométricas básicas; e 4) editor distribuído. Além disso, é apresentado um estudo sobre mecanismos para construção de editores distribuídos. Editores distribuídos são ferramentas úteis no processo de criação de documentos em um ambiente colaborativo tal como a World Wide Web. O uso de editores distribuídos torna o processo de colaboração mais rápido, pois o colaborador participa ativamente do processo de construção através da inserção de anotações que. Estas anotações podem ser aceitas ou rejeitadas total ou parcialmente pelo ou coordenador. Por fim, é descrito um protótipo, o FreDocUML, desenvolvido para testar a aplicação do framewrok FreDoc, mostrando um processo colaborativo de edição de diagrama de classes na notação UML é descrito.|http://hdl.handle.net/10183/4281
Respostas dinâmicas em sistemas distribuídos e decomposição forçada da superfície livre para um modelo acoplado Oceano-Atmosfera|2003|Open Access|Dissertação|Vibracoes;Modelo geofísico;Sistemas dinâmicos distribuídos|por|O objetivo deste trabalho é a introdução e desenvolvimento de uma metodologia analítico-simbólica para a obtenção de respostas dinâmicas e forçadas (soluções homogêneas e não homogêneas) de sistemas distribuídos, em domínios ilimitados e limitados, através do uso da base dinâmica gerada a partir da resposta impulso. Em domínios limitados, a resposta impulso foi formulada pelo método espectral. Foram considerados sistemas com condições de contorno homogêneas e não homogêneas. Para sistemas de natureza estável, a resposta forçada é decomposta na soma de uma resposta particular e de uma resposta livre induzida pelos valores iniciais da resposta particular. As respostas particulares, para entradas oscilatórias no tempo, foram calculadas com o uso da fun»c~ao de Green espacial. A teoria é desenvolvida de maneira geral permitindo que diferentes sis- temas evolutivos de ordem arbitrária possam ser tratados sistematicamente de uma forma compacta e simples. Realizou-se simulações simbólicas para a obtenção de respostas dinâmicas e respostas for»cadas com equações do tipo parabólico e hiperbólico em 1D,2D e 3D. O cálculo das respostas forçadas foi realizado com a determinação das respostas livres transientes em termos dos valores iniciais das respostas permanentes. Foi simulada a decomposição da resposta forçada da superfície livre de um modelo acoplado oceano-atmosfera bidimensional, através da resolução de uma equação de Klein-Gordon 2D com termo não-homogêneo de natureza dinâmica, devido a tensão de cisalhamento na superfície do oceano pela ação do vento.;The objective of this work is the introduction and the development of an analytical-symbolic methodology for obtaining dynamic and forced responses of distributed systems, in unlimited and limited domains, through the use of the dynamic basis generated by the impulse response. In limited domains, the impulse response was formulated with the spec- tral method. Systems were considered with homogeneous and nonhomogeneous boundary conditions. For systems of stable nature, the forced response is decomposed into the sum of a particular response and a free response that is induced by the initial values of the particular response. The particular responses, for oscillatory time inputs, were calculated with the use of the spatial Green's function. The theory is developed in a general way that allows that di®erent arbitrary order evolution systems can be systematically treated in a compact and simple way. Symbolic simulations ware performed for obtaining dynamics responses and forced responses with equations of parabolic and hyperbolic type in 1D,2D and 3D. The calculation of the forced responses were accomplished with the determina- tion of the transient free responses in terms of the initial values of the permanent responses. It was simulated the decomposition of the forced response of the free surface of a two-dimensional coupled model ocean-atmosphere, through the inte- gration of a 2D Klein-Gordon equation with a nonhomogeneous terms of dynamic nature, due to the shear stress in the surface of the ocean by the action of the wind.|http://hdl.handle.net/10183/4321
Um Modelo hierárquico baseado em políticas para o gerenciamento integrado de redes de computadores e grids computacionais|2004|Open Access|Dissertação|Redes : Computadores;Gerencia : Redes : Computadores|por| Como resultados constatou-se que o modelo de tradução de políticas proposto permite automatizar o gerenciamento da infra-estrutura de grid e rede. Além disso, a solução apresentada fornece novas facilidades de gerenciamento em comparação as soluções de gerenciamento de grid baseadas em políticas encontradas na literatura. A implementação do protótipo do modelo junto ao sistema QAME permitiu que o gerenciamento do grid e da rede fosse realizado de maneira integrada usando uma hierarquia de tradução de políticas.|http://hdl.handle.net/10183/4326
Polietilenos produzidos com zirconocenos suportados hídricos|2005|Open Access|Dissertação|Polietileno;Catalisadores : Zirconocenos;Zirconocenos;Catalisadores metalocênicos|por|Neste trabalho foi estudada a combinação dos catalisadores Cp2ZrCl2 e (nBuCp)2ZrCl2 em reações de (co)polimerização com eteno-1-hexeno, em presença de metiluminoxano. Os zirconocenos foram combinados em três razões molares 1:1, 1:3 e 3:1. Nestas razões foram estudados 3 sistemas homogêneos e 6 heterogêneos. Os catalisadores foram imobilizados sobre a sílica em diferente ordem de adição sobre o suporte. A ordem de imobilização afeta o teor de metal fixado, tendo sido verificado um maior teor de metal e maior atividade nos sistemas onde o Cp2ZrCl2 foi imobilizado primeiro. Foi observado, pela modelagem molecular usando o método EHMO, que o Cp2ZrCl2 apresenta maior afinidade pelo plano AIS da sílica e que o (nBuCp)2ZrCl2 tem bem menor afinidade por este plano, no qual são gerados os sítios ativos. A temperatura, pressão, teor de comonômero e razão Al/Zr influenciam a atividade das reações de polimerização. Os (co)polímeros obtidos foram avaliados quanto à massa molar média, polidispersão, temperatura de fusão, cristalinidade e teor de comonômero. Os sistemas híbridos suportados produziram copolímeros com similar temperatura de fusão, em relação aos homogêneos, mas com menor cristalinidade.  A incorporação do comonômero na cadeia polimérica levou à diminuição da massa molar média, temperatura de fusão e cristalinidade, nos copolímeros obtidos a partir da variação do teor de comonômero e da razão Al/Zr. Os (co)polímeros gerados pelos sistemas estudados não apresentaram bimodalidade na distribuição de massa molar. Nas razões molares 1:3 e 3:1, com os sistemas homogêneos, foram obtidos copolímeros com larga distribuição de massa molar. A modelagem molecular dos resultados obtidos, em reações de homopolimerização, permitiu sugerir a existência de vários sítios ativos, gerados pela influência de um catalisador sobre o outro, quando utilizados sistemas híbridos e também pela influência da presença do suporte.|http://hdl.handle.net/10183/4353
Early and post transition metal complexes as a single of combined components in the ethylene and isoprene polynerization|2005|Open Access|Tese|Isopreno : Polimerização;Etileno : Polimerizacao;Complexos metálicos;Catalisadores|eng|In this work is reported, in a first step, the effect of different experimental parameters and their relation with polymer properties using the homogeneous binary catalyst system composed by Ni(α-diimine)Cl2 (α-diimine = 1,4-bis(2,6-diisopropylphenyl)- acenaphthenediimine) and {TpMs*}V(Ntbu)Cl2 (TpMs* = hydridobis(3-mesitylpyrazol-1- yl)(5-mesitylpyrazol-1-yl)) activated with MAO. This complexes combination produces, in a single reactor, polyethylene blends with different and controlled properties dependent on the polymerization temperature, solvent and Nickel molar fraction (xNi). In second, the control of linear low density polyethylene (LLDPE) production was possible, using a combination of catalyst precursors {TpMs}NiCl (TpMs = hydridotris(3- mesitylpyrazol-1-yl)) and Cp2ZrCl2, activated with MAO/TMA, as Tandem catalytic system. The catalytic activities as well as the polymer properties are dependent on xNi. Polyethylene with different Mw and controlled branches is produced only with ethylene monomer.  Last, the application group 3 metals catalysts based, M(allyl)2Cl(MgCl2)2.4THF (M = Nd, La and Y), in isoprene polymerization with different cocatalysts systems and experimental parameters is reported. High yields and polyisoprene with good and controlled properties were produced. The metal center, cocatalysts and the experimental parameters are determinant for the polymers properties and their control. High conversions in cis-1,4- or trans-1,4-polyisoprene were obtained and the polymer microstructure depending of cocatalyst and metal type. Combinations of Y and La precursors were effective systems for the cis/transpolyisoprene blends production, and the control of cis-trans-1,4-microstructures by Yttrium molar fraction (xY) variation was possible.|http://hdl.handle.net/10183/4360
Decomposição de Calderon e suas aplicações na teoria da regularidade em equações elípticas|2005|Open Access|Dissertação|Equacoes diferenciais parciais elipticas;Teorema da interpolação de Marcinkiewicz|por|Este trabalho tem por objetivo estudar a regularidade de soluções de Equações Diferenciais Parciais Elípticas da forma Lu = f, para f 2 Lp(­), onde p > 1. Para isto, usamos a Decomposição de Calderon-Zygmund e um resultado que é consequência deste, o Teorema da Interpolação de Marcinkiewicz. Além disso, usando quocientes-diferença provamos a regularidade das soluções para o caso p = 2 e L = ¡¢ de uma forma alternativa.|http://hdl.handle.net/10183/4361
Javal: modelo de ambiente de avaliação remota multiagente baseada em tutores embarcados|2002|Open Access|Dissertação|Informática : Educação;Ensino a distância;Tutores embarcados;Sistemas multiagentes|por|Trata o presente objeto de pesquisa da proposta de desenvolvimento de um modelo de ambiente de avaliação remota para Ensino à Distância, baseado no paradigma de Orientação a Objetos e elaborado com base na tecnologia de sistemas multiagentes. Para a validação do modelo, foi desenvolvido um protótipo denominado Javal, capaz de permitir a aplicação e monitoração da realização de avaliações e testes à distância. As soluções comerciais disponíveis no mercado, como Question Mark Perception, Aula Net e WebCT possuem código fechado e custo elevado, além de serem baseadas em soluções proprietárias (QML - Question Mark, ASP - Microsoft, etc.), necessitando de plataforma específica para instalação de servidores e clientes. Quanto ao aspecto funcional, estas ferramentas possuem a base de sua lógica de funcionamento em execução de scripts no servidor. Para cada item de uma avaliação a ser respondido, torna-se necessária a efetivação de uma nova conexão. O modelo proposto traz toda a funcionalidade do sistema para o ambiente do aluno, tornando necessária apenas uma conexão para a busca da avaliação e outra para o envio dos resultados, reduzindo o tráfego na rede. Quanto ao aspecto pedagógico, estas ferramentas limitam-se apenas a apresentar uma avaliação somativa do aluno, geração de graus e estatísticas, não se preocupando em monitorar seus aspectos comportamentais, capazes de apontar indícios de possíveis falhas no aprendizado ou na elaboração da avaliação.  A proposta do modelo é da apresentação de um ambiente orientado a objetos, capaz de distribuir elementos representativos das avaliações existentes no modelo tradicional de ensino, incorporando recursos capazes de possibilitar a monitoração de aspectos comportamentais do aluno, pelo emprego de agentes monitores ou tutores, que podem acompanhar o aluno e auxiliá-lo em situações de dificuldade. O modelo proposto por este trabalho envolve as avaliações formativas e somativas, aplicadas de forma assíncrona e individual. Como sugestão para trabalhos futuros, o modelo propõe o desenvolvimento de classes capazes de implementar a aplicação de avaliações síncronas e em grupo. A validação do modelo proposto foi realizado através do desenvolvimento de um protótipo que, com base no desenvolvimento de uma API Javal específica, implementa os principais tipos de questões disponíveis no sistema de ensino tradicional, além de agentes tutores de avaliação.|http://hdl.handle.net/10183/4374
Monitoramento morfológico do xerogel híbrido 3-(1,4-fenilenodiamina)propil/silica obtido sob diferentes condições de síntese|2005|Open Access|Dissertação|Sílica;Sol-gel|por|Neste trabalho, foi obtido o xerogel híbrido 3-(1,4-fenilenodiamina) propil/sílica, usando-se o método sol-gel de síntese, variando-se as condições experimentais de síntese. Foram usados como reagentes precursores o tetraetilortosilicato (TEOS) e o 3-[(1,4-fenilenodiamina)propil]trimetoxisilano (FDAPS) sintetizado em nosso laboratório. As condições experimentais de síntese variadas foram: a concentração de precursor orgânico (FDAPS), a temperatura de gelificação, o tipo de solvente e o pH do meio reacional. O trabalho foi dividido em duas etapas: na primeira, foram obtidas duas séries de materiais onde se variou a temperatura de gelificação (5, 25, 50 e 70 °C), além da quantidade de precursor orgânico (FDAPS), adicionado à síntese (1,5 e 5,0 mmol). Na segunda etapa variou-se o pH do meio reacional (4, 7 e 10) além do tipo de solvente (etanol, butanol e octanol), mantendo-se a quantidade de precursor orgânico adicionado e a temperatura de gelificação constantes em 5,0 mmol e 25 oC, respectivamente. Em ambas etapas utilizou-se HF como catalisador e manteve-se o sistema fechado, porém não vedado, durante a gelificação.  Na caracterização dos xerogéis híbridos foram usadas as seguintes técnicas: a) termoanálise no infravermelho, para estimar a estabilidade térmica do componente orgânico além da fração de orgânicos dispersos na superfície; b) isotermas de adsorção e dessorção de nitrogênio, para determinação da área superficial específica, do volume e da distribuição de tamanho de poros; c) análise elementar para estimar a fração de componente orgânico presente no xerogel e d) microscopia eletrônica de varredura onde foi possível observar textura, compactação e presença de partículas primárias nos xerogéis. A partir dos resultados de caracterização foi possível avaliar a influência dos parâmetros experimentais de síntese nas características dos xerogéis híbridos obtidos. Xerogéis híbridos com maior teor de orgânicos foram mais influenciados pela variação da temperatura de gelificação. Um aumento na temperatura de gelificação produz xerogéis com menor porosidade, entretanto, com maior estabilidade térmica do componente orgânico. Considerando-se estabilidade térmica e porosidade, as amostras gelificadas a 25 oC apresentaram os melhores resultados. Em relação à variação de pH e solvente, as amostras gelificadas em pH ácido foram as que apresentaram maior porosidade, enquanto que a maior estabilidade térmica foi alcançada usando-se etanol como solvente.|http://hdl.handle.net/10183/4390
Modelo neuro-evolutivo de coordenação adaptativa em ambientes dinâmicos|2005|Open Access|Dissertação|Inteligência artificial;Agentes inteligentes;Redes neurais|por|Em ambientes dinâmicos e complexos, a política ótima de coordenação não pode ser derivada analiticamente, mas, deve ser aprendida através da interação direta com o ambiente. Geralmente, utiliza-se aprendizado por reforço para prover coordenação em tais ambientes. Atualmente, neuro-evolução é um dos métodos de aprendizado por reforço mais proeminentes. Em vista disto, neste trabalho, é proposto um modelo de coordenação baseado em neuro-evolução. Mais detalhadamente, desenvolveu-se uma extensão do método neuro-evolutivo conhecido como Enforced Subpopulations (ESP). Na extensão desenvolvida, a rede neural que define o comportamento de cada agente é totalmente conectada. Adicionalmente, é permitido que o algoritmo encontre, em tempo de treinamento, a quantidade de neurônios que deve estar presente na camada oculta da rede neural de cada agente. Esta alteração, além de oferecer flexibilidade na definição da topologia da rede de cada agente e diminuir o tempo necessário para treinamento, permite também a constituição de grupos de agentes heterogêneos. Um ambiente de simulação foi desenvolvido e uma série de experimentos realizados com o objetivo de avaliar o modelo proposto e identificar quais os melhores valores para os diversos parâmetros do modelo. O modelo proposto foi aplicado no domínio das tarefas de perseguição-evasão.|http://hdl.handle.net/10183/4404
O uso de modelos compartimentais do tipo hospedeiro-macroparasita na dinâmica de doenças infecciosas causadas por helmintos diretamente transmitidos|2004|Open Access|Dissertação|Modelos matemáticos;Epidemiologia|por|Neste trabalho, nos propomos a estudar o desenvolvimento teórico de alguns modelos matemáticos básicos de doenças infecciosas causadas por macroparasitas, bem como as dificuldades neles envolvidas. Os modelos de transmissão, que descrevemos, referem-se ao grupo de parasitas com transmissão direta: os helmintos. O comportamento reprodutivo peculiar do helminto dentro do hospedeiro definitivo, no intuito de produzir estágios que serão infectivos para outros hospedeiros, faz com que a epidemiologia de infecções por helmintos seja fundamentalmente diferente de todos os outros agentes infecciosos. Uma característica importante nestes modelos é a forma sob a qual supõe-se que os parasitas estejam distribuídos nos seus hospedeiros. O tamanho da carga de parasitas (intensidade da infecção) em um hospedeiro é o determinante central da dinâmica de transmissão de helmintos, bem como da morbidade causada por estes parasitas.  Estudamos a dinâmica de parasitas helmintos de ciclo de vida direto para parasitas monóicos (hermafroditas) e também para parasitas dióicos (machos-fêmeas) poligâmicos, levando em consideração uma função acasalamento apropriada, sempre distribuídos de forma binomial negativa. Através de abordagens analítica e numérica, apresentamos a análise de estabilidade dos pontos de equilíbrio do sistema. Cálculos de prevalências, bem como de efeitos da aplicação de agentes quimioterápicos e da vacinação, no controle da transmissão e da morbidade de parasitas helmintos de ciclo de vida direto, também são apresentados neste trabalho.;The aim of this work is to study the theoretical development of some basic mathematical models of infectious diseases caused by macroparasites as well as the difficulties involved in them. The transmission models which we describe refer to the group of parasites with direct transmission: the helminths. The peculiar reproductive behavior of the helminth in the definitive host, in order to produce stages which will be infectious for other hosts, makes the epidemiology of infections by helminths fundamentally different from all other infectious agents. An important feature of these models is the form assumed for the distribution of parasites among their hosts. The worm burden (intensity of the infection) in a given host is the crucial quantity of the dynamics of the helminth transmission, and also of the morbidity caused by the parasites. By assuming that the parasites are distributed in a negative binomial form, we study the dynamics for both monoic (hermaphrodite) and dioic (male-female) polygamic with a particular mating function.  Through both analytical and numerical approaches, the stability of the equilibrium points is analyzed. Prevalence computations and studies about the control of transmission and morbidity of heminthic parasites of direct life cycle, through the application of chemiotherapic agents and through vaccination, are also presented in this work.|http://hdl.handle.net/10183/4417
Categorização no modelo de Hopfield : efeitos de ruído sináptico e de diluição simétrica|2004|Open Access|Tese|Sistemas caóticos;Redes neurais|por|Nesta tese estudamos os efeitos de diluição simétrica gradual das conexões entre neurônios e de ruído sináptico sobre a habilidade de categorização de padrões no modelo de Hopfield de redes neurais, mediante a teoria de campo médio com simetria de réplicas e simulações numéricas. Utilizamos generalizações da regra de aprendizagem de Hebb, para uma estrutura hierárquica de padrões correlacionados em dois níveis, representando os ancestrais (conceitos) e descendentes (exemplos dos conceitos). A categorização consiste no reconhecimento dos conceitos por uma rede treinada unicamente com exemplos dos conceitos. Para a rede completamente conexa, obtivemos os diagramas de fases e as curvas de categorização para vários níveis de ruído sináptico. Observamos dois comportamentos distintos dependendo do parâmetro de armazenamento. A habilidade de categorização é favorecida pelo ruído sináptico para um número finito de conceitos, enquanto que para um número macroscópico de conceitos este favorecimento não é observado. Entretanto a performance da rede permanece robusta contra o ruído sináptico. No problema de diluição simétrica consideramos apenas um número macroscópico de conceitos, cada um com um número finito de exemplos. Os diagramas de fases obtidos exibem fases de categorização, de vidro de spin e paramagnética, bem como a dependência dos parâmetros de ordem com o número de exemplos, a correlação entre exemplos e conceitos, os ruídos sináptico e estocástico, e a conectividade. A diluição favorece consideravelmente a categorização, particularmente no limite de diluição extrema|http://hdl.handle.net/10183/4433
Efeitos das substituições químicas na irreversibilidade magnética e magnetocondutividade do supercondutor YBa2 Cu3 O7-[delta]|2004|Open Access|Tese|Magnetocondutividade;Flutuações;Campos magnéticos;Vidros de spin;Aprisionamento de fluxo;Supercondutividade;Teoria de landau ginzburg;Condutividade elétrica;Irreversibilidade;Temperatura|por|O presente trabalho consiste da realização de um estudo experimental sobre os efeitos das substituições químicas na irreversibilidade magnética e na magnetocondutividade do supercondutor YBa2Cu3O7-δ. Para tanto, o comportamento da linha de irreversibilidade magnética (LIM) bem como dos regimes de flutuações na magnetocondutividade foram pesquisados em amostras policristalinas e moncristalinas de YBa2-xSrxCu3O7-δ (x = 0, 0.1, 0.25, 0.37 e 0.5) e YBa2Cu2.97D0.03O7-δ (D = Zn ou Mg). Além de reduzir drasticamente o valor da temperatura crítica de transição, Tc, os dopantes introduzem um caráter granular nos monocristais. No monocristal puro, o comportamento da LIM é descrito pela lei de potências prevista pelo modelo de ""flux creep"" gigante para dinâmica de fluxo de Abrikosov convencional. Por outro lado, o comportamento da LIM para as amostras supercondutoras granulares apresenta características própias bastante relevantes. Os dados do limite de irreversibilidade, Tirr(H) seguem a lei de potência ditada pelas teorias de ""flux creep"" somente em altos campos magnéticos.Na região de baixos campos magnéticos, dois diferentes regimes de dinâmica de fluxo surgem: Nos campos magnéticos mais baixos que 1 kOe, os dados de Tirr(H) seguem uma lei de potência do tipo de Almeida-Thouleess (AT). Perto de 1 kOe, ocorre um ""crossover"" e em campos magnéticos intermediários passa a ter seu comportamento descrito por uma lei de potências do tipo Gabay-Toulouse (GT).  A ocorrência de um comportamento AT-GT na LIM é a assinatura de um sistema frustrado onde a dinâmica de fluxo intergranular ou de Josephson é dominante. Na ausência de teorias específicas para este comportamento em baixos campos, descrevemos o comportamento da LIM de nossos supercondutores granulares, na região de baixo campo, em analogia aos sistemas vidros de spin. No entanto, o comportamento de Tirr(H) na região de altos campos, ocorre de acordo com a teoria de ""flux creep"" gigante. Particularmente, para valores acima de 20 kOe, a LIM nos monocristais de YBa2-xSrxCu3O7-δ para H // ab, exibe fortes propriedades direcionais para a orientação de H próximo aos planos de maclas (PMCs). Este comportamento é do tipo ""cusp"", similar ao observado em supercondutores com defeitos colunares, o qual caracteriza uma fase vidro de Bose. Por outro lado, a magnetoresistividade elétrica revela que a transição resistiva dos supercondutores granulares ocorre em duas etapas. Quando a temperatura é decrescida, inicialmente ocorre a transição de pareamento no interior dos grãos. Em temperaturas inferiores, na proximidade do estado de resistência nula, ocorre a transição de coerência, observada pela primeira vez num monocristal. Na transição de coerência, o parâmetro de ordem adquire ordem de longo alcance. Na região de temperaturas imediatamente acima de Tc, nossos resultados de flutuações na magnetocondutividade revelam a ocorrência de regimes críticos e Gaussianos. Abaixo de Tc, na região paracoerente, que antecede à transição de coerência, observaram-se regimes críticos cujo expoente é consistente com o esperado para o modelo 3D-XY com desordem relevante e dinâmica do tipo vidro de spin.|http://hdl.handle.net/10183/4435
Modelos cinéticos da equação linearizada de Boltzmann e um problema de transferência de calor em microescala|2005|Open Access|Dissertação|Fisica matematica : Dinamica dos fluidos : Teoria cinetica;Transferência de calor;Microfluídos;Equacoes de boltzmann|por|Neste trabalho, um problema de transferência de calor da dinâmica de gases rarefeitos, causado pela diferença de temperaturas nas superfícies de um canal, é abordado. O problema é formulado através dos modelos cinéticos BGK, S e Gross-Jackson da equação linearizada de Boltzmann e resolvido, de forma unificada, pelo método analítico de ordenadas discretas (método ADO). Resultados numéricos para as perturbações de densidade e temperatura e também para o fluxo de calor são apresentados e comparados, mostrando que não se pode dizer que algum dos três modelos seja uma melhor aproximação da solução aos resultados da equação linearizada de Boltzmann.;In this work, a heat transfer problem in the rarefied gas dynamics field, in a plane channel, is studied. In particular, the flow is induced by different temperatures at the wall surfaces. The formulation of the problem is based in an ”unified” kinetic equation which includes the BGK model, the S-model and the Gross-Jackson model of the linearized Boltzmann equation. An analytical version of the discrete-ordinates method (the ADO method) is used to develop the solution and to evaluate the density, temperature and heat-flow profiles. Numerical results are presented and used to establish comparisons with the linearized Boltzmann equation results. It is shown that, for an analysis based in all cases, it is not possible to say that one of the models is a better approximation of the solution of the linearized Boltzmann equation.|http://hdl.handle.net/10183/4464
Algoritmos adaptativos para o método GMRES(m)|2005|Open Access|Dissertação|Análise numérica;Métodos iterativos;Método do Resíduo Mínimo Generalizado (GMRES)|por|Nesse trabalho apresentamos algoritmos adaptativos do M´etodo do Res´ıduo M´ınimo Generalizado (GMRES) [Saad e Schultz, 1986], um m´etodo iterativo para resolver sistemas de equa¸c˜oes lineares com matrizes n˜ao sim´etricas e esparsas, o qual baseia-se nos m´etodos de proje¸c˜ao ortogonal sobre um subespa¸co de Krylov. O GMRES apresenta uma vers˜ao reinicializada, denotada por GMRES(m), tamb´em proposta por [Saad e Schultz, 1986], com o intuito de permitir a utiliza¸c˜ao do m´etodo para resolver grandes sistemas de n equa¸c˜oes, sendo n a dimens˜ao da matriz dos coeficientes do sistema, j´a que a vers˜ao n˜ao-reinicializada (“Full-GMRES”) apresenta um gasto de mem´oria proporcional a n2 e de n´umero de opera¸c˜oes de ponto-flutuante proporcional a n3, no pior caso. No entanto, escolher um valor apropriado para m ´e dif´ıcil, sendo m a dimens˜ao da base do subespa¸co de Krylov, visto que dependendo do valor do m podemos obter a estagna¸c˜ao ou uma r´apida convergˆencia. Dessa forma, nesse trabalho, acrescentamos ao GMRES(m) e algumas de suas variantes um crit´erio que tem por objetivo escolher, adequadamente, a dimens˜ao, m da base do subespa¸co de Krylov para o problema o qual deseja-se resolver, visando assim uma mais r´apida, e poss´ıvel, convergˆencia. Aproximadamente duas centenas de experimentos foram realizados utilizando as matrizes da Cole¸c˜ao Harwell-Boeing [MCSD/ITL/NIST, 2003], que foram utilizados para mostrar o comportamento dos algoritmos adaptativos. Foram obtidos resultados muito bons; isso poder´a ser constatado atrav´es da an´alise das tabelas e tamb´em da observa ¸c˜ao dos gr´aficos expostos ao longo desse trabalho.|http://hdl.handle.net/10183/4475
Seqüências Kneading e classificação de aplicações monótonas por partes|2004|Open Access|Dissertação|Teoria Kneading;Conjugação topológica|por|Neste trabalho, nós estudamos propriedades básicas de aplicações monótonas por partes, utilizando a Teoria Kneading na obtenção de uma condição suficiente para a existência de conjugção topológica entre uma certa classe de aplicações padrão.|http://hdl.handle.net/10183/4494
Estudo computacional de líquidos iônicos do tipo dialquilimidazólio|2005|Open Access|Dissertação|Líquidos iônicos;Métodos computacionais|por|Neste trabalho foram desenvolvidos parâmetros de campo de força dos ânions tetrafenilborato (BPh4 -) e hexafluorfosfato (PF6 -) dentro da metodologia AMBER para a simulação computacional por dinâmica molecular de Líquidos Iônicos formados por estes ânions e cátions do tipo dialquilimidazólio, o 1-n-butil-3-metilimidazólio (BMI+), o 1-etil-3-metilimidazólio (EMI+) e o 1,3-dimetilimidazólio (MMI+). A validação destes parâmetros foi realizada por comparação entre as freqüências dos modos normais obtido por cálculo ab initio com aquelas obtidas por mecânica molecular, juntamente com uma comparação entre as estruturas moleculares e momentos multipolares obtidos pelas duas metodologias. Seguiu-se então a validação por comparação dos resultados dos cálculos de dinâmica molecular com dados experimentais, como densidades, entalpias de vaporização, condutividade elétrica, estrutura radial e espacial e também dados de difração de nêutrons. Foi atingida uma concordância bastante grande entre dados experimentais e cálculo teórico principalmente no que diz respeito à estrutura dos Líquidos Iônicos e foi possível racionalizar em termos de tamanho do grupo alquila do cátion (n-butil, metil ou etil) e do tamanho do ânion tanto propriedades estruturais deste líquidos quanto características dinâmicas dos mesmos.|http://hdl.handle.net/10183/4502
Aplicação da 4-fenilenodiaminopropilsílica xerogel como adsorvente na pré-concentração de cobre em águas|2005|Open Access|Dissertação|Gel de sílica : Síntese;Sol-gel|por|Neste trabalho utilizou-se um adsorvente híbrido mesoporoso, a 4- fenilenodiaminopropilsílica (4-PhAP/sílica) que foi obtida através do processo sol-gel. O material foi caracterizado através de espectroscopia de infravermelho, análise elementar de carbono, hidrogênio e nitrogênio (CHN). Na continuidade do trabalho o adsorvente foi empregado na pré-concentração de amostras certificadas de água com posterior determinação de Cu2+ por espectroscopia de absorção atômica com forno de grafite (GFAAS). A massa característica de Cu2+ encontrada foi 15,0 ± 0,2 pg, sendo que o limite de detecção na determinação de cobre em água empregando o procedimento de préconcentração foi 0,2 µg l-1. Foi possível realizar em torno de 750 ciclos de leitura com o mesmo tubo de grafite. Durante a pré-concentração, agitou-se dentro em um frasco de polietileno: aliquotas de 5,00 ml de água certificadas, 20,0 ml de tampão acetato (pH 5,2) e 20,0 mg do adsorvente. Após 60 minutos de contato, separou-se a fase sólida contendo adsorvente mais adsorbato da fase líquida através de filtração, suspendeu-se então 15,0 mg do adsorvente contendo Cu2+ em 1 ml de solução contendo HNO3 0,5% e Triton X-100 0.05%. No prosseguimento do trabalho 20,0 µl desta suspensão foi diretamente introduzida numa plataforma integrada de um tubo de grafite, previamente tratada com modificador permanente W-Rh. O fator de pré-concentração obtido com a utilização do xerogel 4-PhAP/sílica como adsorvente foi 3,75, sendo que a capacidade de retenção do adsorvente foi 0,52 mmol de cobre por grama de material adsorvente.|http://hdl.handle.net/10183/4503
Integração de sistemas de informação através de web services|2004|Open Access|Dissertação|Internet;Sistemas : Informação;Serviços Web;Integracao : Informacao|por|Sistemas de informação incorporam processos de negócios particulares de cada organização. A medida em que se observa uma crescente pressão de mercado para que empresas troquem informações de forma automatizada e segura para obtenção de melhores resultados, faz-se necessário repensar a forma como são concebidos os sistemas de informação, desde a modelagem da empresa propriamente dita até a modelagem dos processos de negócio e sua interação com os demais colaboradores. Modelar os processos de negócio de uma empresa em um contexto global significa não apenas estabelecer regras de comportamento, mas também expressar a forma como os processos poderão ser acionados e interagir com sistemas de informação diferentes. Existem várias tecnologias empregadas para a integração de sistemas de informação. Entre tantas tecnologias, uma delas vêm recebendo especial atenção: a tecnologia Web services. A suposta interoperabilidade dos Web services permite a comunicação de aplicações desenvolvidas em diferentes plataformas de hardware e diferentes linguagens de programação através da Internet ou de uma rede local. No entanto, algumas particularidades devem ser observadas para que a implementação de Web services seja eficiente. Disponibilizar processos de negócio de uma empresa através da Internet pode ser uma ótima opção para o incremento de suas atividades, mas requer cuidados especiais. Este trabalho apresenta uma revisão bibliográfica sobre a modelagem de empresas, modelagem de processos de negócio e a integração de sistemas de informação através do uso de Web services. Através de um estudo de caso, são apresentados os principais conceitos e as etapas necessárias para a implementação de Web services em um sistema Web. Como contribuição deste trabalho, é proposta uma alternativa de modelagem de sistemas que permite um melhor controle sobre o tratamento de exceções em Web services.  O trabalho desenvolvido compreendeu a especificação, desenvolvimento e aplicação de um ambiente para suportar esta classe de aplicação. No texto é descrito o funcionamento da biblioteca NuSOAP, apresentando o código-fonte completo da aplicação desenvolvida, acessando Web services através de chamadas em alto nível (WSDL). Com o presente trabalho, tem-se uma proposta, já avaliada e validada, para funcionar como referencial conceitual e prático para o desenvolvimento de aplicações usando a tecnologia de Web services.|http://hdl.handle.net/10183/4560
Utilizando conceitos como descritores de textos para o processo de identificação de conglomerados (clustering) de documentos|2004|Open Access|Tese|Armazenamento : Dados;Recuperacao : Informacao;Descoberta : Conhecimento;Agrupamento : Informacao textual|por|A descoberta e a análise de conglomerados textuais são processos muito importantes para a estruturação, organização e a recuperação de informações, assim como para a descoberta de conhecimento. Isto porque o ser humano coleta e armazena uma quantidade muito grande de dados textuais, que necessitam ser vasculhados, estudados, conhecidos e organizados de forma a fornecerem informações que lhe dêem o conhecimento para a execução de uma tarefa que exija a tomada de uma decisão. É justamente nesse ponto que os processos de descoberta e de análise de conglomerados (clustering) se insere, pois eles auxiliam na exploração e análise dos dados, permitindo conhecer melhor seu conteúdo e inter-relações. No entanto, esse processo, por ser aplicado em textos, está sujeito a sofrer interferências decorrentes de problemas da própria linguagem e do vocabulário utilizado nos mesmos, tais como erros ortográficos, sinonímia, homonímia, variações morfológicas e similares. Esta Tese apresenta uma solução para minimizar esses problemas, que consiste na utilização de “conceitos” (estruturas capazes de representar objetos e idéias presentes nos textos) na modelagem do conteúdo dos documentos.  Para tanto, são apresentados os conceitos e as áreas relacionadas com o tema, os trabalhos correlatos (revisão bibliográfica), a metodologia proposta e alguns experimentos que permitem desenvolver determinados argumentos e comprovar algumas hipóteses sobre a proposta. As conclusões principais desta Tese indicam que a técnica de conceitos possui diversas vantagens, dentre elas a utilização de uma quantidade muito menor, porém mais representativa, de descritores para os documentos, o que torna o tempo e a complexidade do seu processamento muito menor, permitindo que uma quantidade muito maior deles seja analisada. Outra vantagem está no fato de o poder de expressão de conceitos permitir que os usuários analisem os aglomerados resultantes muito mais facilmente e compreendam melhor seu conteúdo e forma. Além do método e da metodologia proposta, esta Tese possui diversas contribuições, entre elas vários trabalhos e artigos desenvolvidos em parceria com outros pesquisadores e colegas.|http://hdl.handle.net/10183/4576
Nanopartículas de irídio em líquidos iônicos : síntese, caracterização e aplicação em reações de hidrogenação catalítica|2005|Open Access|Tese|Nanopartículas;Líquidos iônicos;Hidrogenação catalítica|eng|Resumo não disponível|http://hdl.handle.net/10183/4598
Determinação de elementos traço em amostras ambientais por ICP OES|2005|Open Access|Dissertação|Elementos-traço : Determinação|por|Neste trabalho é avaliada a aplicação da técnica de ICP OES (Espectrometria de Emissão com Plasma Indutivamente Acoplado) para a determinação de elementos traço em amostras ambientais. Foram investigados os comportamentos de As, Ba, Cd, Co, Cu, Cr, Mn, Ni, Pb, V e Zn, utilizando-se espectrômetro com vista de observação axial/radial do plasma e sistema de detecção baseado em dispositivos de carga acoplada (CCD). No presente estudo, foi avaliado o desempenho dos nebulizadores pneumáticos do tipo concêntrico (Meinhard), “cross flow” e GemCone® acoplados às câmaras de nebulização de duplo passo (Scott) e ciclônica, bem como do nebulizador ultra-sônico para a introdução de amostras no plasma. Investigou-se a robustez do plasma, potência de radiofreqüência (RF), vazão dos gases de nebulização e auxiliar, bem como a altura de observação, para ambas as vistas de observação do plasma. Sob condições otimizadas os limites de detecção (LD), em ng mL-1, para os elementos As, Ba, Cd, Co, Cr, Cu, Mn, Ni, Pb, V e Zn, em solução aquosa de HNO3 5% (v/v), utilizando-se a configuração axial do plasma, foram: 1,1 - 16; 0,002 - 0,32; 0,03 - 1,2; 0,02 - 0,72; 0,03 - 0,82; 0,04 - 3,0; 0,003 - 0,76; 0,08 - 3,8; 0,22 - 8,9; 0,04 - 2,6; e 0,02 - 1,2 respectivamente. Utilizando-se a configuração radial, os LDs (ng mL-1) dos mesmos elementos foram: 10 - 87; 0,01 - 0,91; 0,07 - 3,8; 0,16 - 4,3; 0,13 - 8,1; 0,16 - 4,3; 0,01 - 0,81; 0,43 - 7,6; 1,4 - 37; 0,28 - 6,0 e 0,77 - 9,5 respectivamente. Com relação à nebulização pneumática, os LDs são relativamente mais baixos quando é utilizado o nebulizador concêntrico acoplado à câmara de nebulização ciclônica. LDs ainda melhores são obtidos mediante o uso de nebulização ultra-sônica mas, neste caso, foi observado que o plasma é menos robusto.  As metodologias foram desenvolvidas mediante o uso de materiais de referência certificados sendo analisados os seguintes materiais: sedimento marinho (PACS-2/NRCC), folhas de maçã (apple leaves – 1515/NIST) e água (natural water – 1640/NIST), obtendo-se concentrações concordantes com as certificadas, com exceção do Ni em folha de maçã, que não foi detectado utilizando-se a nebulização pneumática e também não pôde ser determinado com exatidão mediante o uso de nebulizador ultra-sônico. Interferências não espectrais observadas na análise de sedimento marinho foram contornadas através da diluição da amostra, ou através da lavagem da câmara de nebulização com solução de HNO3 5% (v/v), por 60 s entre cada ciclo de leitura. Estas interferências não puderam ser contornadas com o uso de padrão interno (PI). A interferência espectral do As sobre o Cd não foi observada fazendo-se a medição do sinal em área de pico, demarcado com somente 3 pontos/pico. Após serem estabelecidas as metodologias de análise, foram analisadas as seguintes amostras não certificadas: água de rio, água subterrânea (poço artesiano), água tratada e de chuva, folhas de eucalipto e bambu, acículas de pinus e infusão de chá preto, sendo possível quantificar baixas concentrações dos elementos investigados, utilizando-se a calibração externa.|http://hdl.handle.net/10183/4604
A review of particle transport theory in a binary stochastic medium|2005|Open Access|Dissertação|Teoria de transporte estocástico;Atomic Mix;Levermore-Pomraning|por|A finalidade deste trabalho ´e apresentar uma revis˜ao da teoria do transporte de part´ıculas em meios compostos por uma mistura aleat´oria bin´aria. Para atingir este objetivo n´os apresentamos brevemente alguns conceitos b´asicos de teoria do transporte, e ent˜ao discutimos em detalhes a deriva¸c˜ao de duas abordagens desenvolvidas para a solu¸c˜ao de tais problemas: os modelos de mistura atˆomica e de Levermore-Pomraning. Providenciamos ainda, com o uso da formula¸c˜ao LTSN, compara ¸c˜oes num´ericas destes modelos com resultados de benchmark gerados atrav´es de um processo de Monte Carlo.|http://hdl.handle.net/10183/4620
Nanoestruturas de ferro crescidas em superfícies vicinais de silício: morfologia, estrutura e magnetismo|2004|Open Access|Tese|Nanotecnologia;Nanopartículas;Ferro;Silício;Efeito magneto-otico kerr;Magnetismo|por|Neste trabalho foram estudadas as propriedades morfológicas, estruturais e magnéticas de nanoestruturas de Fe crescidas em Si(111) vicinal. A análise de superfície foi feita usando microscopia de força atômica e microscopia de tunelamento, e as medidas de caracterização estrutural, por espectroscopia de absorção de raios-X. As propriedades magnéticas foram investigadas usando dois métodos distintos: efeito Kerr magneto-óptico e magnetômetro de força de gradiente alternado. Os substratos foram preparados quimicamente com uma solução NH4F e caracterizados por microscopia de força atômica. As análises morfológicas das superfícies permitiram classificá-las em dois grupos: Si(111)- monoatômicos e Si(111)-poliatômicos. Filmes finos de ferro de 1.5, 3, 6 e 12 nm foram crescidos sobre eles. A análise das superfícies indicou dois modos diferentes de crescimento do ferro; o sistema Fe(x)Si(111)-monoatômico resulta em grãos de ferro aleatoriamente distribuídos, e o sistema Fe(x)Si(111)-poliatômico em nanogrãos de ferro alongados na direção perpendicular aos degraus, auto-organizados. Particularmente no filme Fe(3 nm)/Si(111)-poliatômico, ao redor de metade dos grãos estão alinhados ao longo da direção [110] , ou seja, paralelo aos degraus. O padrão de nanogrãos de ferro alongados orientados perpendicular aos degraus foi interpretado com uma conseqüência da anisotropia induzida durante o processo de deposição e a topologia do substrato Si(111)-poliatômico. Uma forte relação entre a morfologia e a resposta magnética dos filmes foi encontrada. Um modelo fenomenológico foi utilizado para interpretar os dados experimentais da magnetização, e uma excelente concordância entre as curvas experimentais e calculadas foi obtida.|http://hdl.handle.net/10183/4672
Anomalia na densidade em um gás de rede com potencial repulsivo atenuado|2004|Open Access|Dissertação|Método de Monte Carlo;Diagramas de fase;Gás de rede|por|Apesar de ser um líquido comum na natureza, muitas dúvidas ainda pairam sobre várias características da água. A existência de uma relação comum entre o tipo de potencial intermolecular, criticalidade e as várias formas de anomalia existentes nessa substância ainda é uma questão em aberto, apesar da intensa pesquisa que têm-se feito ao longo dos anos sobre esse assunto . Nesta dissertação, propomos a hipótese de que a anomalia na densidade está correlacionada à presença de multicriticalidade, e que ambos os fenômenos surgem de um potencial de duas escalas. Para dar suporte a esta hipótese, além de trabalhos anteriores, usamos e estudamos um gás de rede com interações que competem (primeiros vizinhos V1 repulsivos e segundos vizinhos V2 atrativos). Construímos para este sistema um diagrama de fases µ vs. T usando dois métodos: aproximação de campo médio e simulações. Encontramos na aproximação de campo médio duas linhas críticas, uma das quais encontra a linha de 1a ordem separando duas fases líquidas, e um ponto tricrític  Com a aproximação de campo médio não encontramos anomalia na densidade. Os resultados obtidos com as simulações alteram qualitativamente o diagrama de fases. Tanto as linhas críticas quanto os pontos tricríticos tem suas posições modificadas com relação ao campo médio. Neste caso encontramos um comportamento anômalo na densidade. Concluímos que o potencial de duas escalas competitivas é um ingrediente necessário ao aparecimento de anomalia na densidade e coexistência entre duas fases líquidas. Ainda, mostramos que essa anomalia pode estar associada não apenas a dois pontos críticos, como se espera para a água, mas a uma multicriticalidade em geral, tal como linhas críticas.|http://hdl.handle.net/10183/4673
Uma tradução de gramáticas de hipergrafos baseadas em objetos para cálculo-π|2003|Open Access|Dissertação|Teoria : Ciência : Computação;Gramatica : Grafos|por|O aumento da escala e funcionalidade dos sistemas de computação e sua crescente complexidade envolvem um aumento significante de custos e exigem recursos humanos altamente qualificados para o desenvolvimento de software. Integrando-se o uso de métodos formais ao desenvolvimento de sistemas complexos, permite-se realizar análises e verificações destes sistemas, garantindo assim sua correção. Existem diversos formalismos que permitem descrever sistemas, cada qual com diferentes níveis de abstração. Quando consideramos sistemas complexos, surge a necessidade de um modelo que forneça construções abstratas que facilitem o entendimento e a especificação destes sistemas. Um modelo baseado em objetos fornece um nível de abstração que tem sido muito aplicado na prática, onde os dados e os processos que os manipulam são descritos juntos em um objeto. Gramática de Grafos Baseada em Objetos (GGBO) é um modelo baseado em objetos, que além de ser uma linguagem visual, apresenta a vantagem de as especificações adquirirem um estilo baseado em objetos, que é bastante familiar à maioria dos desenvolvedores.  Porém, as GGBOs não possuem ainda ferramentas para verificação automática de propriedades desejadas nos sistemas modelados. Uma alternativa para resolver isso é definir uma tradução (que preserve a semântica) desta linguagem para outra, para a qual existam verificadores automáticos. Um formalismo bastante conhecido e estabelecido para descrição de sistemas concorrentes, para o qual existem verificadores automáticos, é o cálculo-π. Porém, sob o aspecto de especificação de sistemas complexos, GGBOs parecem ser mais adequadas como linguagem de especificação que o cálculo-π, pois são visuais, mais intuitivas e possuem um estilo baseado em objetos. Neste trabalho foi definido um formalismo (baseado nas GGBOs), denominado Gramática de Hipergrafos Baseada em Objetos e uma tradução deste formalismo para o cálculo-π, aliando assim as vantagens desses dois métodos. Além disso, para validar a tradução definida, foram feitas provas de que a semântica das gramáticas de hipergrafos baseadas em objetos é preservada na tradução.|http://hdl.handle.net/10183/4678
Suporte à reengenharia de processos e negócios com base em sistemas de workflow|2003|Open Access|Dissertação|Organização : Administracao : Empresas;Reengenharia : Processos|por|O mundo globalizado de hoje conduz as organizações a buscarem, cada vez mais, a Reengenharia de Processos de Negócio ou BPR – Business Process Reengineering como uma solução para se manterem competitivas em seus respectivos mercados. Uma das tecnologias de informação que aparece como habilitadora da BPR é a tecnologia de Workflow (fluxos de trabalho), onde os WfMS – Workflow Management Systems ou Sistemas de Gerenciamento de Workflow, através de metodologias e software, facilitam a transformação que está sendo exigida às empresas para se manterem competitivas. Existem, atualmente, muitos estudos sobre WfMS, o que evidência esta como sendo uma área bastante próspera e em desenvolvimento. Apesar de existirem diversos WfMS comerciais com ferramentas para dar suporte à modelagem e, posterior, reengenharia dos processos de negócio, os recursos fornecidos para a reengenharia de processos que são voltados para a análise, teste e monitoramento de workflow, não levam em consideração os mecanismos de coordenação de atividades, inerentes à estrutura organizacional existente.  Busca-se, através deste trabalho, estudar o problema do processo de reengenharia e da otimização de processos, a partir da análise de (sub)processos de WfMS, considerando alguns aspetos estruturais das organizações. Neste contexto, o objetivo deste trabalho é identificar e descrever, de forma sistemática, o relacionamento existente entre (sub)processos de workflow e mecanismos de coordenação de atividades. Com isso, pretende-se que com os resultados obtidos possam ser utilizados como ponto de partida para o desenvolvimento de um sistema automatizado para auxiliar na reengenharia de processos com base em WfMS.|http://hdl.handle.net/10183/4679
Avaliação das condições de adaptatividade ao usuário de apresentações hipermídia|2002|Open Access|Dissertação|Informática : Educação;Hiperdocumento;Adaptação;Ensino a distância|por|O volume cada vez maior de informações que as pessoas estão precisando ter acesso, as especificidades dos ambientes onde elas encontram-se e suas características individuais são responsáveis pelo crescimento da área hipermídia adaptativa. Os sistemas desta área tentam antecipar as necessidades e desejos dos usuários, entregando-lhes um sistema o mais personalizado possível. Este trabalho distingue sistema hipermídia adaptável de sistema hipermídia adaptativo, conforme a participação do usuário na indicação de suas características ao sistema. Qualquer um destes sistemas podem ser usados em áreas tais como hipermídia educacional, informação on-line, ajuda on-line, recuperação de informações, informação institucional e gerenciamento de visões personalizadas em espaços de informações. Um sistema hipermídia com característica de adaptatividade deve considerar dados do usuário, dados de utilização e dados do ambiente. Os dados do usuário são considerados na maioria dos sistemas e usados para construir o modelo do usuário, que é uma das principais partes de um sistema da área hipermídia adaptativa. O modelo do usuário geralmente mantém informações sobre preferências, conhecimento, interesse, objetivos, background e traços de personalidade do usuário. Além do modelo do usuário, existem também o modelo do domínio e as formas de adaptação presentes em muitos modelos. Por exemplo, o modelo AHAM (Adaptive Hypermedia Application Model) define através de regras os referidos modelos e é apresentado de forma sucinta neste trabalho. As formas de adaptação de um sistema hipermídia são realizadas, usando duas tecnologias principais de adaptação: apresentação adaptativa, que inclui todas as técnicas que adaptam o conteúdo de uma página de acordo com o modelo do usuário e suporte à navegação adaptativa, que abrange as técnicas que manipulam os links de uma apresentação para adequar o hiperespaço navegacional conforme o modelo do usuário. Atualmente, existem diversos sistemas com ferramentas de autoria que permitem construir apresentações adaptativas e controlar seu processo de adaptação. Um dos sistemas que considera a maior quantidade de características adaptativas do usuário e que está disponível para as plataformas mais difundidas é o sistema AHA! (Adaptive Hypermedia Architecture) que é apresentado neste trabalho. A percepção de que o uso de uma ferramenta de autoria para hipermídia adaptativa não garante que uma apresentação seja construída com alto grau de adaptatividade impulsionou o desenvolvimento de um sistema chamado AdaptAvaliador, capaz de avaliar o grau de adaptatividade de uma apresentação em diversos aspectos. Devido às diferenças de sintaxe das apresentações criadas por diferentes ferramentas de autoria, o AdapAvaliador apenas avalia apresentações construídas no AHA!. As características, arquitetura e funcionamento do sistema de avaliação de apresentações hipermídia adaptativas são apresentadas em detalhes no presente trabalho. Para testar a eficácia do AdaptAvaliador foi desenvolvido um curso, usando o AHA!, e submetido para avaliação pelo AdaptAvaliador. O curso e os resultados obtidos com a avaliação são também apresentados neste trabalho.|http://hdl.handle.net/10183/4680
Desenvolvimento de procedimento analítico para determinação de fármacos e pesticidas em amostra aquosas ambientais|2005|Open Access|Tese|Análise de traços;Fármacos|por|O presente trabalho conjuga estudo de fármacos e pesticidas em nível de traços como poluentes no meio aquoso, envolvendo o uso de cromatografia líquida com detector espectrofotométrico na região do ultravioleta (HPLC-UV) para fármacos e cromatografia líquida com detector espectrométrico de massas (LC-ESI-MS-MS) para pesticidas. Quatro fármacos (antibióticos), a saber: ampicilina, amoxilina, tetraciclina e cefalexina, foram avaliados em presença de diversas fases móveis, que foram selecionadas de acordo com a melhor performance cromatográfica, sendo que ampicilina e amoxilina apresentaram melhor performance cromatográfica na mesma fase móvel. Para extração em fase sólida dos fármacos amoxilina e ampicilina foram avaliados três fases comerciais: LC- 18 (SUPELCLEAN), Supelclean TM ENVI TM-Chrom P, Abselut NEXUS. Para os fármacos cefalexina e tetraciclina, além das fases comercias foram avaliadas três novas fases a base de sílicas funcionalizadas com zirconocenos: (MeCp)2ZrCl2, (nBuCp)2ZrCl2 e (iBuCp)2ZrCl2. Amoxilina e ampicilina não apresentaram boas recuperações nas fases e eluentes estudados. Para tetraciclina e cefalexina, a fase comercial Chrom P apresentou boas recuperações quando utilizados os solventes metanol e a mistura 1% de solução aquosa de ácido acético e metanol (60:40), respectivamente. Com relação aos pesticidas, diversas classes (triazinas, feniluréias, organofosforados, anilina, ácidos, molinato e propanil) foram estudadas, somando um total de 20 pesticidas. Três colunas foram avaliadas em diferentes gradientes. Foi selecionada a coluna Purospher START RP e o gradiente acetonitrila-água. Triazinas, feniluréias, organosfosforados, anilinas e o pesticida propanil foram determinadas em modo de ionização spray eletrônico positivo, os pesticidas ácidos e propanil foram determinados em modo de ionização spray eletrônico negativo.  Para cada pesticida foi encontrada duas transições (íon produtor-íon produzido) e confirmadas por MRM (Multiple Reaction Monitoring). O método apresentou linearidade com coeficientes de correlação maiores que 0,99. Uma metodologia para extração em fase sólida on line foi também desenvolvida. Essa metodologia apresentou-se altamente sensível (limites de detecção entre 0,040 e 2,794 ng L-1), simples rápida (45 minutos por análise) e precisa (desvio padrão relativo entre 1,99 e 12,15 %).|http://hdl.handle.net/10183/4683
DOMonitor: um ambiente de monitoração de aplicações distribuídas Java|2002|Open Access|Dissertação|Objetos distribuidos;Java (Linguagem de programação);Programação orientada : Objetos;Programacao distribuida|por|A linguagem de programação Java vem sendo uma das escolhidas para a implementação de aplicações compostas por objetos distribuídos. Estas aplicações caracterizam-se por possuir comportamento complexo e, portanto, são mais difíceis de depurar e refinar para obter melhores desempenhos. Considerando a necessidade do desenvolvimento de uma ferramenta de monitoração para o modelo de objetos distribuídos, que colete informações mais detalhadas sobre a execução da aplicação, é apresentado neste trabalho um ambiente de monitoração de aplicações distribuídas escritas em Java, o DOMonitor. Um dos objetivos do DOMonitor é obter o comportamento que a aplicação apresenta durante a execução, possibilitando a detecção de comportamentos equivocados e seu respectivo refinamento. O DOMonitor é voltado para aplicações compostas por objetos distribuídos e caracteriza-se por identificar principalmente: (i) o comportamento dinâmico das threads; (ii) a utilização dos métodos de sincronização; e (iii) a comunicação entre os entes distribuídos da aplicação. O DOMonitor está fundamentado em quatro premissas: (i) ser transparente para o usuário, não exigindo anotações no código fonte; (ii) apresentar uma organização modular, e por isto ser flexível e expansível; (iii) ser portável, não exigindo nenhuma alteração na Maquina Virtual Java; e (iv) operar de forma a garantir a ordem dos eventos previstos pelo programa. Os dados produzidos pelo DOMonitor podem ser utilizados com diversas finalidades tais como visualização da execução, escalonamento e como suporte à execução de aplicações móveis. Para comprovar esta versatilidade, foi proposta a integração do sistema a dois outros projetos, o Pajé e o ISAM. O projeto ISAM utilizará os dados monitorados para tomadas de decisão durante o curso da execução e o projeto Pajé permite a visualização gráfica das características dinâmicas de uma aplicação Java.|http://hdl.handle.net/10183/4688
Arquitetura para um ambiente de grade computacional direcionado às aplicações distribuídas, móveis e conscientes do contexto da computação pervasiva|2004|Open Access|Tese|Computação móvel;Computação pervasiva;Computação em grade|por|Neste início de década, observa-se a transformação das áreas de Computação em Grade (Grid Computing) e Computação Móvel (Mobile Computing) de uma conotação de interesse emergente para outra caracterizada por uma demanda real e qualificada de produtos, serviços e pesquisas. Esta tese tem como pressuposto a identificação de que os problemas hoje abordados isoladamente nas pesquisas relativas às computações em grade, consciente do contexto e móvel, estão presentes quando da disponibilização de uma infra-estrutura de software para o cenário da Computação Pervasiva. Neste sentido, como aspecto central da sua contribuição, propõe uma solução integrada para suporte à Computação Pervasiva, implementada na forma de um middleware que visa criar e gerenciar um ambiente pervasivo, bem como promover a execução, sob este ambiente, das aplicações que expressam a semântica siga-me. Estas aplicações são, por natureza, distribuídas, móveis e adaptativas ao contexto em que seu processamento ocorre, estando disponíveis a partir de qualquer lugar, todo o tempo. O middleware proposto, denominado EXEHDA (Execution Environment for Highly Distributed Applications), é adaptativo ao contexto e baseado em serviços, sendo chamado de ISAMpe o ambiente por este disponibilizado. O EXEHDA faz parte dos esforços de pesquisa do Projeto ISAM (Infra-Estrutura de Suporte às Aplicações Móveis Distribuídas), em andamento na UFRGS. Para atender a elevada flutuação na disponibilidade dos recursos, inerente à Computação Pervasiva, o EXEHDA é estruturado em um núcleo mínimo e em serviços carregados sob demanda. Os principais serviços fornecidos estão organizados em subsistemas que gerenciam: (a) a execução distribuída; (b) a comunicação; (c) o reconhecimento do contexto; (d) a adaptação; (e) o acesso pervasivo aos recursos e serviços; (f) a descoberta e (g) o gerenciamento de recursos  No EXEHDA, as condições de contexto são pró-ativamente monitoradas e o suporte à execução deve permitir que tanto a aplicação como ele próprio utilizem essas informações na gerência da adaptação de seus aspectos funcionais e não-funcionais. O mecanismo de adaptação proposto para o EXEHDA emprega uma estratégia colaborativa entre aplicação e ambiente de execução, através da qual é facultado ao programador individualizar políticas de adaptação para reger o comportamento de cada um dos componentes que constituem o software da aplicação. Aplicações tanto do domínio da Computação em Grade, quanto da Computação Pervasiva podem ser programadas e executadas sob gerenciamento do middleware proposto.|http://hdl.handle.net/10183/4689
Algumas generalizações para o último teorema de Fermat|2005|Open Access|Dissertação|Teorema de fermat|por|Neste trabalho estudamos três generalizações para o último Teorema de Fermat. A primeira generalização trata de expoentes negativos e de expoentes racionais. Além de mostrar em que casos estas equações possuem soluções, damos uma caracterização completa para todas as soluções inteiras não-nulas existentes. A segunda generalização também trata de expoentes racionais, porém num contexto mais amplo. Aqui permitimos que as raízes n-ésimas sejam complexas, não necessariamente reais. Na terceira generalização vemos que o último Teorema de Fermat também vale para expoentes inteiros gaussianos.;In this work we study three extensions of Fermat's Last Theorem. The first extension deals with negative and rational exponents. Here we show when these equations have nonzero integral solutions and we characterize these solutions when they exist. The second extension also deals with rational exponents, but in a wider context. Here we allow the use of complex roots, not necessarily the real ones. In the third extension we show that Fermat's Last Theorem also holds for Gaussian integer exponents.|http://hdl.handle.net/10183/4691
Sobre anéis e módulos distributivos|2005|Open Access|Dissertação|Aneis distributivos;Aneis : Modulos|por|Este trabalho tem por objetivo apresentar resultados sobre módulos e anéis distributivos. Trataremos de algumas caracterizações e propriedades desta classe de módulos. O teorema principal nos dá uma caracterização sobre módulos e anéis distributivos através de seus submódulos e ideais saturados.;This work has for objective to present resulted on modules and distributive rings. We will deal with some characterizations and properties of this class of modules. The main theorem in gives to a characterization on modules and distributive rings through of its saturated submodules and ideals to them.|http://hdl.handle.net/10183/4693
Adesão de filme auto-sustentado de diamante CVD em metal duro|2004|Open Access|Tese|Física da matéria condensada|por|O objetivo deste trabalho foi desenvolver conhecimento científico e tecnológico sobre o mecanismo de adesão de filmes de diamante CVD em metal duro. Foi estudado de forma sistemática o processo de brasagem reativa convencional em vácuo, com liga de AgCuTi, onde foram avaliados os efeitos do tempo, da temperatura e da atmosfera durante o processo de brasagem. O grau de aderência foi avaliado por um ensaio de tração mecânica adaptado. Os melhores resultados foram obtidos para brasagem em vácuo, em temperaturas na faixa de 900 a 920 oC e tempos de 3 a 20 minutos. Tendo em vista as limitações da brasagem em vácuo, relacionadas à reatividade do elemento reativo, o Ti, a baixa resistência da liga (à base de cobre e prata, que são metais macios) e a necessidade de uma atmosfera controlada para evitar a oxidação do próprio diamante na faixa de temperatura utilizada, foi desenvolvido neste trabalho um novo método de Adesão e brasagem de filmes auto-sustentados de diamante CVD em metal duro. Este método utiliza altas pressões e altas temperaturas para promover a adesão, utilizando câmaras simples e de baixo custo. Os resultados obtidos mostraram ser possível unir diamante ao metal duro com ou sem o material de adição, em condições de pressão e temperatura adequadas. As vantagens de utilizar alta pressão relacionam-se à possibilidade de manter o diamante estável em altas temperaturas, minimizar as falhas de preenchimento na interface entre o diamante e o metal duro, facilitar a difusão na interface, através das fronteiras de grão, promover o ancoramento e minimizar a espessura da película de brasagem quando utilizado o material de adição  O grau de aderência das amostras foi avaliado através de um ensaio de cisalhamento e os resultados foram excelentes. No caso da adesão direta em altas pressões, sem o material de adição, o mecanismo responsável pela aderência provavelmente está relacionado ao ancoramento induzido pela pressão e à difusão do cobalto, presente no metal duro, para a interface com o diamante, servindo como uma espécie de “cola”. Entretanto, esta “cola” atua como catalisador para o carbono e pode interferir na estabilidade do filme de diamante. De fato, para adesão direta a 2,5 GPa e 1200 oC, observou-se a completa grafitização do filme de diamante. Este grafite apresentou alto grau de cristalinidade e de orientação preferencial, com planos basais não paralelos à superfície de interface. Para adesão direta a pressões dentro da região de estabilidade termodinâmica do diamante, este permaneceu íntegro e, inclusive, observou-se uma melhora na cristalinidade para adesão direta a 7,7 GPa e 1500 oC. Na adesão direta em alta pressão, é preciso levar em consideração a cinética do processo de adesão, relacionada à difusão do cobalto, e a estabilidade de fase do diamante. Na brasagem em alta pressão, foram obtidos resultados positivos para processamentos a 2,5 GPa e 4 GPa, na mesma faixa de temperatura utilizada na brasagem convencional. Como estas temperaturas são menores, não houve grafitização do filme a 2,5 GPa. A 7,7 GPa, a liga ficou completamente aderida ao diamante e não houve a união com o metal duro. Isso ocorreu pela diferença de compressibilidade da liga em relação ao diamante e ao metal duro  Foram produzidas algumas ferramentas utilizando a brasagem convencional, as quais foram afiadas e testadas em condições de usinagem, com resultados satisfatórios.|http://hdl.handle.net/10183/4699
Nanoestruturas luminescentes de Ge e Sn em camadas de SiO/sub 2/ implantadas com íons|2005|Open Access|Tese|Materia condensada;Implantacao ionica|por|Neste trabalho estudam-se as propriedades de nanoestruturas de Ge e Sn formadas em amostras de SiO2/Si(100) através dos processos de implantação iônica e tratamento térmico. A formação de nanocristais de Ge foi investigada em função de tratamentos térmicos em ambiente de N2. Os resultados obtidos foram correlacionados com as propriedades de luminescência das amostras, sendo feita uma discussão sobre os mecanismos atômicos envolvidos no processo de crescimento dos nanocristais de Ge, bem como seus efeitos na criação de centros luminescentes no interior da camada de SiO2, que são responsáveis por intensas bandas de fotoluminescência (PL) nas regiões espectrais do azul-violeta (≈ 3,2 eV) e ultravioleta (≈ 4,2 eV). Além disso, experimentos de irradiação com diferentes íons (He+, Si+, Kr++, Au+) foram realizados antes da implantação do Ge com o objetivo de estudar o efeito de memória que os danos criados pela irradiação apresentam sobre as propriedades estruturais e luminescentes das amostras de SiO2/Si(100)  No estudo das amostras de SiO2/Si(100) implantadas com Sn, a síntese de nanopartículas de Sn foi estudada em função da temperatura e do ambiente de tratamento térmico (N2 e vácuo). De maneira pioneira mostrou-se que através da manipulação desses parâmetros é possível formar desde grandes nanocristais bi-fásicos de Sn (≈ 12 a 25 nm) em estruturas concêntricas com núcleo de β-Sn e camada externa de SnOx, até pequenas nanopartículas de Sn com diâmetros de ≈ 2 nm e uniformemente distribuídas ao longo da camada de SiO2. Além disso, observou-se que a evolução estrutural do sistema de nanopartículas de Sn influencia diretamente as características das emissões de PL azul-violeta e UV. Por fim, um outro aspecto das nanoestruturas de Sn foi estudado: a formação de um denso arranjo de ilhas epitaxiais de β-Sn na região de interface SiO2/Si. Este sistema de nano-ilhas, que cresce epitaxialmente, é uniformemente distribuído sobre a superfície do Si, apresentando uma pequena dispersão em tamanho e tendência a se auto-organizar. A criação desse sistema de nano-ilhas epitaxiais através da utilização da implantação iônica é um processo inédito, sendo discutida aqui com base nas propriedades de equilíbrio do sistema Sn-Si.|http://hdl.handle.net/10183/4702
O Uso de árvores de decisão na descoberta de conhecimento na área da saúde|2003|Open Access|Dissertação|Banco : Dados;Descoberta : Conhecimento;Mineracao : Dados;Arvore : Decisao;Informática médica|por|As árvores de decisão são um meio eficiente para produzir classificadores a partir de bases de dados, sendo largamente utilizadas devido à sua eficiência em relação ao tempo de processamento e por fornecer um meio intuitivo de analisar os resultados obtidos, apresentando uma forma de representação simbólica simples e normalmente compreensível, o que facilita a análise do problema em questão. Este trabalho tem, por finalidade, apresentar um estudo sobre o processo de descoberta de conhecimento em um banco de dados relacionado à área da saúde, contemplando todas as etapas do processo, com destaque à de mineração de dados, dentro da qual são aplicados classificadores baseados em árvores de decisão. Neste estudo, o conhecimento é obtido mediante a construção de árvores de decisão a partir de dados relacionados a um problema real: o controle e a análise das Autorizações de Internações Hospitalares (AIHs) emitidas pelos hospitais da cidade de Pelotas, conveniados ao Sistema Único de Saúde (SUS). Buscou-se encontrar conhecimentos que auxiliassem a Secretaria Municipal da Saúde de Pelotas (SMSP) na análise das AIHs, realizada manualmente, detectando situações que fogem aos padrões permitidos pelo SUS. Finalmente, os conhecimentos obtidos são avaliados e validados, possibilitando verificar a aplicabilidade das árvores no domínio em questão.|http://hdl.handle.net/10183/4703
Ciclos sedimentares em seqüências siliciclásticas : uma proposta de análise metodológica.|2000|Open Access|Tese|Estratigrafia : Sedimentologia;Seqüências siliciclásticas|por|Este trabalho de estratigrafia quantitativa teve como objetivo o estudo de ciclos sedimentares em seqüências siliciclásticas. Para isso, utilizou-se ferramentas matemáticas e estatísticas, interpretando os resultados obtidos no contexto da estratigrafia de seqüências. Os padrões quase cíclicos de empilhamento sedimentar foram associados a padrões de ciclos de tempo conhecidos – os da banda de Milankovitch (Milankovitch, 1947). Para superar as dificuldades inerentes às medidas diretas em afloramentos e testemunhos, adotou-se o estudo das variações da distância entre marcas consecutivas, observadas na curva do perfil de raios gama de sondagens para petróleo ou carvão. Esta distância foi associada à espessura da camada sedimentar e a série de observações foi estudada pelos métodos de análise de séries temporais, empregando-se: estatísticas básicas, histogramas e distribuições de freqüência, diagramas de tempo (gráficos XY), gráficos de Fischer, autocorrelação e correlação cruzada e análise espectral. A abordagem do problema exigiu um tratamento matemático das observações estratigráficas, mantido de forma “orientada para a geologia”. Deu-se ênfase ao significado físico (geológico) dos resultados obtidos com as diversas análises.  As variações nas espessuras das camadas permitiram reconhecer parasseqüências e suas geometrias internas, levando à identificação acurada dos ambientes deposicionais, das fácies e dos tratos de sistema, em um contexto de estratigrafia de seqüências. As razões entre os períodos encontrados nos ciclos sedimentares foram associadas com os ciclos astronômicos da banda de Milankovitch, produzindo estimativas do tempo de deposição e das taxas de acumulação e fornecendo a visão dos processos de preenchimento da bacia, das oscilações do nível do mar e do fluxo de sedimentos. O emprego desta metodologia de análise evidenciou seqüências de quinta e de quarta ordem (no sentido da Exxon) correlacionáveis, localmente. Em nível regional, mostrou ser possível a correlação de seqüências de terceira ordem, por distâncias consideráveis, permitindo correlações com a curva global de oscilações do nível do mar. Para ilustrar, foram discutidos exemplos de aplicação da metodologia, em seções do Permiano da Bacia do Paraná e do Andar Buracica (Barremiano), na Bacia do Recôncavo. A metodologia do trabalho foi desenvolvida pelo autor, junto aos participantes de pesquisas e de cursos do Laboratório de Análise de Bacias e Correlação Geológica (LABCG) da Faculdade de Geologia (FGEL) da UERJ - Universidade do Estado do Rio de Janeiro.|http://hdl.handle.net/10183/4739
Metalogênese do depósito de Cu Cerro dos Martins, RS.|2004|Open Access|Dissertação|Geoquímica;Metalogenia;Petrografia;Isótopos|por|Este trabalho revisa a geologia e apresenta dados inéditos do Depósito de Cobre Cerro dos Martins (DCM), incluindo geocronologia Pb-Pb em zircão, inclusões fluidas, isótopos estáveis (C, O e S), composição isotópica do Sr e geoquímica de elementos maiores e traços das rochas vulcânicas encaixantes. O depósito está hospedado na seqüência vulcano-sedimentar do Grupo Bom Jardim, da Bacia do Camaquã, do Neoproterozóico do Escudo Sul Rio-grandense, e possui reservas calculadas de 1.450.000 t, com teor médio de 0,83% Cu. O depósito consiste de um conjunto de veios sulfetados que preenchem fraturas de direção N40º-60ºW em rochas andesíticas e sedimentares clásticas, com disseminações confinadas em níveis de siltito, arenito, andesito e conglomerado, da Formação Hilário do Grupo Bom Jardim. Os minerais do minério filoneano são a calcosina e bornita com calcopirita, pirita, galena e esfalerita subordinadas. Digenita, covelita, malaquita cuprita e azurita ocorrem como minério secundário em ganga constituída de carbonatos, quartzo, minerais argilosos, barita e rara hematita.  A composição química das vulcânicas (elementos maiores e traços, incluindo ETR) indicam uma afinidade alcalina para o vulcanismo relacionado à Formação Hilário na região do Cerro dos Martins. Um corpo de quartzo-diorito, intrusivo nas rochas vulcânicas e sedimentares, mostrou idade de 550 ±5 Ma (Pb-Pb em zircões) indicando um valor mínimo para a geração do minério do DCM. Esta idade confirma a posição estratigráfica desta rocha na Formação Acampamento Velho e também fornece uma idade mínima para a deposição da seqüência vulcano-sedimentar encaixante do DCM. Os sulfetos do DCM mostram δS34 CDT com valores relativamente homogêneos entre - 6.2 e + 0.9‰ (n= 7). O valor de δS34 CDT da calcopirita, levemente positivo (+0.9‰), indica uma origem magmática para o S, mas os valores negativos encontrados nestes sulfetos, poderiam indicar o envolvimento de outras fontes com enxofre reduzido. Entretanto, a presença de hematita nas paragêneses minerais indica que o minério foi formado sob condições oxidantes, modificando a composição isotópica original do enxofre magmático (δS34 CDT ~ 0‰) para valores negativos. As baritas analisadas apresentam valores com δS34 CDT entre +9.25 e +10.65‰ (n=4) indicando deposição em condições oxidantes, originadas pela mistura de um fluido magmático-hidrotermal com água meteórica.  A composição isotópica do C das calcitas do DCM varia com δC13 PDB entre - 1,90 a -4,45‰, interpretada como resultante da mistura entre carbono de fonte magmática com mármores do embasamento. Inclusões fluidas em quartzo do minério indicam temperaturas de deposição entre 157 e 273 °C com mediana de 215 °C (n = 45). A composição isotópica do oxigênio da água em equilibrio com a calcita do fluido hidrotermal (T= 215 °C) mostra valores de δ O18 SMOW entre 3 e 14, indicando H2O de origem magmática, com contribuição de água meteórica. A razão Sr87/Sr86 das mesmas calcitas mostram valores entre 0,7068 – 0,7087, de crosta superior. Rochas plutônicas e vulcânicas do escudo com idades próximas de 550 Ma possuem razões iniciais Sr87/Sr86 entre 0,704 – 0,710, compatíveis com aquelas encontradas nas calcitas da mineralização. Os fluidos hidrotermais do magmatismo shoshonítico-alcalino com idade de 595 Ma e Sr87/Sr86 entre 0,7041 a 0,7053, também são candidatos a fonte do Sr dos carbonatos hidrotermais, mas necessitariam de um componente mais radiogênico. Assim, a fonte de C-O e Sr das calcitas do minério pode ter sido originada diretamente de um fluido magmático-hidrotermal ou de uma mistura entre este fluido e mármores do embasamento. Portanto, o depósito Cerro dos Martins é interpretado como de origem magmática-hidrotermal, relacionado ao evento magmático alcalinoshoshonítico, pós-colisional da Orogênese Dom Feliciano, com idade entre 595-550 Ma. Novos modelos exploratórios para depósitos de cobre no Escudo do Rio Grande do Sul devem considerar o magmatismo alcalino na gênese dos depósitos.|http://hdl.handle.net/10183/4757
Placas maxilo-dentárias em Rincossauros Hyperodapedon Huxley, 1859. Histologia e morfogênese maxilo-dental: nova abordagem.|2004|Open Access|Dissertação|Hyperodapedon;Dental;Plate;Differentiation;Ontogenesis;Maxillary;Paleovertebrados;Rincossauros : Dentição;Ontogenese : Hyperodapedon : Maxilo-dental|por|O estudo das estruturas da placas maxilo-dentárias dos rincossauros Hyperodapedon Huxley, 1859, do Triássico da Formação Santa Maria, em nova abordagem histológica e ontogênica resultou na identificação da natureza do esmalte aprismático verdadeiro, de variados elementos histológicos dentinários e osteológicos, e de um centro de ossificação periosteal primário. Também foram encontradas evidências histológicas dos mecanismos de fusionamento maxilo-dentinário e amelo-maxilar. Com estes elementos, inferimos os modelos de organogênese dental, da ontogênese maxilar e dos mecanismos de fusionamento maxilodental. Encontrou-se uma singular e raríssima coroa dental, imatura e ainda não erupcionada, na região posterior da placa dental e assim evidenciou-se a correta posição da margem odontogenicamente ativa. Adicionalmente inferiu-se a localização da posição da lâmina dentária embrionária. Constatou-se a não formação de alvéolos dentários, de cemento radicular e do espaço necessário à formação do ligamento periodontal e, assim, se deduziu a não formação do folículo dental embriônico.  As presenças de especiais elementos anatômicos e histológicos nos tecidos ósseos periapicais evidenciam o crescimento radicular contínuo, enquanto a forma e o fusionamento radicular imediato depõe a favor de uma função dentária fisiológica diferenciada para as baterias dentárias maxilares dos Rincossauros do gênero Hyperodapedon. Os mecanismos que possibilitaram o controle embriônico para a deposição das lamelas de tecido ósseo coronal e seu preciso fusionamento sobre o esmalte dentário, declinam por modificações nas funções tardias do órgão reduzido do esmalte e pela presença de uma membrana oral com funções osteogênicas e também protetivas, situada nas porções posteriores da placa maxilo-dentária em desenvolvimento. Mudanças heterocrônicas no tempo de diferenciação das células da crista neural embriônica e em seus derivados, como a lâmina dentária e órgãos dentários embrionários ou correlacionadas com a organogênese das placas maxilo-dentárias e seus anexos periodontais, todos como condições plesiomórficas para Diápsidas Triássicos, poderiam ser as causas responsáveis pela origem e evolução deste estranho aparelho estomatognático nos clados de Hyperodapedon sp..;The study of the maxillary tooth-plate of Hyperodapedon Huxley, 1859, rhynchosaurs of the Triassic from Santa Maria Formation, in a new histological and odontogenic researches, resulted in a identification the nature of the true non prismatic (aprismatic) enamel, of a variety of dental and osteological elements, and of one primary periosteal center of ossification. We found histological evidences of the maxillary-dentinary and maxillary-enamel attachment process, and with these elements we infer the patterns of dental organogenesis, maxillary ontogenesis, and the embryonic mechanisms of maxillarydental junction. We found one rare immature and unerupted dental crown in the posterior dental-plate and so we turn evident the correct position of the maxillary odontogenic active border, and additionally we determine the correct position of embryonic dental lamina. This work constates that the dental sockets, dental cement and periodontal space are absenting in Hyperodapedon sp. maxillary tooth-plate and thus we deduce the dental embryonic follicle non-formation. The presence of the anatomics and histological osteal elements of periapical’s zone evidences the continuous radicular growth, while the form and the immediate radicular fusioning witness favorably of physiologically different teeth function for the dental-plate battery in Hyperodapedon rhynchosaur’s genus.  The mechanisms that permit the embryonic control at the deposition of lamellar osteal coronal tissues and their precise fusioning on the dental enamel, testified the modifications of the late functions of the reducted enamel organ and the presence of oral mucous covertures, with present’s osteogenic and protetive activities. These mucous covertures should be situated in the posterior region of the development of the maxillary tooth plates. Heterochronic changes in the timing of differentiation of the embryonic neural crests cells and on their derivers with the dental lamina and dental organs or correlationed with the ontogenesis of the maxillary tooth plates and periodontal annex’s, would be responsible for the origin and evolution of this stranger stomatognatic apparel in Hyperodapedon sp. clades. These changes reflect plesiomorphic condition in relation the others Triassic Diapsides.|http://hdl.handle.net/10183/4760
Avaliação das alterações ambientais causadas por perfuração exploratória em talude continental a partir de dados geoquímicos - Bacia de Campos, Brasil.|2005|Open Access|Dissertação|Geologia marinha : Geoquímica : Campos, Bacia de;Geoquímica : Impacto ambiental : Perfuraçao exploratória|por|As atividades de exploração e produção de petróleo e gás no Brasil têm se tornado mais intensas ao longo desta última década, apresentando uma tendência de avanço em direção a ambientes de maior profundidade, onde se localizam grande parte das reservas de óleo já comprovadas. Os processos de exploração e produção de petróleo e gás apresentam muitas etapas com considerável potencial gerador de impactos ao meio ambiente, entre elas, a perfuração de poços exploratórios marítimos, objeto do presente estudo. Este estudo originou-se do Projeto MAPEM – Monitoramento Ambiental em Atividades de Perfuração Exploratória Marítima (Águas Profundas), do qual foram utilizados os dados analisados nesta dissertação. O monitoramento foi realizado visando avaliar os efeitos da perfuração do poço Eagle, localizado em talude continental, região norte da Bacia de Campos, Brasil, próximo ao limite entre os estados do Rio de Janeiro e Espírito Santo, a 902 metros de profundidade. Foram coletadas amostras de sedimentos superficiais na região de entorno da atividade, em 48 estações de monitoramento e 6 estações de controle, durante os cruzeiros oceanográficos realizados um mês antes, um mês após e um ano após a perfuração. As análises dos sedimentos geraram informações sobre sua composição granulométrica, mineralógica (argilominerais e carbonato de cálcio) e química (hidrocarbonetos e metais), e foram comparadas em sua variação espacial (área de monitoramento/estações de controle) e temporal (3 cruzeiros oceanográficos). A variação temporal foi abordada de três maneiras distintas, onde o Cruzeiro I representou o background da área, a variação do Cruzeiro I para o II representou o impacto sobre a área de monitoramento e na variação do Cruzeiro II para o III, buscou-se evidências de recuperação da área monitorada com tendência de retorno às suas características iniciais  O background da área definiu os níveis médios de todas variáveis analisadas, identificando, além de teores naturais para alguns dos componentes dos sedimentos, sinais de contaminação de origem antrópica, principalmente de As, Pb e hidrocarbonetos petrogênicos (n-alcanos) e pirogênicos (aromáticos). Na perfuração do poço Eagle foi utilizado fluido de base aquosa (FBA) e fluido de base sintética (FBS), dos quais se buscou identificar as áreas de influência e as alterações causadas nos sedimentos. Identificou-se a ocorrência de um fluxo gravitacional de massa, no período entre o Cruzeiro I e a perfuração, restrito ao cânion submarino que cruza a área de monitoramento, do qual também foi avaliada a influência sobre a composição dos sedimentos. A influência do FBA (indicada pelos teores de bário) estendeu-se por uma grande área, apresentando maiores concentrações nas estações próximas do poço. A área de influência do FBS (indicada pelos n-alcanos entre C14 e C22) apresentou distribuição mais restrita, em duas manchas, uma a norte e outra a oeste do poço. Além dos n-alcanos, foi identificado aumento dos teores de bário, UCM (mistura complexa não resolvida), fluoreno e acenaftaleno. O fluxo gravitacional de massa causou elevações na proporção de areia das estações do cânion submarino e redução do carbono orgânico. Efetivamente pode se concluir que a atividade de perfuração exerceu influência significativa nas propriedades químicas dos sedimentos, contudo, de improvável efeito tóxico sobre a biota. Pode-se concluir também que ocorreu recuperação da área, após o período de um ano, por redução das concentrações médias de algumas variáveis e sinais de reposição de n-alcanos naturais, porém não foi possível a identificação de degradação do material sintético utilizado no FBS.|http://hdl.handle.net/10183/4762
Proposta de conjunto de simulações para análise de desempenho de processadores superescalares e ensino de arquitetura de computadores|2004|Open Access|Dissertação|Simulação computacional;Arquiteturas super escalares|por|O objetivo deste trabalho é a definição de um conjunto de roteiros para o ensino de arquitetura de computadores com enfoque em arquiteturas superescalares. O procedimento é baseado em simulação e verificação da influência dos parâmetros arquiteturais dos processadores, em termos funcionais e de desempenho. É dada ênfase a conceitos como memória cache, predição de desvio, execução fora de ordem, unidades funcionais e etc. Através do estudo e avaliação dos parâmetros que constituem estes conceitos, procurava-se através dos roteiros identificar as configurações com melhor desempenho. Para a implementação destes roteiros é dotado o conjunto de ferramentas de simulação SimpleScalar. Este conjunto, além de estar disponibilizado em código aberto na página oficial das ferramentas, traz como vantagem a possibilidade de alteração do código para fins de pesquisa. Este trabalho e os roteiros que o compõem têm como objetivos auxiliar professores e estimular os alunos através de simulações, como forma didática de testar conceitos vistos em sala de aula. Os roteiros são apresentados com os respectivos resultados de simulação e incrementados com comentários e sugestões de um conjunto de perguntas e respostas para que o trabalho possa ter continuidade necessária, partindo da sala de aula para a simulação, busca de respostas e culminando com um relatório final a ser avaliado.|http://hdl.handle.net/10183/4763
Critérios de avaliação de técnicas de visualização de informações hierárquicas|2003|Open Access|Tese|Computação gráfica;Visualização|por| Após o refinamento, os critérios foram utilizados experimentalmente em três tipos de métodos de avaliação tradicionais, a saber, avaliação heurística, inspeção de conformidade e ensaios de interação. Os resultados obtidos com estas avaliações demonstraram que os critérios de avaliação, definidos neste trabalho, capturaram muito mais problemas relacionados à técnica do que os critérios tradicionalmente usados para avaliar interfaces gráficas. Um resultado promissor, não previsto, foi a constatação que o conjunto de critérios propostos detectaram, também, um número expressivo de problemas de usabilidade na interface, quando da aplicação das sessões de avaliação heurística. Este resultado evidencia a possibilidade de extensão deste conjunto de critérios a fim de avaliar, também, as principais características da interface das técnicas de visualização.|http://hdl.handle.net/10183/4764
Agentes improvisacionais como agentes deliberativos|2004|Open Access|Tese|Inteligência artificial;Agentes : Software;Engenharia : Software|por|Improvisação tem sido considerada uma característica importante para agentes que pretendem operar de maneira consistente com a situação do momento, exibindo um comportamento credível e interessante. A improvisação deve estar presente tanto nos agentes individuais quanto nas sociedades de agentes. Desta maneira, esta tese irá abordar estes dois aspectos da improvisação. Propomos a visão de que, agentes capazes de realizar improvisação, os agentes improvisacionais, são um tipo de agente deliberativo capaz de solucionar problemas por improvisação. Neste sentido, buscamos identificar dentro de uma arquitetura clássica de agentes deliberativos, a arquitetura BDI (belief-desire-intention), a existência e/ou a possibilidade da inclusão de componentes de improvisação nesta arquitetura. Para resolver problemas complexos, estes agentes precisam estar agrupados em sociedades e estas sociedades, por sua vez, precisam produzir comportamentos coerentes. A coordenação é a área da Inteligência Artificial responsável por este objetivo. Propomos que a coordenação de agentes que improvisam pode ser realizada por meio de um processo de direção improvisacional, no sentido usado no contexto do teatro improvisacional. Ao longo deste documento, iremos mostrar nosso entendimento sobre agentes improvisacionais como agentes deliberativos e coordenação como direção improvisacional. Com isto, defende-se nesta tese que o uso da improvisação em agentes improvisacionais possibilita que os agentes improvisem comportamentos interativos, de maneira coerente, melhorando seu desempenho como solucionadores de problemas, criando e mantendo uma ilusão de vida para os agentes interativos e contribuindo para o aperfeiçoamento dos sistemas multiagentes.|http://hdl.handle.net/10183/4769
Modelagem estratigráfica de clinoformas deposicionais : construção e aplicação de um modelo computacional baseado em mecânica estatística|2004|Open Access|Tese|Estratigrafia;Modelagem estratigráfica|por|Sob a premissa de que a geometria do perfil deposicional das clinoformas pode conter informações sobre a composição textural dos sedimentos que as compõem, bem como sobre a energia da coluna d’água, foi desenvolvido um modelo computacional para a análise teórica dos fatores controladores do tipo de curvatura dos foresets das clinoformas, e, por extensão dos taludes submarinos. Um modelo análogo de suspensão dinâmica foi implementado experimentalmente com um programa em código Matlab, denominado MAXWELL, e é classificado como um modelo estratigráfico, bidimensional, analítico, determinístico, dinâmico, parcialmente dedutivo e parcialmente baseado em regras. Contém um modelo de escoamento de fluido em linhas de corrente, e trata indiretamente a tensão de cisalhamento no domínio de um sistema fechado bifásico líquido-vapor análogo, a partir a proposta de analogias entre os processos de evaporação e de suspensão de sedimentos. É uma abordagem baseada na competência de transporte do fluxo d’água, pois considera temperatura e velocidade de fluxo combinado onda-corrente como variáveis controladoras análogas da intensidade de transferência de energia. Os processos deposicionais marinhos são reduzidos a um fenômeno da superfície deposicional, que é considerada análoga a uma interface líquidovapor dentro de um sistema fechado. A equação de distribuição estatística de velocidades moleculares de Maxwell é usada para calcular a proporção de moléculas na fase líquida, considerada análoga à proporção de sedimentos depositados, bem como a proporção na fase vapor, tomada como análoga à proporção de sedimentos em suspensão. O estado energético do sistema é parametrizado por três potenciais: energia interna, como função do tamanho de grão (areia, silte ou argila); energia térmica do meio, como função da energia hidrodinâmica, e energia gravitacional, como função do declive topográfico  As simulações indicam que os principais fatores controladores do perfil deposicional das clinoformas, e por extensão, dos taludes submarinos em geral, são a energia hidrodinâmica da coluna d’água e a granulometria (ou coesão) do sedimento, que podem ser consideradas dois parâmetros comutáveis, isto é, grãos grossos ou coesos podem produzir sobre a geometria das clinoformas o mesmo efeito da baixa energia hidrodinâmica, e vice-versa. Com base no fator de decaimento da energia hidrodinâmica com o aumento da batimetria, foram definidas três ordens de grandeza da intensidade da energia da coluna d’água: baixa energia (10–1), alta x energia (10-2) e muito alta energia (10-3). Com base nesse critério, foram caracterizados quatro tipos de perfis deposicionais de clinoformas: dois tipos sigmoidais e dois tipos exponenciais. Os tipos sigmoidais podem ser de alta energia ou de muito alta energia, e distinguem-se pela granulometria do sedimento e pela distribuição de declividades em relação à dimensão na direção horizontal. Os perfis de tipo exponencial podem ser tanto de baixa energia, quanto de alta energia. A subida relativa do nível do mar afeta o tipo geométrico do perfil deposicional apenas de forma indireta, a partir da retenção da fração grossa do influxo sedimentar na plataforma ou topset. Os principais fatores controladores do ângulo de declividade do perfil das clinoformas são a granulometria do sedimento, a energia hidrodinâmica, a profundidade d’água da bacia e o desvio lateral da corrente de fundo. A dedução da litofácies a partir da geometria das clinoformas parece promissora apenas para os tipos teóricos sigmoidais, que são distintos na forma e no conteúdo sedimentar.|http://hdl.handle.net/10183/4772
Uma Proposta para a representação geométrica de imagens com aplicação em segmentação e compressão|2005|Open Access|Dissertação|Computação gráfica;Segmentacao : Imagem;Compressao : Imagem|por|O presente trabalho descreve uma proposta para a representação geométrica de imagens. Através da subdivisão espacial adaptativa de uma imagem em triângulos, uma representação simplificada da estrutura da imagem pode ser obtida. Demonstramos que a representação gerada é adequada para aplicações como a segmentação e a compressão de imagens. O método de segmentação de imagens desenvolvido faz uso deste tipo de representação para obter resultados robustos e compactos, comparados a outros métodos existentes na literatura, e adequado para aplicações como a detecção, descrição e codificação de objetos. Utilizando uma representação geométrica semelhante a métodos de modelagem de superfícies, criamos um novo método de compressão de imagens que apresenta vantagens em relação a outros métodos existentes, em especial na compressão de imagens sem perdas.|http://hdl.handle.net/10183/4773
Caracterização de filmes ópticos compósitos nano-estruturados, inomogêneos ou anisotrópicos, produzidos por troca iônica e pelo método sol-gel|2003|Open Access|Tese|Filmes oticos;Otica nao-linear;Dispositivos otico-eletronicos;Optoeletronica integrada;Íons;Interação de troca;Indice de refracao;Tecnica de modos guiados;Filmes finos;Configurações moleculares;Birrefringência;Elipsometria;Simetria|por|Este trabalho tem como objetivo o desenvolvimento e a aplicação de métodos de caracterização de filmes ópticos, associados à sua estrutura inomogênea ou anisotrópica. Os materiais estudados são guias ópticos planares e filmes compósitos com propriedades ópticas não-lineares. Esses materiais são relevantes para aplicações na área de optoeletrônica e óptica integrada. O trabalho é dividido em duas partes principais. A primeira parte é dedicada à caracterização de guias de onda planares produzidos por troca iônica, vidros dopados com íons de Ag e/ou K, através de um e/ou dois processos de troca. O perfil de índice de refração é estudado através da técnica de Modos Guiados, uma técnica óptica empregada tradicionalmente em guias desse tipo. Em complementação a essa medida óptica, são realizadas medidas do perfil de concentração do íon dopante, empregando as técnicas de RBS e EDS.  É dedicado um interesse especial pela região próxima à superfície da amostra, a região crítica na análise por Modos Guiados. Os métodos de Abelès-Hacskaylo e de Brewster-Pfund são estendidos a esses guias inomogêneos, permitindo a medida direta do valor do índice de refração superficial. Essa informação e os dados obtidos por Modos Guiados permitem a determinação de um perfil de índice de refração mais acurado ao longo da profundidade do guia. A segunda parte é dedicada ao estudo de materiais compósitos: filmes finos constituídos por uma matriz (silicato, silicato + PVP, e PMMA) dopada com moléculas orgânicas que apresentam propriedades ópticas não-lineares de segunda ordem (PNA, DR-1 e HBO-BO6). Nessas amostras, é aplicado um campo elétrico de alta voltagem (efeito corona), gerando um alinhamento dos cromóforos dopantes. Essa mudança na simetria estrutural do material, de isotrópica para uniaxial, é observada através da assimetria correspondente no valor do índice de refração (birrefringência). O valor da birrefringência induzida é obtido através da medida da variação da refletância de luz pelo material, auxiliada por medidas prévias das constantes ópticas do material por Elipsometria.|http://hdl.handle.net/10183/4775
Suporte a argumentos de consulta vagos através da linguagem XPath|2005|Open Access|Dissertação|Consulta : Banco : Dados;Linguagens : Consulta;Banco : Dados|por|Abordagens clássicas de linguagens de consultas para bancos de dados possuem certas restrições ao serem usadas, diretamente, por aplicações que acessam dados cujo conteúdo não é completamente conhecido pelo usuário. Essas restrições geram um cenário onde argumentos de consultas, especificados com operadores boleanos, podem retornar resultados vazios. Desse modo, o usuário é forçado a refazer suas consultas até que os argumentos usados estejam idênticos aos dados armazenados no banco de dados. Em bases XML, este problema é reforçado pela heterogeneidade das formas em que a informação encontra-se armazenada em diferentes lugares. Como solução, uma alternativa seria o uso de funções de similaridade na substituição de operadores boleanos, a fim de que o usuário obtenha resultados aproximados para a consulta especificada.  Neste trabalho é apresentada uma proposta para suporte a argumentos de consulta vagos através da extensão da linguagem XPath. Para isso, são utilizadas expressões XPath que utilizam novas funções, as quais são, diretamente, adicionadas ao processador da linguagem de consulta. Além disso, é apresentada uma breve descrição das métricas de similaridade utilizadas para a criação das funções. As funções que foram adicionadas a um processador XPath possuem uma ligação muito estreita com as métricas utilizadas. Como as métricas, as funções trabalham com valores simples (elementos atômicos) e compostos (elementos complexos). As funções que trabalham com elementos atômicos podem ser classificadas tanto pelo tipo de dado que será analisado, como pelo tipo de análise que será feita. As funções para elementos complexos comparam conjuntos de elementos atômicos de acordo com a forma do agrupamento (conjunto, lista ou tupla).|http://hdl.handle.net/10183/4776
Implicações do estilo de descrição de códigos VHDL na testabilidade|2005|Open Access|Dissertação|Microeletrônica;Vhdl|por|Devido ao aumento da complexidade dos circuitos integrados atuais, os projetos são desenvolvidos utilizando linguagens de descrição de hardware (por exemplo, VHDL) e os circuitos são gerados automaticamente a partir das descrições em alto nível de abstração. Embora o projeto do circuito seja facilitado pela utilização de ferramentas de auxílio ao projeto, o teste do circuito resultante torna-se mais complicado com o aumento da complexidade dos circuitos. Isto traz a necessidade de considerar o teste do circuito durante sua descrição e não somente após a síntese. O objetivo deste trabalho é definir uma relação entre o estilo da descrição VHDL e a testabilidade do circuito resultante, identificando formas de descrição que geram circuitos mais testáveis. Como estudo de caso, diferentes descrições VHDL de um mesmo algoritmo foram utilizadas. Os resultados mostram que a utilização de diferentes descrições VHDL tem grande impacto nas medidas de testabilidade do circuito final e que características de algumas descrições podem ser utilizadas para modificar outras descrições e com isso aumentar a testabilidade do circuito resultante.|http://hdl.handle.net/10183/4777
Diagnóstico on-line do estilo cognitivo de aprendizagem do aluno em um ambiente adaptativo de ensino e aprendizagem na web: uma abordagem empírica baseada na sua trajetória de aprendizagem|2003|Open Access|Tese|Informática : Educação;Ensino a distância;Aquisicao : Conhecimento;Ensino-aprendizagem;Descoberta : Conhecimento|por|Uma das questões críticas relacionadas com um Ambiente Adaptativo de Ensino e Aprendizagem baseado na Web diz respeito à eficácia da aprendizagem do aluno remoto. Assim como diversos trabalhos de pesquisa na literatura, nosso estudo preocupou-se com a modelagem do Estilo Cognitivo de Aprendizagem (ECA) do aluno visando, em um futuro próximo, a adaptação dos conteúdos pedagógicos a esta importante característica individual do aluno. Esta tese descreve a metodologia utilizada para investigar como modelar o ECA do aluno remoto, baseado na observação e análise de seu comportamento em um ambiente de ensino e aprendizagem na Web. Em nosso estudo, o ECA representa o estágio de desenvolvimento cognitivo do aluno, de acordo com a taxonomia de Bloom. Nós acreditamos que os principais benefícios de adaptar a instrução ao ECA do aluno estão relacionados com a possibilidade de oportunizar a ampliação de suas habilidades cognitivas, assim como oportunizar a aprendizagem em profundidade sobre os conteúdos em estudo.  A metodologia quase-experimental usada para a modelagem do ECA do aluno compreendeu duas fases distintas: (i) geração das classes de ECA a partir da aplicação de um teste psicológico em uma amostra da população-alvo; e (ii) desenvolvimento do módulo de ensino experimental e o estudo das Trajetórias de Aprendizagem (TA) padrão das classes de ECA, a partir da observação de seus comportamentos durante a execução de uma sessão de estudo na Web. Como resultado deste estudo, identificamos os principais indicadores, que melhor discriminaram as classes de ECA consideradas. Os resultados foram obtidos a partir da observação e análise das TAs na Web. Todo o conhecimento obtido a partir desta investigação deverá nos permitir automatizar o diagnóstico do ECA do aluno remoto. Este conhecimento também será utilizado como base para o desenvolvimento dos conteúdos a serem oferecidos ao aluno pelo Ambiente Adaptativo de Ensino e Aprendizagem baseado na Web.|http://hdl.handle.net/10183/4837
Avaliação da qualidade de imagens médicas geradas por Ray Casting|2003|Open Access|Dissertação|Informática médica;Visualizacao volumetrica;Computação gráfica|por|Técnicas de visualização volumétrica direta propiciam a geração de imagens de alta qualidade já que se baseiam na amostragem do volume de dados original. Tal característica é particularmente importante na área da Medicina, onde imagens digitais de dados volumétricos devem ganhar maior importância como meio de apoio à tomada de decisão por parte dos médicos. No entanto, a geração de imagens com melhor qualidade possível acarreta um alto custo computacional, principalmente em relação ao algoritmo de ray casting, onde a qualidade de imagens depende de um maior número de amostras ao longo do raio fato este refletido no tempo de geração. Assim, a utilização de tais imagens em ambientes interativos é muitas vezes inviabilizada e, para a redução do custo computacional, é necessário abdicar parcialmente da qualidade da imagem. O conceito de qualidade é altamente subjetivo, e sua quantificação está fortemente relacionada à tarefa para qual a imagem está destinada. Na área da Medicina, imagem de boa qualidade é aquela que possibilita ao médico a análise dos dados através da sua representação visual, conduzindo-o a um diagnóstico ou prognóstico corretos. Nota-se que é necessário, então, avaliar a qualidade da imagem em relação a uma determinada tarefa a partir de critérios e métricas subjetivas ou objetivas. A maior parte das métricas objetivas existentes medem a qualidade de imagens com base no cálculo da diferença de intensidade dos pixels, fator que pode não ser suficiente para avaliar a qualidade de imagens do ponto de vista de observadores humanos.  Métricas subjetivas fornecem informação mais qualificada a respeito da qualidade de imagens, porém são bastante custosas de serem obtidas. De modo a considerar tais aspectos, o presente trabalho propõe uma métrica objetiva que procura aproximar a percepção humana ao avaliar imagens digitais quanto à qualidade apresentada. Para tanto, emprega o operador gradiente de Sobel (enfatização de artefatos) e o reconhecimento de padrões para determinar perda de qualidade das imagens tal como apontado por observadores humanos. Os resultados obtidos, a partir da nova métrica, são comparados e discutidos em relação aos resultados providos por métricas objetivas existentes. De um modo geral, a métrica apresentada neste estudo procura fornecer uma informação mais qualificada do que métricas existentes para a medida de qualidade de imagens, em especial no contexto de visualização volumétrica direta. Este estudo deve ser considerado um passo inicial para a investigação de uma métrica objetiva mais robusta, modelada a partir de estudos subjetivos.|http://hdl.handle.net/10183/4848
Comportamento infravermelho do propagador do glúon na rede|2003|Open Access|Tese|Fisica de particulas elementares e campos;Gluons|por|Investigamos o comportamento infravermelho do propagdor do glúon,no calibre de Ladau, em três dimensões (2+1) para teoria Yang-Mills SU (2) (YM23 )usando simulações em redes euclidianas de grande volume(403, 803, 1403). Obtemos indicações bastante fortes de que esse propagador tende a valor infinito para momentum nulo,decrescendo, a partir de ~350 MeV, com , expoente crítico κ ~ 0.6. Comparações com predições analíticas não-perturbativas mostram boa concordância e sugerem existência de pólos imaginários no propagador.Obtemos clara evidência de violação em YM23, condição suficiente para o confinamento de cor.|http://hdl.handle.net/10183/4919
Síntese, caracterização e estudo fotofísico de heterociclos fluorescentes por ESIPT e suas aplicações na preparação de novos materiais|2005|Open Access|Tese|Benzazolas : Síntese;Transferencia protonica intramolecular no estado excitado;Corantes orgânicos fluorescentes|por|Neste trabalho é apresentada a síntese e a caracterização de compostos do tipo 2-(2'-hidroxifenil)benzazólicos, fluorescentes e com um grande deslocamento de Stokes, devido a um mecanismo de transferência protônica intramolecular no estado excitado (ESIPT). Estes derivados possuem a função amino, de grande versatilidade sintética. A possibilidade de aplicação dos compostos sintetizados foi testada em cinco diferentes sistemas. Os derivados foram utilizados como monômetros vinilênicos e acriloilamida para a polimerização com metacrilato de metila para a produção de novos materiais poliméricos orgânicos fluorescentes; como sondas fluorescentes para proteínas, marcando com sucesso albumina sérica bovina, apresentando conjugados com maior fotoestabilidade em relação a sondas comerciais; como uma molécula com propriedades altamente não-lineares, possuindo o maior valor para a primeira hiperpolarizabilidade conhecido até hoje; como ligante para a complexação com metais de transição. com potencial aplicação em dispositivos orgânicos emissores de luz, e como derivados silifuncionalizados para a obtenção de materiais híbridos através de reação sol-gel, e a partir destes, obter os primeiros aerogéis de sílica fluorescentes por ESIPT.|http://hdl.handle.net/10183/4926
Um Estudo sobre modelos conceituais para ferramentas de definição de processos de workflow|2004|Open Access|Dissertação|Sistemas : Informação;Modelagem : Workflow|por|A crescente necessidade de padronização, eficácia e gerenciamento de processos têm aumentado o interesse das mais diferentes organizações pela tecnologia de workflow. Entretanto, a rápida propagação da tecnologia de workflow, ocorrida na última década, fez com que muitos sistemas desenvolvidos nesta área fossem disponibilizados e difundidos sem uma padronização clara. Analisando-se especificamente a fase de construção do workflow, nota-se que há muitas divergências. Conceitualmente, ainda não há modelos nem metodologias de modelagem amplamente aceitos e consolidados. Quanto às implementações, ainda não existe uma especificação clara e detalhada das ferramentas de definição. A conseqüência deste panorama é a baixa interoperabilidade e a baixa padronização dos conceitos, funcionalidades e interações das ferramentas de definição. Contudo, muitos esforços estão sendo feitos para solucionar o problema da interoperabilidade e padronização. A área de workflow como um todo já começa a apontar fortes tendências de padronização dos conceitos, funcionalidades e interações de seus sistemas. Detectar e avaliar tais tendências são os focos de estudos desta dissertação.  Mais precisamente, o objetivo desta dissertação é fornecer informações e métodos que possam ajudar desenvolvedores e usuários de ferramentas de definição a: compreender, descrever e avaliar os conceitos, funcionalidades e interações deste tipo de ferramenta. Para tanto, é mostrado um método de descrição de modelos conceituais de ferramentas de definição. O referido método é resultado de uma pesquisa sobre o modelo conceitual deste tipo de ferramenta. Com base nas informações pesquisadas, foi desenvolvido o modelo conceitual de uma ferramenta de definição chamada de Amaya Workflow (AW). Para mostrar a viabilidade de implementação do modelo conceitual AW e concretizar em um software os estudos desenvolvidos durante esta dissertação, foi implementado o protótipo Amaya Workflow. Por fim, é mostrado um panorama geral das funcionalidades, conceitos e interações das principais ferramentas de definição existentes. Com base neste panorama e nos estudos anteriores, é descrito um método de avaliação de modelos conceituais de ferramentas de definição.|http://hdl.handle.net/10183/4928
Critérios para avaliação de coordenação multiagente|2004|Open Access|Tese|Inteligência artificial;Sistemas multiagentes;Futebol : robôs;Coordenação : Agentes|por|A utilização da abordagem de agentes, nas mais diversas áreas de aplicações, mostra o interesse nas pesquisas sobre sistemas multiagentes. Este interesse surgiu da necessidade de aplicar novas técnicas e conceitos para a construção de sistemas e para auxiliar no seu desenvolvimento. Neste sentido, os agentes satisfazem às expectativas, não sendo apenas utilizados para a solução de problemas acadêmicos, mas também de sistemas reais. Na ciência da computação, a inteligência artificial distribuída está profundamente relacionada com o problema de coordenação. O objetivo é projetar mecanismos de coordenação para grupos de agentes artificiais. Várias características envolvem a atuação de agentes em um ambiente multiagente, como os mecanismos de cooperação, coordenação, comunicação, organização, entre outros. Este trabalho apresenta um estudo sobre coordenação multiagente, enfatizando a sua avaliação. O objetivo é apresentar uma proposta de avaliação, com um conjunto de critérios definidos para serem aplicados em modelos de coordenação. Inicialmente, é apresentado um estudo sobre coordenação de agentes. A seguir, são abordados vários modelos de coordenação encontrados na literatura da área.  A parte principal do trabalho corresponde à definição de critérios para avaliação da coordenação, a serem utilizados em duas etapas: uma análise do problema, com vistas à escolha de um modelo de coordenação a ser empregado em uma determinada aplicação, e uma avaliação a posteriori, baseada nos critérios propostos para avaliar o comportamento de um sistema coordenado após o uso de um modelo de coordenação específico.Para exemplificar a aplicação dos critérios, dois estudos de caso são apresentados e foram utilizados para os experimentos: um referente ao domínio da Robocup, utilizando o Time UFRGS e, outro, referente ao gerenciamento de agendas distribuídas.|http://hdl.handle.net/10183/4949
Serviços para auxiliar decisão mediante incerteza|2005|Open Access|Tese|Inteligência artificial;Representacao : Conhecimento;Agentes : Software;Incerteza : Sistemas : Informação|por|O objetivo deste trabalho é apresentar um modelo eficiente de representação de conhecimento que é construído a partir de relações de causa e efeito entre percepções e ações. Assume-se que é possível perceber o ambiente, é necessário fazer decisões mediante incerteza, é possível perceber a realimentação (feedback) referente ao sucesso ou fracasso das ações escolhidas, e é possível aprender com a experiência. Nós descrevemos uma arquitetura que integra o processo de percepção do ambiente, detecção de contexto, tomada de decisão e aprendizagem, visando obter a sinergia necessária para lidar com as dificuldades relacionadas. Além da descrição da arquitetura, é apresentada de forma sucinta uma metodologia chamada Computação Contextual, composta por duas fases principais: Definição e Operação. A fase de Definição envolve o projeto e modelagem de: i) Os subespaços de conhecimento conceitual e canônico; e ii) As regras de crescimento dinâmico. A fase de Operação complementa (isto é, estende e adapta) as definições iniciais através da aprendizagem feita pela interação com o ambiente.|http://hdl.handle.net/10183/4951
From XML to relational view updates: applying old solutions to solve a new problem;De atualizações sobre visões XML para atualizações sobre visões relacionais: aplicando soluções antigas a um novo problema |2004|Open Access|Tese|Updates through views;Relational databases;Banco : Dados;Banco : Dados relacionais;XML (Linguagem de marcação);Visoes : Banco : Dados|eng|XML has become an important medium for data exchange, and is frequently used as an interface to - i.e. a view of - a relational database. Although lots of work have been done on querying relational databases through XML views, the problem of updating relational databases through XML views has not received much attention. In this work, we give the rst steps towards solving this problem. Using query trees to capture the notions of selection, projection, nesting, grouping, and heterogeneous sets found throughout most XML query languages, we show how XML views expressed using query trees can be mapped to a set of corresponding relational views. Thus, we transform the problem of updating relational databases through XML views into a classical problem of updating relational databases through relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database. Since query trees are a formal characterization of view de nition queries, they are not well suited for end-users. We then investigate how a subset of XQuery can be used as a top level language, and show how query trees can be used as an intermediate representation of view de nitions expressed in this subset.;XML vem se tornando um importante meio para intercâmbio de dados, e é frequentemente usada com uma interface para - isto é, uma visão de - um banco de dados relacional. Apesar de existirem muitos trabalhos que tratam de consultas a bancos de dados através de visões XML, o problema de atualização de bancos de dados relacionais através de visões XML não tem recebido muita atenção. Neste trabalho, apresentam-se os primeiros passos para a solução deste problema. Usando query trees para capturar noções de seleção, projeção, aninhamento, agrupamento e conjuntos heterogêneos, presentes na maioria das linguagens de consulta XML, demonstra-se como visões XML expressas através de query trees podem ser mapeadas para um conjunto de visões relacionais correspondentes. Consequentemente, esta tese transforma o problema de atualização de bancos de dados relacionais através de visões XML em um problema clássico de atualização de bancos de dados através de visões relacionais. A partir daí, este trabalho mostra como atualizações na visão XML são mapeadas para atualizações sobre as visões relacionais correspondentes. Trabalhos existentes em atualização de visões relacionais podem então ser aplicados para determinar se as visões são atualizáveis com relação àquelas atualizações relacionais, e em caso a rmativo, traduzir as atualizações para o banco de dados relacional. Como query trees são uma caracterização formal de consultas de de nição de visões, elas não são adequadas para usuários nais. Diante disso, esta tese investiga como um subconjunto de XQuery pode ser usado como uma linguagem de de nição das visões, e como as query trees podem ser usadas como uma representação intermedi ária para consultas de nidas nesse subconjunto.|http://hdl.handle.net/10183/4952
Um Sistema para monitoração de redes IP baseado em políticas|2003|Open Access|Dissertação|Redes : Computadores;Gerencia : Redes : Computadores;Qualidade : Servico|por|O fornecimento de facilidades de QoS em redes de computadores tem por objetivo introduzir níveis das garantias que são ausentes, por exemplo, no paradigma de melhor-esforço das redes IP. Diferentes arquiteturas de QoS possuem padrões diferentes para os serviços fornecidos, que conduzem a um cenário em que a QoS prevista não possa ser sempre fornecida corretamente pela arquitetura utilizada. Neste contexto, uma monitoração da QoS é necessária para verificar a QoS observada e para compará-la com a QoS esperada/ contratada. Em uma rede que utilize gerenciamento baseado em políticas, a QoS esperada é definido pelas políticas que regem o comportamento da rede. O Internet Engineering Task Force (IETF) tem padronizado vários elementos para um sistema de gerenciamento de redes baseado em políticas (PBNM), no qual políticas são definidas utilizando-se linguagem de alto nível, armazenadas em repositórios para políticas, aplicadas com o auxilio de Policy Decision Points (PDPs) e mantidas por Enforcement Points (PEPs). Pela definição do IETF, uma vez que uma política é aplicada com sucesso, o sistema de PBNM não mais checará o comportamento desta, e a QoS definida é assumida com a fornecida pela rede. De fato, isso nem sempre é verdade. A arquitetura da qual provém a QoS pode apresentar-se instável ou mediante problemas, logo a QoS esperada não seria atingida. Para verificar a degradação na QoS em ambientes de gerenciamento reais, atualmente, o administrador da rede monitora a rede e determina a QoS fornecida. Tal QoS é, por sua vez, manualmente comparada com a QoS definida pelas políticas de rede. Finalmente, se diferenças são encontradas, o administrador procede com medidas que levem a arquitetura a um estado consistente Nos dias de hoje, as definições e aplicações de políticas e monitoração de QoS são tarefas executadas separadamente pelas soluções disponíveis. Além disso, como demonstrado anteriormente, a verificação da QoS fornecida contra a QoS definida pelas políticas da rede é deixada a cargo do administrador da rede. Nesse contexto, a automação da monitoração das políticas de QoS e a integração entre as tarefas citadas são necessárias do ponto de vista do administrador da rede. Nesta dissertação, é proposta uma definição (e um sistema) para a monitoração de QoS em que as definições de políticas são dados de entrada utilizados para checar a QoS observada. No momento em que uma degradação na QoS é detectada, o sistema de monitoração notifica um gerente com suporte a SNMP utilizando mensagens do tipo InformRequest. A arquitetura do sistema é dividida internamente em monitores de QoS e controladores de monitores de QoS. Cada controlador de monitor de QoS controla vários monitores de QoS, os quais coletam dados da rede. Tais dados são comparados com as políticas traduzidas pelo controlador, e, caso sejam detectadas degradações, o controlador de monitor de QoS notifica o gerente. A comunicação entre controlador de monitores de QoS e monitores de QoS também é baseada em SNMP. O principal objetivo do trabalho é fornecer uma solução que integre monitoração de QoS e PBNM em um único ambiente de gerenciamento.|http://hdl.handle.net/10183/4954
Variações das propriedades granulométricas dos sedimentos da Barreira Costeira da Pinheira (SC) durante a sua progradação no holoceno superior|2004|Open Access|Dissertação|Geologia marinha;Sedimentologia costeira;Pinheira, Praia da (SC)|por|A barreira costeira da Pinheira esta localizada no litoral Centro-Sul do estado de Santa Catarina. Sua rnorfologia e estratigrafia são típicas de uma barreira regressiva. Durante o Holoceno Superior (Últimos 5 ka), a barreira progradou cerca de 5.500 metros, a uma taxa media de 1 ,I mlano. Sua progradação foi determinada por um rebaixamento de aproximadamente 2 m do nível do mar e, principalmente, por uma expressivo aporte de sedimentos arenosos em seu sistema praial, provenientes da plataforma continental adjacente. Nos Últimos 3.500 metros de progradaHo da barreira, ocorridos nos Últimos 3.1 ka, cerca de 60 cordões de dunas frontais foram formados. O intervalo de tempo decorrido entre a formação de dois cordões sucessivos foi de aproximadamente 52 anos.  O estudo das propriedades granulométricas do sistema praia-duna atual e de dois dos cordões antigos de dunas frontais, denominados cordões intermediArio e interno, e formados, respectivamente, há cerca de 1280 e 3140 anos AP, revelou que, nos Últimos 3 ka não ocorreram variações significativas destas propriedades. Esta não variação, no tempo, das propriedades granulométricas é atribuída a natureza policíclica do estoque de areia consumido na progradação da barreira e a sua manutenção como fonte de sedimentos durante a progradação, elou a uma relativa constância, nos últimos 3 ka, das condições dinâmicas gerais dos sistema praia-duna atual da enseada da Pinheira. O estudo comparativo entre os diferentes sub-ambientes, ou zonas do sistema praia-duna atual (face da praia, berma e duna frontal), mostrou que o desvio padrão e a assimetria são potencialmente importantes na distinção entre depdsitos eolicos (dunas frontais) e praiais (face da praia) da barreira.;The coastal barrier of Pinheira is located in the South-Central coast of Santa Catarina State. Its morphology and stratigraphy are typical of a regressive barrier. During the Late Holocene (last 5 ka) the barrier has prograded approximately 5,500 meters, under and average rate of 1.1 mear. Its progradation was determined by a sea-level fall of 2 rneters and mainly due to an expressive transference of sands from the adjacent continental shelf into the beach system of the barrier. In the last 3,500 rneters of progradation, occurred in the last 3,1 ka, approximately 60 foredune ridges were forrned, each ridge corresponding, in average, to a time interval of 52 years. The study of the granulometric properties of the modern beach-dune system, and of two old foredune ridges called intermediate and inner ridges, respectively formed at 1280 and 3140 years BP, has demonstrated that in the last 3,1 ka this properties has not changed significantly.  These constant granulometric properties are being explained by the policyclic nature of the sands used for barrier progradation and its maintenance as source of sediments during progradation, andlor by none or even very short variation of the general dynamic conditions of the modern beach-dune system of the Pinheira em bayment. The comparative study between the sub-environments or zones of the modern beach-dune system (beach face, berm and foredune) has showed that standard deviation and skewness are potentially important on t he distinction between aeolian (foredunes) and beach (beach face) deposits.|http://hdl.handle.net/10183/4970
Produção de quarkonium : aspectos perturbativos e não-perturbativos da QCD|2003|Open Access|Tese|Cromodinâmica quântica;Teoria de perturbacao;Interacoes fortes de particulas elementares;Quarks pesados|por|Neste trabalho de tese investigamos o papel de dinâmica perturbativa e não-perturbativa da Cromodinâmica Quântica, a teoria das interações fortes, em processos de produção de quarks pesados e de quarkonium-estados ligados de um par de quarks pesados. Um aspecto importante na produção de quarks pesados consiste no tratamento de ordens mais altas em QCD perturbativa, que abordamos por meio de elementos de matriz QCD em segunda ordem dominante (NLO) e através de um gerador de eventos Monte Carlo, mais útil fenomenologicamente, onde a produção perturbativa de pares Q Q e obtida utilizando elementos de matriz em ordem dominante e a aproximação de chuveiros partônicos de processos em ordens mais altas. Os processos suaves formando estados ligados de quarkonium são descritos em termos do modelo de evaporação de cor (CEM), ou alternativamente através do modelo de interações suaves de cor (SCI) e do modelo da lei das áreas generalizado (GAL). Neste trabalho, calculamos as distribuições em xF e p? para o J= e 0 em hadroprodução em alvo xo e no colisionador p p do Tevatron. Outros observáveis como a seção de choque total para J= , 0 e charme aberto tamb em são reproduzidos. Além disso, extrapolamos os modelos para descrever a produção de J= e no futuro colisionador LHC, onde as taxas de produção de J= estão at e uma ordem de magnitude acima de outra predição da literatura, o qual pode implicar em J= ser um ru do não negligenciável para estudos de violação da simetria CP no LHC.  Além disso, com o objetivo de descrever as taxas de produção relativas entre os vários estados de charmonium, desenvolvemos um modelo para o mapeamento do espectro contínuo de massas do par c c produzido perturbativamente, nas ressonâncias de charmonium, onde introduzimos uma correlação entre a massa invariante do par produzido perturbativamente e a massa física do estado de charmonium. Outra abordagem importante ao estudo dos aspectos perturbativos e não-perturbativos da QCD na produção de quarks pesados e o formalismo de fatorização k?, o qual investigamos em processos de fotoprodução de charme e bottom, com ênfase em resultados de um modelo de saturação. Efeitos de evolução DGLAP tamb em são estudados, considerando a derivada da distribuição de glíuons. Analisamos em detalhe seções de choque totais e distribuições em pT , mostrando as regiões de validade de cada descrição. Através do estudo de vários aspectos perturbativos e não-perturbativos da QCD, este trabalho de tese contribui para um melhor entendimento da conexão entre essas duas dinâmicas.|http://hdl.handle.net/10183/4995
Modelos para o mapeamento de aplicações em infra-estruturas de comunicação intrachip|2005|Open Access|Tese|Microeletrônica;Algoritmos computacionais|por|O projeto de sistemas intrachip (SoCs) é uma atividade de alto grau de complexidade, dados a dimensão de SoCs, na ordem do bilhão de transistores, os requisitos de tempo de desenvolvimento e de consumo de energia, entre outros fatores. A forma de dominar a complexidade de projeto de SoCs inclui dividir a funcionalidade do sistema em módulos de menor complexidade, denominados de núcleos de propriedade intelectual (núcleos IP), interligados por uma infra-estrutura de comunicação. Enquanto núcleos IP podem ser reusados de outros projetos ou adquiridos de terceiros, a infra-estrutura de comunicação deve sempre ser desenvolvida de forma personalizada para cada SoC. O presente trabalho volta-se para o projeto de infraestruturas de comunicação eficientes. Questões importantes neste contexto são a eficiência da comunicação, refletida e.g. em medidas de vazão e latência, a redução de área de silício para implementar a comunicação, e a redução da energia consumida na comunicação. Estas questões dependem da escolha da infra-estrutura de comunicação. Barramentos são as infra-estruturas mais usadas nas comunicações intrachip, mas têm sido consideradas como pouco adequadas para servir a necessidade de comunicação de SoCs futuros. Redes intrachip vêm emergindo como um possível melhor candidato. Nesta infra-estrutura de comunicação, um problema a ser resolvido é o posicionamento relativo de núcleos IP dentro da rede, visando otimizar desempenho e reduzir o consumo de energia, no que se denomina aqui problema de mapeamento. Dada a complexidade deste problema, considera-se fundamental dispor de modelos para capturar as características da infra-estrutura de comunicação, bem como da aplicação que a emprega  A principal contribuição deste trabalho é propor e avaliar um conjunto de modelos de computação voltados para a solução do problema de mapeamento de núcleos de propriedade intelectual sobre uma infra-estrutura de comunicação. Três modelos são propostos (CDM, CDCM e ECWM) e comparados, entre si e com três outros disponíveis na literatura (CWM, CTM e ACPM). Embora os modelos sejam genéricos, os estudos de caso restringem-se aqui a infra-estruturas de comunicação do tipo rede intrachip. Dada a diversidade de modelos de mapeamento, propõe-se uma segunda contribuição, o metamodelo Quantidade, Ordem, Dependência (QOD), que relaciona modelos de mapeamento usando os critérios expressos na denominação QOD. Considerando o alto grau de abstração dos modelos empregados, julga-se necessário prover uma conexão com níveis inferiores da hierarquia de projeto. Neste sentido, uma terceira contribuição original do presente trabalho é a proposta de modelos de consumo de energia e tempo de comunicação para redes intrachip. Visando demonstrar a validade de todos os modelos propostos, foram desenvolvidos métodos de uso destes na solução do problema de mapeamento, o que constitui uma quarta contribuição. Estes métodos incluem algoritmos de mapeamento, estimativas de tempo de execução, consumo de energia e caminhos críticos em infra-estruturas de comunicação. Como quinta contribuição, propõe-se o framework CAFES, que integra os métodos desenvolvidos e os modelos de mapeamento em algoritmos computacionais. Uma última contribuição do presente trabalho é um método habilitando a estimativa de consumo de energia para infra-estruturas de comunicação e sua implementação como uma ferramenta computacional.|http://hdl.handle.net/10183/5016
Implementação de recuperação por retorno de aplicações distribuídas baseada em checkpoints coordenados|2005|Open Access|Dissertação|Sistemas distribuídos;Algoritmos computacionais;Recuperacao : Processos;Tolerancia : Falhas|por|A recuperação por retorno baseada em checkpointing é largamente usada como técnica de tolerância a falhas. O modelo complexo de sistemas distribuídos tem motivado o desenvolvimento de diversos algoritmos na tentativa de encontrar soluções mais simples e eficientes. Os processos que formam o sistema distribuído podem coordenar suas operações para garantir que o conjunto de checkpoints locais componha um estado global consistente (linha de recuperação). A partir desse estado, no caso de ocorrência de falhas, o sistema pode ser recuperado e a computação retomada a partir de um momento anterior ao da manifestação da falha, evitando o retrocesso para o estado inicial da computação e prevenindo a ocorrência de prejuízos com a perda de todo processamento até então realizado. No Grupo de Tolerância a Falhas da UFRGS foi proposto recentemente um algoritmo que é voltado para aplicações que executam em sistemas distribuídos assíncronos que se comunicam exclusivamente pela troca de mensagens. Ele opera com salvamento coordenado de checkpoints (não bloqueando as aplicações) e prevê o tratamento de mensagens órfãs e perdidas. Os mecanismos do algoritmo sugerem que nenhuma alteração deveria ser realizada no código das aplicações, criando a possibilidade de implementação transparente sob o ponto de vista dos usuários e dos programadores das aplicações. Como o algoritmo não requer o bloqueio das aplicações, a sobrecarga imposta pelos mecanismos à execução livre de falhas é pequena. Além disso, o processo de recuperação tende a ser efetuado rapidamente, uma vez que é garantida a existência de uma linha de recuperação consistente, facilmente identificada  Este trabalho apresenta as decisões de projeto, a implementação, os resultados e a avaliação de desempenho desse algoritmo. A avaliação das alternativas de implementação resultou na decisão de uma implementação então realizada diretamente sobre o sistema operacional Linux, sem recorrer a protocolos auxiliares para garantir a execução dos serviços e sem a necessidade de adaptações no código das aplicações nem no código do sistema operacional. Adicionalmente, os resultados comprovaram a expectativa inicial de que o algoritmo causaria pouca sobrecarga no sistema (menos de 2%), embora ele ainda apresente alta dependência do tamanho dos checkpoints salvos.|http://hdl.handle.net/10183/5018
Supressão de J/[psi] em processos próton-núcleo e núcleo-núcleo devido aos efeitos de alta densidade|2003|Open Access|Dissertação|Mesons;Colisao de ions pesados;Plasma de quarks e gluons;Íons;Altas energias;Hardrons;Densidade;Partons;Gluons;Colisao proton-nucleon;Leptons;Interações nucleon-nucleon|por|A supressão da produção do méson J/Ψ em colisões de íons pesados tem sido apontada como um sinal da formação de um estado desconfinado da matéria - o Plasma de Quarks e Glúons. Este sinal, por em, não e inequívoco e muitos modelos, que não assumem a formação do plasma, podem descrever igualmente bem os resultados da colaboração NA50, no CERN, que apontou uma supressão anômala, não explicada pela absorção nuclear, nas colisões mais centrais entre íons de chumbo. De modo geral, estes modelos, considerando ou não a formação do plasma, procuram explicar os resultados experimentais através de mecanismos que causam a supressão no estado final da colisão, isto e, mecanismos que agem sobre as partículas produzidas na colisão. Por outro lado, para núcleos pesados e em processos envolvendo altas energias, as distribuições partônicas nucleares são alteradas em relação as distribuições para nucleons livres. Estas alterações ocorrem devido ao fato das dimensões do nucleon serem um limite geométrico para o crescimento das distribuições - seu vínculo de unitariedade - pois o meio nuclear, em altas energias, apresenta uma alta densidade partônica. A existência deste vínculo de unitariedade requer modificações das distribuições partônicas, o que deve ser considerado nos cálculos das seções de choque nucleares. Tais modificações afetam a produção de hádrons no estado final, diminuindo sua taxa de produção.  Nesse trabalho, investigamos a inclusão dos efeitos de alta densidade nas distribuições partônicas para o tratamento da supressão de J/Ψ em colisões envolvendo alvos nucleares. Estes efeitos são decorrentes do aumento da distribuição de glúions na região de pequeno x (altas energias). A evolução DGLAP, que considera apenas a emissão de pártons, prevê um crescimento ilimitado da distribuição de glúons nesta região, quebrando assim o vínculo da unitariedade. Por isso, o mecanismo de recombinação partônica passa a contribuir para restaurar a unitariedade. Estes efeitos de recombinação, basicamente, são tratados como os efeitos de alta densidade referidos nesse trabalho, alterando as distribuições partônicas nucleares. Utilizamos processos próton-núcleo para estimar a magnitude destes efeitos, uma vez que estes processos não apresentam um meio nuclear tão denso quanto o proporcionado por colisões núcleo-núcleo. Esta premissa torna os processos próton-núcleo testes mais confiaveis para a investigação dos efeitos de alta densidade. Analisamos em especial a razão entre as taxas de produção do méson J/Ψ e do par de léptons, via processo Drell- Yan, uma vez que este observável e utilizado para apontar a supressão na produção de J/Ψ . Estendemos esta análise para processos núcleo-núcleo, onde novos mecanismos de supressão, entre eles a formação do Plasma de Quarks e Glúons são esperados. Os resultados aqui apresentados mostram que a inclusão dos efeitos de alta densidade introduz uma supressão adicional na produção de J/Ψ , que se torna mais significativa com o aumento da energia do processo. Nossa conclusão e, portanto, que estes efeitos devem ser incorporados na análise deste sinal em experimentos realizados em RHIC e LHC.|http://hdl.handle.net/10183/5029
Comportamento eletroquímico do ferro em solução aquosa de acetato e benzoato de sódio|2000|Open Access|Dissertação|Dissolução anódica : Ferro : Eletroquímica;Ferro : Comportamento eletroquímico : Passivação : Corrosão;Corrosão : Ferro : Técnica de microeletrodos|por|Estudou-se o comportamento eletroquímico do ferro em solução aquosa, de acetato e benzoato de sódio em diferentes concentrações, pH 6, mantida constante a força iônica. Foram utilizados eletrodos rotatórios de Fe de tamanho convencional e microeletrodo, em condições estáticas e sob rotação. Medidas da variação do potencial de corrosão com o tempo de imersão e ensaios voltamétricos foram realizados. A forte interação do ânion benzoato com o ferro foi detectada nos ensaios com microeletrodos, onde o potencial de corrosão foi deslocado para valores correspondentes a região passiva do metal em curtos intervalos de tempo. Constatou-se o papel preponderante da concentração de benzoato no processo dissolução e passivação do ferro. A adição de 0,1M NaCl as soluções provoca o aparecimento de pites. O aumento da concentração de benzoato desloca o potencial de rompimento de filme e o potencial de repassivação para valores mais positivos. Vantagens decorrentes do uso de microeletrodos em estudos de corrosão foram evidenciadas.|http://hdl.handle.net/10183/5038
Síntese de materiais fotossensíveis baseados em corantes fluorescentes como meio ativo para dispositivos ópticos|2003|Open Access|Tese|Corantes orgânicos fluorescentes;Copolímeros;Benzazolas : Síntese;Transferencia protonica intramolecular no estado excitado|por|Neste trabalho realizou-se a síntese e caracterização de novos derivados 2- (2’-hidroxifenil)benzazólicos. Estas moléculas apresentam um bom rendimento quântico de fluorescência, um grande deslocamento de Stokes (> 150 nm) e elevadas estabilidades térmica e fotofísica devido a um mecanismo de transferência protônica intramolecular que ocorre no estado excitado (TPIEE). Estes derivados contêm grupos funcionais suscetíveis a sofrer reações de polimerização com monômeros acrílicos e grupos trietoxissilanos capazes de ligarem-se covalentemente a polímeros inorgânicos como sílicas e zeolitas, com interesse na geração de materiais fotossensíveis. Estes fluoróforos apresentam bandas de absorção na região entre 300 - 360 nm do espectro, justamente onde se localizam os comprimentos de onda de emissão de alguns lasers de bombeamento, como por exemplo o laser de N2 (337 nm) e o terceiro harmônico do laser de Nd:Yag (355 nm). Além disso, sob excitação com luz ultravioleta, estes compostos emitem fluorescência entre 500 - 580 nm, região espectral de alto interesse tecnológico para aplicações em telecomunicações e medicina.  A copolimerização dos novos corantes com metilmetacrilatos e a heterogeneização com sílica e zeolitas resultou em polímeros altamente fluorescentes com excelentes propriedades térmicas e ópticas. Além dos corantes benzazólicos foram utilizados outros corantes comercialmente disponíveis com emissão laser do azul até o vermelho. Foram eles: o Estilbeno 420, DPS (4,4’-difenilestilbeno), PBBO 2-(4-bifenilil)-6-fenilbenzoxazol- 1,3, as cumarinas C500, C503, C540A, C343, C440, C460, C480 e uma nova cumarina funcionalizada com um grupo acrilato C434+HEMA, os pirrometenos PM597 e PM650 e a rodamina Rh640. Com estes corantes foram sintetizados polímeros e copolímeros de metacrilato de metila e metacrilato de 2-hidroximetila e suas propriedades fotofísicas e laser foram avaliadas.|http://hdl.handle.net/10183/5056
Efeitos associados a El Niño e La Niña na vegetação do estado do Rio Grande do Sul|2002|Open Access|Dissertação|Agrometeorologia;Sensoriamento remoto|por|O objetivo deste trabalho foi avaliar os efeitos associados a El Niño e La Niña sobre o crescimento e desenvolvimento da cobertura vegetal e sua evolução temporal no Estado do Rio Grande do Sul, utilizando imagens do satélite NOAA. Foram utilizados dados mensais de precipitação pluvial e imagens de Índice de Vegetação por Diferença Normalizada (NDVI), no período de julho de 1981 a junho de 2000, sendo as análises feitas para todo o Estado e para as diversas Zonas de Cobertura e Uso do Solo. Os dados, classificados como El Niño, La Niña e neutro, foram utilizados para confeccionar imagens médias, imagens de anomalias e para traçar gráficos de evolução temporal de NDVI. Por fim, foi feita a análise da relação entre precipitação pluvial e NDVI. Os resultados mostraram que as diversas Zonas de Cobertura e Uso do Solo apresentam padrões diferenciados de variação na cobertura vegetal ao longo do ano, o qual é determinado pela disponibilidade hídrica, de radiação solar e de temperatura, sendo possível quantificar as alterações do padrão, através do monitoramento com imagens de NDVI/NOAA. Parte da variabilidade interanual do padrão de evolução do NDVI está associada à ocorrência do fenômeno El Niño e La Niña, como conseqüência, principalmente, do efeito deste fenômeno sobre a precipitação pluvial do Estado. Em anos de El Niño há um aumento na precipitação pluvial e conseqüentemente anomalias positivas de NDVI, enquanto que em anos de La Niña ocorre diminuição da precipitação pluvial a qual proporciona predominância de anomalias negativas de NDVI.  Existe um tempo de resposta da vegetação às condições hídricas, ocasionado por uma defasagem entre o aumento ou diminuição da precipitação pluvial e o conseqüente aumento ou decréscimo de NDVI. O padrão e a intensidade dos efeitos no NDVI associados ao fenômeno El Niño e La Niña, estão relacionados às condições edafoclimáticas e de uso e cobertura do solo. As relações entre NDVI e precipitação pluvial evidenciam que este é um dos principais elementos que influi nas condições de crescimento vegetal para o Estado do Rio Grande do Sul.|http://hdl.handle.net/10183/5087
Estimativa da área impermeável dentro da bacia hidrográfica do Arroio Dilúvio (Porto Alegre/RS) através de técnicas de sensoriamento remoto e geoprocessamento|2004|Open Access|Dissertação|Sensoriamento remoto;Geoprocessamento;Recursos naturais : Meio ambiente|por|Neste trabalho foram realizadas classificações utilizando-se as bandas 1 a 5 e 7 dos sensores Landsat 5 TM (1987) e Landsat 7 ETM+ (2000). A caracterização espectral dos materiais foi realizada em laboratório utilizando um espectrorradiômetro, e através das bandas 1 a 5 e 7 dos sensores Landsat 5 TM (1987) e Landsat 7 ETM+ (2000). A transformação dos dados multiespectrais de imagens de sensoriamento remoto é uma maneira de reduzir o volume de dados através da identificação de classes de interesse numa imagem digital. No intuito de verificar condições de melhoramento na classificação de alvos urbanos em imagens digitais, identificados por procedimentos já conhecidos, como a classificação pela Máxima Verossimilhança, escolheu-se um classificador baseado na lógica fuzzy. O classificador utilizado foi o Fuzzy Set Membership classification - Fuzclass, que faz parte de um conjunto de classificadores não-rígidos disponíveis no programa Idrisi 32. Uma vez que informações sobre o desempenho de produtos deste classificador em áreas urbanas são escassas, foram conduzidos ensaios de comparação de resultados obtidos por este classificador com a verdade terrestre, representada por uma imagem de alta resolução espacial do satélite QuickBird. As áreas teste selecionadas desta imagem atendem ao critério de inalterância das condições de ocupação para o intervalo temporal considerado  A comparação feita, permite concluir que o classificador apresenta limitações na classificação de áreas urbanas devido ao comportamento espectral semelhante dos materiais que fazem parte dessa cobertura. A utilização de uma classe única para identificar áreas impermeáveis foi a solução adotada para contornar este óbice. O emprego de áreas teste possibilitou acertar a escolha do grau de possibilidade de presença da classe no pixel (PPCP). Uma comparação entre os resultados apresentados na classificação de áreas impermeáveis, com base nos classificadores Máxima Verossimilhança e Fuzclass, demonstrou um desempenho melhor do classificador fuzzy, em função do nível de PPCP ajustado durante a análise comparativa Landsat e Quickbird nas áreas teste. Um procedimento alternativo de estimativa de áreas impermeáveis em bacias urbanas é apresentado no final.|http://hdl.handle.net/10183/5093
Zoneamento da Floresta Nacional de Brasília-DF utilizando técnicas de geoprocessamento e sensoriamento remoto|2004|Open Access|Dissertação|Sensoriamento remoto;Zoneamento : Vegetação;Recursos naturais : Meio ambiente|por|Utilizando técnicas de geoprocessamento, desenvolveu-se metodologia para avaliar e diagnosticar os impactos ambientais decorrentes do uso e ocupação do solo em áreas de Florestas Nacionais. A área teste da pesquisa foi a Floresta nacional de Brasília. No modelamento para a avaliação da perda e tolerância de solo em função de seu uso, utilizou-se das ferramentas de geoprocessamento e imagem de satélite para a solução da equação USLE. Foi possível determinar as áreas de preservação permanente e as áreas passíveis de reflorestamento, através da criação de cenários de colheita florestal, bem como as diferentes formas de manejo que devem ser adotadas em função das perdas de solo por erosão laminar. As áreas de perda de solo acima de sua tolerãncia representam 1,90% (177,44 ha) da área total da Flona. As áreas consideradas de Preservação Permanente sob o aspecto da legislação vigente e do uso atual representam 6,78% (633,46ha) de sua área total. Destes, 140,18ha (22,13%) apresentam uma cobertura vegetal do tipo campo e devem ser recuperados e convertidos em áreas com cobertura vegetal do tipo floresta, a fim de adequar-se a legislação vigente como também minimizar os processos erosivos que possam comprometer os cursos d'água. A área passível de implantação de floresta corresponde a 8.712,82 ha (93,22%), onde 5.642,48 ha (64,76%) podem ser manejados a corte raso e 3.070,34 hectares (32,24%) devem ser manejados a corte seletivo, para que não haja comprometimento do solo devido às perdas por erosão laminar.|http://hdl.handle.net/10183/5096
Pré-seleção de sítios astronômicos por imagens de satélites meteorológicos|2004|Open Access|Dissertação|Astronomia;Meteorologia|por|A Astronomia, como origem e, talvez, como fim de todas as Ciências, sempre esteve voltada à observação dos astros e à busca de novas técnicas e instrumentos que permitissem ampliar os limites e a qualidade destas observações. Dessa busca resultou o desenvolvimento do telescópio óptico, em 1608, por Galileu Galilei. Com o passar dos anos, esse instrumento foi sofrendo incontáveis aperfeiçoamentos, chegando aos nossos dias como um aparelho preciso e de sofisticada tecnologia. Apesar das fronteiras observacionais terem avançado para além do espectro visível, o telescópio óptico ainda é indispensável à Astronomia. O Brasil, embora não apresente condições meteorológicas ideais para observações astronômicas, possui observatórios ópticos de razoável qualidade, como o Laboratório Nacional de Astrofísica, LNA, em Minas Gerais, entre outros. Em seu extremo sul, no entanto, o País carece de bons instrumentos ópticos, destacando-se unicamente o telescópio da UFRGS instalado no Observatório do Morro Santana, em Porto Alegre. O aumento da iluminação artificial na Região Metropolitana de Porto Alegre e, conseqüentemente, da claridade do céu noturno tem praticamente inviabilizado a operação do Observatório. Assim, a escolha de novos locais para a futura instalação de telescópios ópticos no Estado é imprescindível. Acrescenta-se a isto o fato do ciclo climático desta região diferenciarse daquele das demais regiões do País, fato relevante, dado que um dos fatores determinantes na escolha de novos sítios para telescópios ópticos é a taxa de nebulosidade.  Levantamentos in situ de nebulosidade são longos e custosos. Como alternativa, neste trabalho foi realizado um estudo estatístico do Estado, a partir da montagem de um banco de 472 imagens noturnas dos satélites GOES e MeteoSat. A combinação das imagens, por processo de superposição e cálculo de valores médios de contagens (brilho), à escala de pixel, forneceu informações em nível de préseleção, ou indicativo, de locais com altas taxas de noites claras. Foram cobertos os períodos de 1994-1995 e 1998-1999, com focalização nas áreas em torno de Bom Jesus, Vacaria e Caçapava do Sul. Como controle, foi também monitorada a área em torno do LNA em Minas Gerais. Ademais da demonstração metodológica, os dados orbitais indicaram que, na média destes anos, estas áreas são adequadas à instalação de observatórios astronômicos, pela conjugação dos fatores de nebulosidade e altitude.|http://hdl.handle.net/10183/5097
Desenvolvimento e caracterização de nanovesículas lipossômicas compósitas de fosfatidilcolina da lecitina de soja e quitosana|2004|Open Access|Dissertação|Polímeros : Caracterização;Farmacologia;Vesículas lipossômicas;Nanocápsulas : Características físico-químicas;Quitosana|por|Nanovesículas lipossômicas constituem promissoras estruturas auto-organizadas hábeis para encapsular e transportar diferentes substâncias, como os fármacos, e foram produzidas neste trabalho pelo método da evaporação em fase reversa. Fosfatidilcolina de alta pureza foi utilizada na produção das estruturas. Paralelamente procedeu-se a purificação de fosfatidilcolina por cromatografia em coluna a partir da lecitina de soja fornecida pela indústria de óleo de soja local. O processo foi monitorado por cromatografia em camada delgada e a caracterização foi feita por RMN do 1H e do 31P. O resultado mostrou que fosfatidilcolina pode ser purificada facilmente pela cromatografia em coluna, mas a sua degradação deve ser evitada por processos eficientes de conservação. Quitosana foi obtida pela desacetilação por hidrólise alcalina da quitina comercial. O grau de desacetilação resultante foi caracterizado por infra-vermelho e a massa molar média, assim como outros parâmetros do polímero em solução como raio de giro e segundo coeficiente virial, foram determinados pela técnica de espalhamento de luz estático. O polímero de caráter catiônico quitosana foi empregado na produção de nanovesículas lipossômicas para revestimento das estruturas, visando o aumento da estabilidade física das mesmas.  A preparação de lipossomas pela evaporação em fase reversa constitui um processo em etapas, onde os diferentes estágios levam à formação de três sistemas vesiculares de distintas organizações estruturais: micelas reversas, organogel e lipossomas. As técnicas de viscosimetria, turbidimetria, ultra-violeta visível, espalhamento de luz estático e dinâmico e espalhamento de raios-X a baixo ângulo permitiram ampla caracterização destas estruturas com e sem quitosana. Dados como raio hidrodinâmico e ponto de percolação das micelas reversas, espessura lamelar do organogel, intensidade de espalhamento de luz, raio hidrodinâmico, raio de giro e espessura lamelar dos lipossomas foram obtidos. Além disso, o solvente orgânico éter etílico, clássico para este método de preparação foi substituído pelo acetato de etila sem prejuízo na formação das estruturas e com a vantagem do seu menor impacto ambiental. Lipossomas revestidos externa e internamente com quitosana formando um compósito foram obtidos com sucesso e apresentaram uma estabilidade física em solução aquosa maior do que lipossomas convencionais ou não revestidos.|http://hdl.handle.net/10183/5100
Aspectos sedimentológicos e petrológicos dos Beachrocks do estado do Rio Grande do Norte|2005|Open Access|Tese|Sedimentologia : Rio Grande do Norte;Petrologia : Rio Grande do Norte|por|A maior parte dos beachrocks distribuídos ao longo das costas oriental e setentrional do Estado do Rio Grande do Norte (53% da espessura total) foi depositada na zona de ante-praia superior, representada pelas litofácies arenitos com estratificação cruzada tabular-planar e acanalada de média escala e arenitos conglomeráticos bioturbados por Skolithos. Conglomerados e arenitos com estratificação cruzada de baixo ângulo, depositados na zona de estirâncio, representam 31% das seções descritas. Os 16% restantes são atribuídos ao colapso de material sobrejacente como resultado de solapamento basal de falésias (conglomerados maciços), de transporte como tapetes de tração (conglomerados incipientemente estratificados) e de alto grau de alteração (arenitos maciços). Uma sucessão geral de fases diagenéticas pode ser reconhecida, nos beachrocks estudados, incluindo a precipitação de esmectita autigênica, cutículas micríticas, agregados radiais, franjas isópacas de cristais prismáticos, espato equante, cimento criptocristalino de preenchimento de poros e agregados pseudo-peloidais, bem como a infiltração vadosa de sedimentos micríticos, margosos ou sílticos  A ausência de estruturas orgânicas, tais como filamentos e corpos microbiais (bactérias ou fungos), dentro dos cimentos, sugere que o mecanismo por trás da cimentação é essencialmente inorgânico, muito provavelmente devido à evaporação de água do mar, em resposta às condições climáticas secas prevalecentes. Os valores de 13CVPDB máximo, mínimo e médio obtidos para os cimentos são +3.57, –7.8 e +2.34‰, respectivamente. Os valores de 18OSMOW e 18OVPDB variam de 26.32 a 31.41 (valor médio: 30.64) e de –4.41 a 0.54‰ (valor médio: –0.22‰), respectivamente. A maior parte dos valores de 18OVPDB e 13CVPDB é compatível com os de cimentos marinhos. Algumas amostras apresentam valores de 18O fortemente negativos, o que provavelmente reflete uma origem a partir de uma mistura de águas marinhas e meteóricas ou recristalização do cimento marinho através da interação com águas meteóricas. As temperaturas de precipitação assumindo 18OVPDB da água igual a 2,0 (água do mar modificada por evaporação) e -2,0 (água mista, em boa parte meteórica) variam de 23,3 a 34,9oC (valor médio: 25,8oC).|http://hdl.handle.net/10183/5101
Análise quantitativa de pontos de controle para correção geométrica de imagens orbitais|2004|Open Access|Dissertação|Imagens orbitais;Análise quantitativa;Sensoriamento remoto|por|Neste trabalho será feita uma análise da quantidade de pontos de controle para correção geométrica de imagens, mais especificamente, do satélite CBERS-I, utilizando o sensor CCD, através de análises estatísticas para o cálculo da quantidade de pontos e estudo quantitativo através da análise de variância da média dos resíduos obtidos de amostras de tamanhos variados. Os pontos de controle foram coletados com receptor GPS e foi utilizado um modelo polinomial de segunda ordem para a correção geométrica da imagem. Os resultados experimentais obtidos na análise da média para o cálculo da quantidade de pontos mostram que o erro residual tende a se estabilizar para a quantidade de pontos definidos pela estatística. Apresenta-se ao final, considerações iniciais sobre a aplicação desta proposta para diversos outros sensores, permitindo um maior aproveitamento destes na atualização cartográfica e na geração de cartas imagens.|http://hdl.handle.net/10183/5104
Índice de qualidade de praia : o exemplo de Capão da Canoa|2005|Open Access|Dissertação|Geologia marinha;Geologia ambiental;Capão da Canoa (RS)|por|A população dos municípios do litoral norte do Rio Grande do Sul vem crescendo nas últimas duas décadas (IBGE, 2004). Em conseqüência, disso, começaram a surgir diversos problemas ambientais nessa região. Com o objetivo de avaliar o impacto ambiental produzido por esse crescimento foi criado, de forma inédita, um método para aferir o Índice de Qualidade de Praia (IQP), constituído por cinco indicadores, são eles: limites de segurança de praia, plano de manejo de dunas, balneabilidade da água, qualidade sanitária da areia e adequação da disposição final dos resíduos sólidos. Esse método foi testado na zona de balneário do município de Capão da Canoa, no período de dezembro de 2001 a março de 2002. Para tanto, foram atribuídos pesos para cada indicador por parte dos atores – técnicos, turistas e residentes – por meio do método de pesquisa de opinião-técnica Delphi. O somatório do produto do peso pela nota de cada indicador (0 ou 1), resultou numa classificação da praia do tipo C. Nessa categoria a balneabilidade é imprópria para banho. Não existe programa de limites de segurança de praia. Já a qualidade da areia é satisfatória, há área adequada para o depósito dos resíduos sólidos e há plano de manejo de dunas em funcionamento. De acordo com esse resultado, pode-se sugerir uma agenda de ações mitigadoras, ordenadas por prioridades, tendo em vista a gestão ambiental. Essa agenda e a avaliação confirmada com base no IQP pode, a longo prazo, contribuir para melhorar a qualidade de vida dos freqüentadores das praias, diminuir a degradação do ambiente costeiro e medir as melhorias implementadas.|http://hdl.handle.net/10183/5105
Aplicação de imagens ASTER para estudos territoriais no nordeste do estado do Rio Grande do Sul|2004|Open Access|Dissertação|Monitoramento ambiental;Sensor ASTER;Rio Grande do Sul, Nordeste|por|É apresentada uma nova abordagem na técnica e na avaliação de área de florestas nativas, exóticas e uso do solo do Nordeste do Estado do Rio Grande do Sul, em particular no entorno da escarpa que divide os Campos de Cima da Serra e a Planície Costeira, caracterizada por apresentar espécies de Pinus elliotti Engelm var elliottiii e Pinus taeda L., Eucalyptus sp. e Araucaria angustifólia (Bert.) O. Ktze. e áreas de campo nativo. Nas últimas décadas tem se verificado avanço das florestas exóticas, principalmente de florestas de pinus, sobre as áreas de campo e florestas nativas, surgindo como uma das maiores fontes de exploração econômica da região. Parcialmente em razão disto, as florestas de araucária distribuem-se de forma pouco homogênea, em decorrência de décadas de desmatamento. As técnicas de classificação em Sensoriamento Remoto, usando imagens de Landsat, mostraram que é possível separar tipos diferentes de vegetação, e no exemplo das florestas na região estudada, tanto nativas como exóticas. As limitações em definições espacial e espectral até meados da década de 1990 motivaram o desenvolvimento de uma nova geração de satélites e sensores, incluindo o sensor ASTER a bordo do satélite Terra. Este sensor apresenta 14 bandas espectrais com diferentes resoluções espaciais, sendo usado nesta pesquisa suas 9 bandas correspondentes ao espectro de radiância refletida. O foco central deste trabalho está na utilização de sensoriamento remoto e geoprocessamento para determinação de áreas de vegetação e uso do solo no extremo leste dos Campos de Cima da Serra, através de imagens orbitais do sensor ASTER. Utilizando métodos de classificação supervisionada foi possível caracterizar a área, separar as espécies vegetais entre si, além de quantificá-las  O grupo das 9 bandas foram distribuídas em três grupos: com 3 bandas de resolução espacial de 15 metros no visível e infravermelho próximo (VNIR); com 6 bandas com resolução espacial de 30 metros no infravermelho médio (SWIR); e com 9 bandas com resolução espacial de 30 metros cobrindo toda a faixa de resolução espectral do espectro de radiância refletida do sensor ASTER (VNIR+SWIR). A metodologia incluiu processamento de imagem e classificação com o algoritmo de máxima verossimilhança gaussiana. Os resultados são: 1) é possível identificar tipos diferentes de manejo e idade nas florestas de Pinus elliottii (jovens, adulto, velho e manejado diferenciado); 2) a exatidão geral foi de 90,89% no subsistema VNIR, e o índice do Kappa foi de 0,81 em ambos subsistemas VNIR e (VNIR+SWIR); 3) a classificação apresentando o mapa do uso do solo mostra que, de forma geral, os dados de VNIR têm os melhores resultados, incluindo o detalhamento para pequenas áreas da superfície terrestre. O grupo (VNIR+SWIR) têm potencial superior para a determinação das classes araucária , eucalipto e pinus com manejo / pinus adulto , enquanto que o grupo SWIR não apresenta não vence em nenhuma classe; 4) com relação aos dados de exatidão geral resultantes do subsistema VNIR, a área estimada de pinus é 22,28% da área estudada (cena de 1543,63 quilômetros quadrados totais), e de araucária é 10,10%, revelando que aquela espécie exótica está mudando rapidamente a paisagem da região. Na comparação destes resultados com outros estudos na região pesquisada, verifica-se que a utilização de dados ASTER implica em um aumento na acurácia da classificação de vegetação em geral e que este sensor é altamente apropriado para estudos ambientais, devido a suas excelentes características espaciais e espectrais.|http://hdl.handle.net/10183/5107
Aplicação do modelo de RAMS para o estudo de um vórtice ciclônico que atingiu o município de Viamão|2002|Open Access|Dissertação|Ciclones;Vento;Pressao;Umidade relativa;Temperatura;Simulação;Previsão do tempo;Viamão (RS)|por|A partir de dados do CPTEC, utilizou-se o modelo numérico Regional Atmospheric Modeling System (RAMS) para simular as condições ambientais que originaram a formação e evolução de um ciclone no dia 11 de outubro de 2000, no município de Viamão, no estado do Rio Grande do Sul. Utilizou-se três grades aninhadas, com resolução progressivamente mais refinada e de forma simultânea. O modelo simulou com bastante precisão os campos de vento, pressão, umidade relativa e temperatura potencial. As simulações foram analisadas e comparadas à dados de superfície, imagens de satélite, cartas sinóticas e radiossondagens. A topografia e a brisa marítima exerceram forte influencia no sentido de intensificar o sistema convectivo que se formou sobre o Estado do Rio grande do Sul, bem como os altos valores de umidade relativa e o intenso cisalhamento vertical. Com estes procedimentos foi possível analisar e caracterizar o perfil atmosférico do sistema estudado e definir parâmetros meteorológicos que justificaram a formação de um tornado.|http://hdl.handle.net/10183/5109
Aplicação de sensoriamento remoto orbital no mapeamento de unidades vulcano-sedimentar no Platô da Ramada, Vila Nova do Sul, RS|2005|Open Access|Dissertação|Geoquímica : Rochas vulcânicas : Rio Grande do Sul;Sensoriamento remoto : Rio Grande do Sul|por|Os produtos de sensoriamento remoto gerados pelos novos sensores orbitais, aliados ao desenvolvimento de sistemas computacionais e de técnicas de processamento de imagens possibilitam a manipulação numérica das imagens, visando o realce de informações de interesse para a interpretação visual ou automática. O uso dessas técnicas traz boas perspectivas no mapeamento geológico básico, pois possibilita uma visão sinóptica de amplas áreas e a integração de dados litológicos e estruturais de várias fontes. Este trabalho selecionou as imagens do sensor ASTER (Advanced Spaceborne Thermal Emission and Reflection Radiometer) devido as características de suas resoluções espectral e espacial, como as mais promissoras para identificação dos diferentes litotipos da área do Platô da Ramada, em especial as unidades vulcânicas e hipabissais relacionadas e sedimentares subjacentes. O Platô da Ramada, região de Vila Nova do Sul (RS) constitui uma feição geomorfológica de destaque na área, facilmente identificável nas imagens orbitais, sendo formado por uma seqüência vulcânica caracterizada por depósitos efusivos e piroclásticos, de composição dominantemente ácida. Representa uma fração significativa do magmatismo alcalino sódico neoproterozóico no Escudo Sul-rio-grandense e marca um dos ciclos vulcânicos do período pós-colisional do Ciclo Brasiliano  Este estudo testou e avaliou diversos processamentos das imagens multiespectrais para diferenciação litológica, a identificação de alvos e a definição de morfoestruturas da área do Platô da Ramada. A integração de dados geológicos existentes com os produtos obtidos neste estudo possibilitou novas informações anexadas ao mapa geológico utilizado como verdade terrestre. O processamento utilizando a técnica de Transformação por Componentes Principais Seletivas proporcionou os melhores resultados realçando diferenças espectrais existentes entre as rochas vulcânicas e hipabissais e as rochas sedimentares. A partir dessa técnica, selecionou-se as imagens CP2 dos pares das bandas 4-5 (R), 3-4 (G) e 8-9 (B) na geração de uma composição colorida. Esta imagem permitiu a diferenciação espectral entre as rochas vulcânicas do Platô da Ramada e as rochas sedimentares do Grupo Maricá, bem como a individualização, no Grupo Maricá, de duas subunidades, levando-se em conta a densidade de intrusões de diques riolíticos ao norte da área. Identificou-se, ainda, um corpo diorítico com forma elíptica na borda SW do Platô da Ramada, não registrado anteriormente.|http://hdl.handle.net/10183/5115
Bioestratigrafia de Nanofósseis calcários e estratigrafia de isótopos (C e O) do talude médio, quaternário, porção N da Bacia de Campos, ES|2005|Open Access|Dissertação|Estratigrafia : Quaternário;Bioestratigrafia|por|A Bacia de Campos, em sua porção norte, na parte sul da área oceânica confrontante ao Estado do Espírito Santo é uma área ainda pouco estudada e conhecida, principalmente no que se refere aos sedimentos quaternários em águas profundas. Foram escolhidos dois testemunhos do talude médio na área, nos quais foram realizadas amostragens para Bioestratigrafia de Nanofósseis Calcários e Estratigrafia de Isótopos (Carbono e Oxigênio). Os resultados de Nanofósseis Calcários mostraram que a base do bioevento de acme de Emiliania huxleyi deve ser localizada em 74 mil anos BP na área, ao contrário do que foi proposto anteriormente. Os dados exibem também uma predominância absoluta de dois taxa: Emiliania huxleyi e Gephyrocapsa spp. A Estratigrafia de Isótopos de Oxigênio indicou uma excelente correlação com os modelos, permitindo um bom balizamento com os estágios isotópicos de oxigênio existentes. A integração da Bioestratigrafia de Nanofósseis Calcários e Estratigrafia de Isótopos podem explicar o comportamento dos taxa majoritários. A correlação da abundância relativa de E. huxleyi e de d18O revelou-se boa, indicando influência da paleotemperatura na abundância deste taxa. Os dados de Estratigrafia de Isótopos de Carbono mostraram uma concordância com as variações de abundância do gênero Gephyrocapsa, sugerindo que a disponibilidade de nutrientes mudou na área.|http://hdl.handle.net/10183/5117
Estudo da hidrogenação catalítica de NBR|2005|Open Access|Dissertação|Borracha nitrílica;Hidrogenação catalítica|por|Neste trabalho temos o Estudo da Hidrogenação Catalítica de NBR em meio Homogêneo, empregando complexos dos metais de transição. Nos experimentos de hidrogenação foram testados os complexos de paládio e rutênio em duas etapas diferentes. Para estes complexos, foram estudados os efeitos de temperatura, pressão, tempo e concentração de catalisador. Ainda, investigou-se os efeitos do teor de sólido de NBR em solução. Os solventes utilizados foram acetona e metiletilcetona. A Hidrogenação do Butadieno-Acrilonitrila (NBR) foi realizado em escala laboratorial num reator Parr de 1 L. No sistema utilizando o complexo de Paládio, as duplas ligações foram totalmente convertidas nas condições de 60 °C, 27 atm, 2 g de NBR, 58 mg de catalisador e 1 h de reação. Nas reações com o complexo de Rutênio, a conversão total ocorreu nas condições de 140 °C, 40 atm, 10 g de NBR, 60 mg de catalisador e 8 h de reação. As Hidrogenações de NBR mostraram-se efetiva para ambos os catalisadores atingindo a máxima conversão (100% em mol) nos dois sistemas. No entanto, o sistema utilizando catalisador de rutênio mostrou-se mais efetivo e viável para a produção de HNBR em escala industrial.|http://hdl.handle.net/10183/5118
Tecnologias de informação e comunicação como meio de ampliar e estimular o aprendizado de física|2005|Open Access|Dissertação|Ensino de física;Tecnologias de Informação e Comunicação (TICs)|por|Neste trabalho, implementamos o uso de Tecnologias de Informação e Comunicação no ensino de Física em nível médio a fim de ampliar e estimular a aprendizagem dos alunos. Construímos um sítio sobre Gravitação e Temas Afins, repleto de informações, ilustrações e principalmente animações interativas tipo applets. Utilizamos como suporte do curso, para infra-estrutura virtual de comunicação, a plataforma de ensino a distância - TelEduc, onde exploramos ferramentas que estimularam a comunicação entre os professores e os alunos, e possibilitaram o depositório dos resultados das atividades realizadas. O sítio e o curso criado no TelEduc disponibilizamos em um servidor no Instituto de Física da UFRGS. Implementamos esta experiência didática em duas turmas da primeira série do ensino médio do Colégio Salesiano Dom Bosco na cidade de Porto Alegre, RS, no mês de novembro de 2003. Os referenciais teóricos que adotamos foram a teoria de aprendizagem significativa de Ausubel, a teoria de educação de Novak e o modelo de ensino-aprendizagem de Gowin, especificamente no que se refere à motivação dos alunos. Nossa avaliação indica que conseguimos propiciar uma extensão da sala de aula, aumentando virtualmente a carga horária de Física em no mínimo 40%. Notamos também, nas participações dos alunos, que 67% se envolveram no projeto e 82% expressaram serem favoráveis a esta proposta de aprendizagem de Física. Estes resultados sugerem que conseguimos motivar significativamente os alunos.|http://hdl.handle.net/10183/5119
Caracterização de remanescentes de aglomerados abertos na galáxia|2005|Open Access|Tese|Astrofísica;Aglomerados estelares;Diagramas;Remanescentes estelares;Aglomerados abertos e associações;Observacoes astronomicas;Fotometria astronômica;Espectroscopia;Evolucao estelar|por|o presente trabalho busca ampliar o conhecimento das relações entre aglomerados abertos de estrelas e seus remanescentes. Do ponto de vista observacional, um remanescente pode ser definido como uma concentração pouco povoada de estrelas resultante da evolução dinâmica de um sistema inicialmente mais massivo. Apesar do avanços no conhecimento teórico a respeito desses objetos e, nos últimos anos, da busca pela identificação observacional dos mesmos ter aumep.tado, muitas questões permanecem em aberto. Assim, no presente estudo serão analisados 23 candidatos a remanescentes através de dados fotométricos, espectroscópicos e de movimentos próprios. Esses dados fornecem informações sobre os objetos e seus campos. Por meio destas, buscam-se estabelecer critérios de definição de remanescentes de aglomerados abertos, levando-se em conta incertezas observacionais. Os dados fotométricos no infravermelho oriundos do catálogo The Two Micron Ali sky Survey possibilitam, nesse estudo, (i) estudar as propriedades estruturais dos objetos por meio dos perfis de densidade radial de estrelas; (ii) testar a semelhanças entre objetos e campos através de um método estatístico de comparação entre distribuições de estrelas no plano do diagrama cor-magnitude e, (iii) obter idades, avermelhamentos e distâncias com o uso de diagramas cor-magnitude, além de distinguir os objetos em função de um índice de ajuste de isócronas. Os dados espectroscópicos obtidos através de observações óticas no Complejo Astronómico EI Leoncito (Argentina) fornecem para 12 objetos da amostra informações adicionais de avermelhamentos e idades. Os dados cinemáticos extraídos do The Second U. S. Naval Observatory CCD Astrograph Catalog permitem, por sua vez, uma comparação objetiva entre a distribuição de movimentos próprios dos objetos e campos próximos de grande ângulo sólido  O emprego desses métodos complementares se mostra essencial no estudo em função da carência de dados completos para os candidatos a remanescentes. No que diz respeito à amostra, em geral não é possível afirmar individualmente qual objeto caracteriza-se de forma definitiva como um remanescente de aglomerado aberto devido à incompleteza dos dados, às incertezas observacionais e à baixa estatística. Entretanto, os métodos desenvolvidos permitem uma análise objetiva e sugerem a presença de remanescentes de aglomerados abertos na amostra. Além disso, há evidência da presença de binarismo, o que é esperado para sistemas evoluídos dinamicamente. Portanto, pode-se inferir sobre estágios evolutivos para remanescentes a partir das distribuições de movimento próprio de suas estrelas e de seu mapeamento no diagrama cor-magnitude.|http://hdl.handle.net/10183/5144
Integração de técnicas de SIG e sensoriamento remoto na classificação de imagens digitais através do uso da teoria da evidência|2003|Open Access|Dissertação|Imagem digital : Classificação;Sensoriamento remoto;Sistemas de Informação Geográfica (SIG);Modelo bayesiano;Floresta nativa|por|O desenvolvimento de novos, e mais eficientes, métodos de classificação de imagem digitais em Sensoriamento Remoto se constitui em uma importante área que tem chamado a atenção de muitos pesquisadores. Nesta área em particular, um problema que freqüentemente surge para a classificação de imagens digitais provenientes de cenas naturais, é a ocorrência de classes espectrais com resposta espectral muito similar. Nestes casos, os sistemas sensores mais comuns e os métodos tradicionais de classificação de imagem apresentam muito baixa precisão ou mesmo falham completamente. Várias abordagens vem sendo propostas na literatura. Uma das possíveis abordagens consiste em fazer uso de informações auxiliares que possuam poder discriminante para as classes sob análise. Esta é a possibilidade explorada nesta dissertação, utilizar-se de dados auxiliares, provenientes de fontes diversas, tais como: temperatura, precipitação, altitude e classes de solo. Estes dados são então combinados com dados provenientes de imagens multiespectrais de acordo com a Teoria de Evidência proposta por Dempster e Shafer. Esta abordagem é testada usando dados de uma área no Estado do Rio Grande do Sul, Brasil, com a finalidade de delimitar a ocorrência da Mata Nativa com Araucária (composta pela conifera Araucaria angustifolia), que é de difícil separação em relação a outras classes espectrais que ocorrem na região, tornando difícil o processo preciso de classificação.|http://hdl.handle.net/10183/5170
Localização de sítios astronômicos através de imagens dos satélites NOAA|1999|Open Access|Dissertação|Observacoes astronomicas;Imagens de satelite|por|Este trabalho se ocupa do problema da localização de sítios adequados para a operação de observatórios astronômicos. O constante crescimento das cidades e a urbanização de grandes áreas vizinhas é acompanhada por um aumento proporcional da iluminação artificial, o que resulta em níveis maiores de brilho do céu noturno. Isto tem consequências extremas para a astronomia óptica, a qual exige um céu escuro, e justifica buscas de novos sítios para a instalação de futuros telescópios ópticos. Um dos critérios mais importantes para sítios astronômicos, além de céu escuro, é uma alta proporção de céu claro, sem nuvens. Buscas de sítios astronômicos são estudos conduzidos ao longo de períodos de tempo de anos. É sugerido que imagens de satélites meteorológicos podem ser úteis para a seleção preliminar destes sítios. A metodologia utilizada é fundamentada em correções geométricas de imagens de dados orbitais das naves NOAA12 e NOAA14 e na soma de imagens obtidas em datas diferentes. As imagens foram coletadas pela estação de recepção instalada no Centro Estadual de Pesquisas em Sensoriamento Remoto e Meteorologia da UFRGS. Somando, pixel por pixel, imagens colhidas em datas diferentes, após correções geométricas, obtém-se médias temporais de cobertura de nuvens, o que é o equivalente moderno da construção de médias a partir de uma série temporal de dados meteorológicos de estações terrestres. Nós demonstramos que esta metodologia é factível para este tipo de dado, originário de órbitas de menor altitude e melhor resolução, se comparado com imagens vindas de satélites de órbitas altas, geoestacionários, com menor resolução, imagens estas de manipulação muito mais fácil, pois o ponto de observação tende a ser estável no tempo.|http://hdl.handle.net/10183/5172
Solução da equação de difusão unidimensional transiente para o estudo da dispersão de poluentes na camada limite planetária|2004|Open Access|Dissertação|Camada limite planetária;Equação difusão-advecção;Transformada de Laplace;Transformadas integrais|por|Neste trabalho apresenta-se uma solução analítica para a dispersão vertical turbulenta em uma Camada Limite Convectiva e em uma Camada Limite Estável. A equação analisada considera a difusão com velocidades finitas, o que representa o transporte turbulento fisicamente correto. Considerando o caráter não-local, adicionam-se na equação que representa uma fonte área instantânea, termos como: o tempo de relaxação, a assimetria, a escala de tempo Lagrangeana e a velocidade turbulenta vertical. A solução é obtida utilizando-se a técnica da Transformada de Laplace. Os parâmetros que encerram a turbulência são derivados da teoria de difusão estatística de Taylor combinada com a teoria de similaridade. Foram utilizados coeficientes de difusão especáficos para cada uma das camadas. A transformada inversa é obtida através do esquema numérico de quadratura Gaussiana. São apresentadas várias simulações para diferentes alturas de fonte área e obtém-se o valor da concentração para alturas próximas ao solo e próximas ao topo da Camada Limite Planetária. A inserção do termo de contra-gradiente na equação resultou em uma pequena influência na concentração de poluentes, observada de forma mais expressiva na Camada Limite Convectiva.|http://hdl.handle.net/10183/5180
Solução semi-analítica da equação de Langevin assintótica para o deslocamento aleatório pelo método Picard|2004|Open Access|Dissertação|Equação de deslocamento aleatório;Dispersão atmosférica;Método iterativo de Picard;Teoria estatística de Taylor|por|Neste trabalho é desenvolvida uma solução semi-analítica para a Equação de Langevin assintótica (Equação de Deslocamento Aleatório) aplicada à dispersão de poluentes na Camada Limite Convectiva (CLC). A solução tem como ponto de partida uma equação diferencial de primeira ordem para o deslocamento aleatório, sobre a qual é aplicado o Método Iterativo de Picard. O novo modelo é parametrizado por um coeficiente de difusão obtido a partir da Teoria de Difusão Estatística de Taylor e de um modelo para o espectro de turbulência, assumindo a supersposição linear dos efeitos de turbulência térmica e mecânica. A avaliação do modelo é realizada através da comparação com dados de concentração medidos durante o experimento de dispersão de Copenhagen e com resultados obtidos por outros quatro modelos: modelo de partículas estocástico para velocidade aleatória (Modelo de Langevin), solução analítica da equação difusão-advecção, solução numérica da equação difusão-advecção e modelo Gaussiano. Uma análise estatística revela que o modelo proposto simula satisfatoriamente os valores de concentração observados e apresenta boa concordância com os resultados dos outros modelos de dispersão. Além disso, a solução através do Método Iterativo de Picard pode apresentar algumas vantagem em relação ao método clássico de solução.|http://hdl.handle.net/10183/5191
Geologia e geoquimica do complexo cambaizinho, sao gabriel - RS|1990|Open Access|Dissertação|Metamorfismo;Complexo cambaizinho;Rochas ultramaficas|por|Mapeamentos geológicos feitos pelo autor na região do Arroio Cambaizinho - são Gabriel/RS, resultaram na definição do Complexo Cambaizinho, representado pelas seqüências meta-sedimentar e máfica-ultramáfica, intimamente associadas ao longo de toda a extensão da associação supracrustal. A seqüência meta-se dimentar é constitu1da por gnaisses quartzo-feldspáticos dominan tes, anfibolitos bandados e quartzitos subordinados, derivados de sedimentos areno-pelítico-carbonatados estruturados, de forma ritmica em ambiente subaquoso. Niveis composicionais de ocorrência restrita contendo estaurolita, definem o grau metamórfico (médio) para esta região. Intercalações de serpentinitos, xistos magnesianos variados e anfibolitos a granulação fina, na forma de camadas e/ou lentes interestratificadas nos meta-sedimentos indicam suas derivações a partir de derrames e/ou intrusões , ígneas de pequena profundidade de composição básica-ultrabásica. Este juntamente com níveis de sedimentos qu1micos intercalados e corpos de gabros, constituem a seqüência máfica-ultramáfica. O complexo, representa o segmento norte de um cintu rão supracrustal polideformado de forma geométrica aproximadamente linear, com orientação NNE, que se extende desde a localidade de Passo do Ivo, situado mais a sul, até a região objeto deste trabalho.  Quatro fases de deformação dúcteis foram das para a área, estando as duas primeiras (Dl e D2) identifica associadas xx aos eventos metamórficos regionais Ml e M2. O metamorfismo mais antigo (Ml), assinalado por paragêneses diagnósticas em metapelitos alcançou o fácies anfibolito (zona da estaurolita), estando representado em outras litologias pela ocorrência de olivina metamórfica em paragênese com tremolita e/ou talco (meta-serpentinitos) e hornblenda mais oligoclásio/andesina em meta-básicas. O M2, mais jovem, atingiu o fácies xistos verdes, cujas assembléias mineralógicas se associam à foliação S2, de distribuição ir regular ao longo do cinturão. As condições fisicas de Ml foram de média P/T, similares às do metamorfismo Dalradiano. Intrusões graniticas na forma de lâminas (corpos ta bulares) durante a segunda fase de deformação D2, datados pelo mé todo Rb/Sr em 661 :: 29 Ma e agrupados sob a denominação de Granatóides Sanga do Jobim, fornecem idades mínimas para o complexo. Os vários grupos composiciconais da seqüência máfic~ -ultramáfica, individualizados com base em critérios petrográf~ cos e conteúdo de elementos maiores correspondem a:  serpentinitos e olivina-talco ultramafitos (cumulados komatilticos); xis tos magnesianos à talco e clorita e anfibólio xistos (komatiitos); clorita-hornblenda xistos (basaltos komatiiticos), litos e metagabros (basaltos e gabros tolelticos). anfibo Estes vários tipos litológicos foram originados através de diferentes graus de fusão parcial do manto como indicado pelo hiato composicional de MgO (11 à 17%) e os diferentes padrões de ETR existentes entre os anfibolitos/metagabros (toleitos) e os serpentinitos/xistos magnesianos (komatiltos).As variações composicionais no interior de cada grupo, foram controladas pelo fracionamento (acumulação ou extração) de olivina e pouco ortopiroxênio (serpentinitos e olivina-talco ultramafitos)clinopiroxênios (clorita e anfibólios xistos, clorita hornblenda xistos), clinopiroxênio e plagioclásio (anfibolitos e metagabros). As abundâncias e os padrões de ERTL (elementos terras raras leves) enriquecidos, juntamente com os baixos valores das razões A1203/Ti02 e CaO/Ti02 das amostras de xistos magnesiinos das camadas A e B sugerem derivações deste material a partir de baixas percentagens de fusão de um manto enriquecido em mentos incompatíveis. As anomalias negativas de Ce e Eu na ele maio ria das rochas da seqüência máfica-ultramáfica indicam que protólitos ígneossofreram alterações em ambiente submarino.;Geologic mapping performed by the author on the Camb~ izinho area resulted in the separation of the Cambaizinho Complexo This includes sedimentary and mafic-ultramafic metamorphosed se quences which area closely intertongued all over the supracrustal association. Meta sedimentary sequence is built up mainly by quartz- feldspáthic gneisses and less abundant banded amphibolites with minor amounts of quartzites. These supposedely represent metamorphosed subaqueous rithmically banded arenaceous marly sediments. At some levels of restrict occurrence, representing an iron rich composition, staurolite bearing metamorphic assemblages suggests medium grade of metamorphism for this region. Interfingered serpentinites, some varieties of magnesian schists and fine grained amphibolites enclosed in the meta sedimentary rocks suggests lava flows and low depth intrusions of basics/ultrabasic composition. These volcanic magmatic rocks altogether with gabro bodies and interlayered chemical sediments built up the mafic-ultramafic sequence. Cambaizinho Complex represents the northern segment of a supracrustal, multideformed, linear belt trending NNE which stretches from this area till Passo do Ivo to the south.  Four deformation phases were recognized for this area being first and second(Dl and D2) associated to regional metamorphic events, Ml and M2. The oldest metamorphic episode (Ml) signaled by diagnostic paragenesis in metapelites reached amphibolite facies (staurolite zone) being represented in magnesian rocks by olivine-tremoliteItalc (meta-serpentinites) and hornblende-oligoclase/ andesine in metabasites. M2 metamorphic event, younger is represented by greenschist facies whose mineralogic assemblages are associated to S2 foliation irregularly distributed along the belt. Physical conditions for Ml metamorphism of intermediate values for P/T are comparable to those of Dalradian metamorphism. Granitic intrusions form sheaf-like bodies belong to the second phase of deformation (D2) give the minimum Rb/Sr age of 661 z29 Ma for the whole complex and were named Sanga do Jobim Gra nitoids. The whole compositional range of the mafic-ultramafic sequence separated on the petrographic cri teria and major elements contents are named serpentinites and olivine-talc ultramaphites (komatiitic cumulates), magnesian talc schists and chlorite-amphibole schists (komatiites), chlorite-hornblende schists (basaltic komatiites) and amphibolites and meta-gabros (tholeitic and gabros).  The lithologic types above are thought to have originated by different degrees of partial mantle fusion as suggested by MgO hiatus (11-17%) and various ETR patterns found for amphibQ lites and meta-gabros(tholeites) and serpentinites/magnesian sch! sts (komatiites). Compositional variations in each group were cog troled by fractionation (accumulation/extraction) of olivine and minor orthopyroxene (serpentinites and olivine-talc ultramaphites) pyroxenes and lesser amounts of olivine (talc magnesian schists), clinopyroxenes (chlorite and amphibole schists and chlorite-hornblende schists), clinopyroxene and plagioclase (amphibolites and meta-gabros) . Abundancies and enriched patterns of LREE with low values of Al2Oa/Ti02 and CaO/Ti02 rates of altogether magnesian schists of A & B layers suggests derivations of this material from feeble percentages of fusion of the mantle enriched in incompatible elements. Negative Ce and Eu anomalies in most rocks of th~ mafic-ultramafic sequence point to protolites submited to altera tion in submarine environment.|http://hdl.handle.net/10183/5238
Evolução mineralógica da alteração laterítica em rochas vulcânicas básicas na borda sudeste da Bacia do Paraná (Rio Grande do Sul e Santa Catarina)|1995|Open Access|Tese|Petrologia;Geomorfologia;Rochas vulcânicas : Paraná, Bacia do;Mineralogia|por|Perfis de alteração em basaltos com baixos teores de Ti02 (LTiB) da Parte Sudeste da Bacia do Paraná (SPB) associam-se a superfícies aplainadas, nos planaltos das Araucárias (Ab' Saber, 1973), entre altitudes de 950 m a 750 m (Vacaria) e 1000m a 920m (a Sul de Lages). Em domínios mais dissecados do relevo, que crescem de Este para Oeste, e nas encostas intensamente dissecadas destes planaltos a Sul (calha do rio Antas) e a Norte (calha do rio Pelotas), os perfis de alteração são truncados ou inexistentes. A associação dos perfis de alteração com superfícies geomorfológicamente mais antigas (aplainadas e elevadas) faz supor que o início dos processos de alteração seja correlativo às superfícies aplainadas, antigo e, provável mente, Terciário. As sequências de alteração mais completas localizam-se em morros de topo plano e apresentam as seguintes fácies: Rocha mãe, saprólito, alterito argiloso, alterito esferoidal, ""stone line"", coberturas móveis e solo atual. Químicamente os produtos de alteração intempérica dos basaltos são ""lateritas"" segundo definição de Schellmann (1981), com enriquecimento em Fe2D.3 e H20; perdas em Si02, FeO, MgO, CaO, Na20 e K20; prováveis pequenas perdas emA12D.3.  No saprólito, os pedaços de rocha fragmentada permitiram a descrição das alterações hidrotermais refletidas nas para gêneses dos sítios intersticiais, constitui dos por materiais cristalinos e criptocristalinos. Os cristais de titanomagnetita aparecem com manchas azuis irregulares que sugerem variações cristaloquímicas contínuas dentro de um mesmo cristal, típicas da maghemitização. O alterito argiloso é sede de pseudomorfoses dos minerais magmáticos e hidrotermais. Esmectitas, ocupam os sítios das camadas mistas hidrotermais; halloysitas 7 e 10 Á são dominantes nos plasmas das pseudomorfoses de plagioclásios e plasmas ricos em ferro e sílica predominam nas pseudomorfoses de piroxênios. Observa-se a transição halloysita -caolinita desordenada rica em ferro estrutural nas partes superiores do conjunto. Os plasmas secundários são silico-aluminosos, nas partes baixas do conjunto, e predominantemente opacos no topo. Estes plasmas constituem-se de halloysita, litioforita (ou plasma rico em Mn), goethita e maghemita. O alterito esferoidal apresenta o núcleo de rocha e um córtex de cor amarelo -alaranjada em que se verifica a presença dominante da goethita aluminosa. Secundáriamente, aparecem cristobalita, maghemita e gibbsita. As coberturas móveis, são constitui das de plasma caolinítico e plasmas ricos em hematita e goethita. Aparecem ainda grânulos, pisólitos e nódulos herdados de antigas couraças desmanteladas.  Os minerais, formados em condições lateritizantes, são os filossilicatos halloysita 7Á e lOÁ, caolinita desordenada e os óxidos e hidróxidos, hematita, goethita, gibbsita e litioforita.Observou-se que a mineralogia de alteração está intimamente associada à textura da rocha original. Encontram-se, ainda, nestes horizontes de alteração intempérica, a cristobalita (metaestável) e a titanomaghemita. As titanomaghemitas identificadas nos saprólitos e alteritos apresentam as características de maghemitização: diminuição da taxa 32(Fe+ Ti)/O, aumento de lacunas na malha cristalina e diminuição do parâmetro ~. Mg diminui com o intemperismo, Mn e AI se concentram nas fases magnéticas. A halloysita 7Á predomina sobre a lOÁ, na fração < 2Jlm, do alterito argiloso, alterito esferoidal e no sistema fissural. A caolinita predomina no horizonte ""tacheté"". No alterito esferoidal, ocorre também caolinita e esmectita. As argilas halloysíticas apresentam quatro morfologias: esferas, tubos, lamelas planares e cones. A halloysita forma-se preferencialmente à caolinita no córtex de alteração do alterito esferoidal e na fácies argilosa, constituindo um primeiro estágio de intemperismo. Os tubos e cones têm os menores teores de Fe2Ü3 enquanto as halloysitas planares têm os mais altos teores de Fe2Ü3. O teor de Fe das partículas esferoidais é variado. Os óxidos e hidróxidos destes perfis caracterizaram variações da atividade a água, de atividade da sílica dissolvida e temperatura, refletindo as paleocondições (climáticas) de formação destas coberturas fósseis.|http://hdl.handle.net/10183/5239
Integração de dados geológicos, de sensoriamento remoto, espectrorradiométricos e geofísica aplicada à prospecção de depósitos filoneanos de fluorita hidrotermal no sudeste de Santa Catarina|2002|Open Access|Tese|Fluorita;Alteração hidrotermal;Sensoriamento remoto;Espectrorradiometria;Santa Catarina|por|Os processamentos de imagens orbitais efetuados através de técnicas de sensoriamento remoto geraram informações qualitativas de natureza textural (morfo-estruturas). Estas permitiram (1) o reconhecimento de áreas com diferentes padrões estruturais tendo diferentes potencialidades para a prospecção de fluorita, (2) a identificação de novos lineamentos estruturais potencialmente favoráveis à mineralização e (3) evidenciaram prolongamentos extensos para as principais estruturas mineralizadas, (4) às quais se associam um grande número de estruturas, antes desconhecidas, com grande potencial prospectivo. O aprimoramento de técnicas de classificação digital sobre produtos de razões de bandas e análise por componentes principais permitiu identificar a alteração hidrotermal associada às estruturas, incorporando novos critérios para a prospecção de fluorita. Buscando-se quantificar os dados de alteração hidrotermal, foi efetuada a análise espectrorradiométrica das rochas do distrito fluorítico. Integrando estas informações com dados TM LANDSAT 5, em nível de reflectância, obteve-se a classificação espectral das imagens orbitais, o que permitiu a identificação de estruturas menores com um detalhe nunca antes obtido.  Os processamentos de dados aerogeofísicos forneceram resultados sobre estruturas (magnetometria) e corpos graníticos afetados por alteração hidrotermal (aerogamaespectrometria). Estes produtos foram integrados com dados TM LANDSAT 5 associando o atributo textural da imagem orbital ao comportamento radiométrico das rochas. Diagnosticou-se o lineamento Grão-Pará como o principal prospecto do distrito. E levantaram-se uma série de dados sobre a compartimentação tectônica da região, a zonação de fácies das rochas graníticas (rocha fonte do flúor) e as alterações hidrotermais associadas ao magmatismo granítico. Isto permitiu a compreensão da distribuição regional dos depósitos de fluorita, adicionando-se um novo critério à prospecção de fluorita, a relação espacial entre a mineralização e a rocha fonte de F. Esta última corresponde à fácies granítica da borda do Maciço Pedras Grandes.;Digital image processing in orbital images by remote sensing techniques generated qualitative textural information (morph structures). These allowed (1) the recognition of areas in different structural patterns with different fluorite search potentialities, (2) identification new structures potentially fluor-bearing and (3) evidence of extensive increase from the principal mineralized structures, (4) It’s associated a great number of structures, before ignored, that have great prospective potential. The accuracy of techniques of digital classification on products of ratio analysis by principal components showed the alteration associated to the structures, incorporating new criteria for the fluorite search. Searching for quantify the alteration; the spectral analysis of the rocks in fluor district was employed. Integrating reflectance information with TM LANDSAT 5 data, obtained the classification of the orbital images, identifying smaller structures in detail. Geophysical data processing supplied results on structures (magnetometric) and granites alteration affected (aerogamaspectrometric). These products were integrated with TM LANDSAT 5 data, associating textural attribute in orbital image to radiometric behavior of the rocks. The Grão-Pará lineament was diagnosed as the principal into district. Tectonic blocking data, facies zonation in granites (F source rock) and alteration associated to the granite magmatism. This allowed to understanding regional distribution of the fluorite deposits, and defined new criteria to fluorite prospecting, spatial relationship by mineralization and rock source of fluor. This one is the external granitic facies of Pedras Grandes Massif.|http://hdl.handle.net/10183/5246
Circulações atmosféricas clássicas e não-clássicas na Região Metropolitana de Porto Alegre/RS|2000|Open Access|Dissertação|Sensoriamento remoto;Circulação atmosférica|por|O Regional Atmospheric Modeling System (RAMS), versão 3b, foi utilizado para simular o escoamento na Região Metropolitana de Porto Alegre (RMPA) para explicar a influência de circulações clássicas e não clássicas devido as proximidades do oceano e da Lagoa dos Patos, e da ilha de calor formada pelas cidades. O RAMS foi utilizado em um domínio tridimensional com duas grades sendo utilizadas, a grade mais grossa com resolução horizontal de 8 Km, e a grade mais fina com resolução horizontal de 2 Km. A inicialização do modelo é homogênea e parte do repouso, em uma situação de verão. Foram realizados 3 experimentos; o primeiro deles com apenas a grade mais grossa e com a parametrização da vegetação sendo grama curta em todo o domínio; o segundo utilizando as duas grades, sendo que a área coberta pela grade mais fina é parametrizada como deserto, nas duas grades; o terceiro diferencia-se do segundo pela adição de uma forçante térmica no primeiro nível da área coberta pela grade mais fina durante o período da noite (após o pôr do sol). Os resultados mostram que circulação de brisa lacustre/terrestre é responsável por uma forte tendência meridional no escoamento resultante sobre a RMPA, só sendo atenuada pela componente zonal da circulação de brisa marítima à partir do final da tarde. A confluência dessas duas circulações provoca uma tendência a elevação da camada de mistura. A presença da ilha de calor aumenta o movimento convectivo, com maior transporte vertical de calor sensível, e também retardou o avanço da frente de brisa marítima, embora durante a noite a sua influência não tenha sido muito significante.|http://hdl.handle.net/10183/5295
Uso da análise discriminante regularizada (RDA) no reconhecimento de padrões em imagens digitais hiperespectral de sensoriamento remoto|2001|Open Access|Dissertação|Sensoriamento remoto;Análise discriminante|por|Em cenas naturais, ocorrem com certa freqüência classes espectralmente muito similares, isto é, os vetores média são muito próximos. Em situações como esta, dados de baixa dimensionalidade (LandSat-TM, Spot) não permitem uma classificação acurada da cena. Por outro lado, sabe-se que dados em alta dimensionalidade [FUK 90] tornam possível a separação destas classes, desde que as matrizes covariância sejam suficientemente distintas. Neste caso, o problema de natureza prática que surge é o da estimação dos parâmetros que caracterizam a distribuição de cada classe. Na medida em que a dimensionalidade dos dados cresce, aumenta o número de parâmetros a serem estimados, especialmente na matriz covariância. Contudo, é sabido que, no mundo real, a quantidade de amostras de treinamento disponíveis, é freqüentemente muito limitada, ocasionando problemas na estimação dos parâmetros necessários ao classificador, degradando portanto a acurácia do processo de classificação, na medida em que a dimensionalidade dos dados aumenta. O Efeito de Hughes, como é chamado este fenômeno, já é bem conhecido no meio científico, e estudos vêm sendo realizados com o objetivo de mitigar este efeito. Entre as alternativas propostas com a finalidade de mitigar o Efeito de Hughes, encontram-se as técnicas de regularização da matriz covariância.  Deste modo, técnicas de regularização para a estimação da matriz covariância das classes, tornam-se um tópico interessante de estudo, bem como o comportamento destas técnicas em ambientes de dados de imagens digitais de alta dimensionalidade em sensoriamento remoto, como por exemplo, os dados fornecidos pelo sensor AVIRIS. Neste estudo, é feita uma contextualização em sensoriamento remoto, descrito o sistema sensor AVIRIS, os princípios da análise discriminante linear (LDA), quadrática (QDA) e regularizada (RDA) são apresentados, bem como os experimentos práticos dos métodos, usando dados reais do sensor. Os resultados mostram que, com um número limitado de amostras de treinamento, as técnicas de regularização da matriz covariância foram eficientes em reduzir o Efeito de Hughes. Quanto à acurácia, em alguns casos o modelo quadrático continua sendo o melhor, apesar do Efeito de Hughes, e em outros casos o método de regularização é superior, além de suavizar este efeito. Esta dissertação está organizada da seguinte maneira: No primeiro capítulo é feita uma introdução aos temas: sensoriamento remoto (radiação eletromagnética, espectro eletromagnético, bandas espectrais, assinatura espectral), são também descritos os conceitos, funcionamento do sensor hiperespectral AVIRIS, e os conceitos básicos de reconhecimento de padrões e da abordagem estatística.  No segundo capítulo, é feita uma revisão bibliográfica sobre os problemas associados à dimensionalidade dos dados, à descrição das técnicas paramétricas citadas anteriormente, aos métodos de QDA, LDA e RDA, e testes realizados com outros tipos de dados e seus resultados.O terceiro capítulo versa sobre a metodologia que será utilizada nos dados hiperespectrais disponíveis. O quarto capítulo apresenta os testes e experimentos da Análise Discriminante Regularizada (RDA) em imagens hiperespectrais obtidos pelo sensor AVIRIS. No quinto capítulo são apresentados as conclusões e análise final. A contribuição científica deste estudo, relaciona-se à utilização de métodos de regularização da matriz covariância, originalmente propostos por Friedman [FRI 89] para classificação de dados em alta dimensionalidade (dados sintéticos, dados de enologia), para o caso especifico de dados de sensoriamento remoto em alta dimensionalidade (imagens hiperespectrais). A conclusão principal desta dissertação é que o método RDA é útil no processo de classificação de imagens com dados em alta dimensionalidade e classes com características espectrais muito próximas.;The remote sensing analysis of natural scenes has been relying primarily on data collected by sensors that provide a relatively small number of spectral bands. In most of the cases, this low dimensional image data has proved capable of performing image data classification in an acceptable way. There are some cases, however, in which some or all of the classes involved are spectrally very similar, i.e., their mean vectors are nearly identical. In these cases, the low dimensional image data yield a very low classification accuracy. This problem may be solved by using high dimensional image data. It is well known that high dimensional image data allows for the separation of classes that are spectrally very similar , provided that their covariance matrices differ significantly. One problem with high dimensional data, however, is related to the estimation of the required parameters, specially the class covariance matrices. As the data dimension increases, so does the number of parameters to be estimated. In real world conditions, however, the sample size normally available for parameter estimation is limited, resulting in poor estimates for the parameters. This problem becomes apparent when one compares the classification accuracy against the data dimensionality. Initially the accuracy of the classifier tends to increase as the number of spectral bands increase, i.e., as the data dimensionality becomes larger.  Eventually the accuracy peaks and as the data dimensionality continues to increase the classification accuracy tends to decrease. This pattern, known as the ""Hughes Phenomenon"" is caused by the gradual deterioration of the parameters estimation as the data dimensionality keeps increasing while the number of available training samples remains constant. One possible way to mitigate this problem consists in the procedure known as regularization of the covariance matrix. This procedure can be performed in two steps: (1) replacing the class covariance matrix by a linear combination of the class covariance matrix and the common covariance matrix for all classes involved in the process and (2) scaling the class covariance matrices to counter the bias that tend to estimate the small eigenvalues too small and the large eigenvalues too large. This bias occur whenever the number of training samples is too small compared with the data dimensionality. This study is concerned with the applications of the regularization techniques to remote sensing high dimensional image data such as the data provided by the AVIRIS sensor system. Methods using the conventional quadratic classifier (QDA) and the classifier implementing the regularized covariance matrices (RDA) are tested and compared.  The experiments have shown that the regularization techniques were very efficient in mitigating the Hughes phenomenon. In some experiments, the maximum accuracy was obtained using the conventional quadratic classifier (QDA), with a smaller number of spectral bands. In other cases the regularized procedure (RDA) produced more accurate results at higher dimensionality having mitigated the Hughes phenomenon. The investigation of the behavior of the regularization procedure (RDA) in remote sensing image data constitutes the main contribution of this study. The regularization procedure, originally proposed by Friedman is shown to perform well in remote sensing digital image classification. The regularization procedure (RDA) seems to be capable of improving the accuracy of the classification procedure when high dimensional image data is used.|http://hdl.handle.net/10183/5296
Utilização de imagens de satélite e modelagem numérica para o estudo da dispersão de poluentes nas usinas termoelétricas de Charqueadas e São Jerônimo|2005|Open Access|Dissertação|Sensoriamento remoto;Dispersão de poluentes|por|O objetivo geral deste trabalho é desenvolver estudos relacionados à dispersão de poluentes, considerando a queima de carvão para geração de energia nas Usinas Termoelétricas de Charqueadas e São Jerônimo. O período de estudo foi do dia 17 a 23 de junho de 2003. Neste período houve a passagem de um sistema frontal. Sendo possível avaliar a dispersão dos poluentes em condições pré-frontal, frontal e pós-frontal. Para simular o comportamento dos poluentes neste período, foi utilizada uma subrotina de dispersão acoplada ao modelo RAMS (Regional Atmospheric Modeling System). Os resultados mostraram que nos dias classificados como pré-frontal e pós-frontal as concentrações do material particulado, dióxido de enxofre e do óxido de nitrogênio, atingiram seus valores máximos, pelo fato da umidade relativa do ar estar bastante baixa em torno de 60%, pressão atmosférica da ordem de 1021 hPa e a intensidade dos ventos fraca. No dia classificado como frontal, as concentrações estavam praticamente nulas, devido à passagem do sistema frontal que causou a queda na pressão atmosférica, aumento da umidade relativa e também pelo fato da ocorrência de precipitação atmosférica neste dia. As comparações dos resultados simulados, com os dados observados na estação de qualidade do ar mostraram-se satisfatórios. Com exceção do dia 22 de junho, que apresentou uma diferença da ordem de 90%.|http://hdl.handle.net/10183/5299
Monitoramento de temperaturas noturnas da superfície terrestre no estado do Rio Grande do Sul com uso do sensor orbital AVHRR/NOAA|2003|Open Access|Dissertação|Sensoriamento remoto;Temperatura|por|A temperatura da superfície terrestre (TST), além da ação determinante sobre o crescimento e o desenvolvimento das plantas, influencia também muitos processos físicos, químicos e biológicos, tendo significativa relevância científica em um vasto campo das atividades de pesquisa e gerenciamento dos recursos naturais. O objetivo geral deste estudo foi verificar a adequação do uso de sensores orbitais AVHRR/3 NOAA (National Oceanic and Atmospheric Administration) classe POES (Polar Operational Environmental Satellites) para mapeamento de temperaturas da superfície terrestre no Rio Grande do Sul, visando sua implementação em programas operacionais de monitoramento agrometeorológico. No estudo realizado no período de junho a setembro de 2002, foram utilizadas 11 imagens noturnas e 20 imagens diurnas, captadas na estação de recepção de imagens NOAA pertencente ao Centro Estadual de Pesquisas em Sensoriamento Remoto e Meteorologia (CEPSRM). Após processamento, as imagens foram utilizadas para a determinação da temperatura de brilho da banda 4 e 5 da TST através dos métodos de Becker e Li (1990), Kerr et al. (1992) e Sobrino et al. (1993). A TST obtida a partir das imagens foi comparada aos dados de temperatura do ar obtidos em 13 estações meteorológicas de superfície  Os resultados mostraram que os sensores orbitais AVHRR/3 são adequados para o mapeamento noturno de temperaturas da superfície terrestre no Rio Grande do Sul. O método de Sobrino et al. (1993) é o mais adequado entre os métodos testados para a estimativa das temperaturas noturnas no Estado, embora ainda seja necessário um aprofundamento teórico em relação aos efeitos atmosféricos sobre regiões localizadas em baixas latitudes. Também a temperatura estimada por sensores remotos pode ser utilizada para estimar a temperatura do ar nos locais avaliados, sendo que ainda deve ser testada a extrapolação para outros locais.|http://hdl.handle.net/10183/5302
Metodologia de mapeamento aéreo com fotografias de pequeno formato aplicadas no planejamento e gerenciamento de unidades de conservação|2000|Open Access|Dissertação|Sensoriamento remoto;Unidades de conservação : Brasil|por|Esta dissertação propõe o desenvolvimento de uma metodologia de obtenção de fotografias aéreas, com a finalidade de mapear áreas selvagens amparadas legalmente e administradas pelo governo federal do Brasil. O objetivo final foi o de vir a ser uma ferramenta de mapeamento dessas áreas para o seu planejamento e gerenciamento, com uma metodologia simples e de baixo custo. O sistema para a obtenção das aerofotos emprega as fotografias obtidas com máquinas de 35 mm e aeronaves de asa alta e ampla visibilidade, próprias para reconhecimento aéreo. Emprega ainda um aparelho, com alguns acessórios, construído especialmente para proporcionar o máximo de verticalidade possível das aerofotos em relação ao solo, além de navegação por GPS, procurando eliminar assim distorções ocasionadas por giros e desvios da aeronave leve e melhorando o desempenho da montagem das aerofotos em um mosaico digital. O produto final a ser obtido é um mosaico de fotografias aéreas, georreferenciado, e que serve de base para a elaboração de diversos tipos de mapas temáticos da área protegida, contendo elementos que possam satisfazer as exigências técnicas requeridas pelo levantamento. Além disso, ficam disponibilizadas as aerofotos em pares estereoscópicos de altíssima resolução, para observações mais detalhadas dos diversos elementos no solo. Embora as imprecisões, procurou-se proporcionar uma base de mapeamento georreferenciado através do uso de GPS em pontos de controle no campo, correção destas informações através de técnicas expeditas de DGPS e correção geométrica do mosaico de aerofotos. O resultado final pode ser considerado bastante satisfatório e a metodologia passível de utilização no cumprimento dos objetivos a que se propõe, para o gerenciamento, planejamento e monitoramento de unidades de conservação de uso indireto federais, estaduais e municipais .|http://hdl.handle.net/10183/5304
Sistema de gerência de estradas municipais com uso de geoprocessamento|2003|Open Access|Dissertação|Sensoriamento remoto;Geoprocessamento|por|Este trabalho tem a finalidade de fornecer os elementos necessários à montagem de um SIG para gerenciamento e controle de obras públicas, tendo como ponto de partida as relacionadas às estradas de terra, portanto na área rural do município, podendo, ser estendido as outras áreas da gestão pública. A preferência pela questão viária dá-se pelo fato dela ter grande apelo econômico e relativa facilidade de aquisição dos planos de informação, quais sejam: malha viária, pontos notáveis, obras de arte, drenagem, relevo e solo. Planos de informação pertinentes à malha viária, aos pontos notáveis e às obras de arte, adquiridos em levantamento de campo com GPS, e drenagem, relevo e solo adquiridos via vetorização de plantas existentes. Através de um programa de apoio à gerência denominado SISGEM - Sistema de Gerência de Estradas Municipais - pode-se obter dados a respeito da produção, produtividade, custos, etc, gerando atributos aos geo objetos dos planos de informação descritos acima, e, desta forma, permitindo o cruzamento de planos de informações vetoriais com os relatórios gerados a partir dos trabalhos realizados.|http://hdl.handle.net/10183/5305
Aspectos algébricos de sistemas dinâmicos|2002|Open Access|Dissertação|Fisica teorica da materia condensada;Sistemas dinâmicos;Metodos matematicos em fisica;Álgebra|por|Este trabalho trata o problema genérico da obtenção analítica exata das variedades algébricas que definem domínios de estabilidade e multiestabilidade para sistemas dinâmicos dissipativos com equações de movimento definidas por funções racionais. Apresentamos um método genérico, válido para qualquer sistema dinâmico, que permite reduzir a análise de sistemas multidimensionais arbitrários à análise de um sistema unidimensional equivalente. Este método é aplicado ao mapa de Hénon, o exemplo paradigmático de sistema multidimensional, para estudar a estrutura aritmética imposta pela dinâmica das órbitas de períodos 4, 5, e 6, bem como seus domínios de estabilidade no espaço de parâmetros. Graças à obtençao de resultados analíticos exatos, podemos explorar pela primeira vez as peculariedades de cada um dos períodos mencionados. Algumas das novidades mais marcantes encontradas são as seguintes: Para período 4, encontramos um domínio de multiestabilidade caracterizado pela coexistência de duas órbitas definidas em corpos algébricos distintos. Observamos a existência de discontinuidades na dinâmica simbólica quando os parâmetros são mudados adiabáticamente ao longo de circulações fechadas no espaço de parâmetros e explicamos sua origem algébrica. Publicamos tais resultados em dois artigos: Physica A, 295, 285-290(2001) e Physical Review E, 65, 036231 (2002).  Para período 5, obtivemos a variedade algébrica que define o ""camarão"" (shrimp) característico, obtemos uma expressão analítica para todas as órbitas de período 5, classificamos todas as singulariedades presentes no espaço de parâmetros e analisamos todas as mudanças que ocorrem ao circular-se em torno de tais singulariedades. Para período 6, da expressão analítica que fornece todas as órbitas, encontramos um resultado muito surpreendente, o mais notável desta dissertação: a possibilidade de coexistência de órbitas reais e complexas estáveis, para valores reais dos parâmetros físicos. Resultados preliminares parecem indicar serem tais órbitas complexas uma espécie de órbitas fantasmas, com semelhanças as órbitas encontradas por Gutzwiller para sistemas Hamiltonianos (não- dissipativos)|http://hdl.handle.net/10183/5310
Uma generalização do algorítmo de Gao para fatoração de polinômios|2004|Open Access|Dissertação|Computação Algébrica;Fatoracao de polinomios|por|A presente dissertação trata da fatoração de polinômios em duas variáveis sobre um corpo F. Mais precisamente, o trabalho traça o desenvolvimento histórico de uma estratégia modular que levou à resolução desse problema em tempo polinomial e culmina com a apresentação de um algoritmo publicado por S. Gao no ano de 2003, que determina simultaneamente as fatorações racional e absoluta de um dado polinômio. A nossa contribuição consiste na extensão desse algoritmo a casos que não satisfazem as condições prescritas pelo autor.|http://hdl.handle.net/10183/5312
Análise de diagramas cor-magnitude de 5 aglomerados ricos da LMC|2004|Open Access|Tese|Grande Nuvem de Magalhães;Aglomerados estelares globulares;Diagramas;Cor;Magnitude estelar;Massa;Idade das estrelas;Metalicidade|por|Apresentamos a análise de diagramas cor-magnitude (CMDs) profundos para 5 aglomerados ricos da Grande Nuvem de Magalhães (LMC). Os dados fazem parte do projeto HST G07307, entitulado ""Formação e Evolução de Aglomerados Estelares Ricos da LMC"", e foram obtidos através do HST/WFPC2 nos filtros F555W (~ V) e F814W (~ I), alcançando V555~25. A amostra de aglomerados é composta por NGC 1805 e NGC 1818, os mais jovens (r < 100 Myr), NGC 1831e NGC 1868,de idades intermediárias (400 < r < 1000Myr), e Hodge 14, o mais velho (r > 1200Myr). Discutimos e apresentamos um método de correção dos CMDs para incompleteza da amostra e contaminação por estrelas de campo. O uso eficiente das informações contidas nos CMDs foi feito por meio de comparações entre os CMDs observados e CMDs modelados. O processo de modelamento de CMDs gera uma Seqüência Principal (MS) sintética, que utiliza como entrada do modelo a informação sobre idade (r), metalicidade (Z), Função de Massa do Presente (PDMF), fração de binárias não resolvidas, distância e extinção da luz. As incertezas fotométricas foram empiricamente determinadas a partir dos dados e incorporadas no modelamento. Técnicas estatísticas de comparação entre CMDs em 1D e 2D são apresentadas e aplicadas como métodos objetivos capazes de determinar a compatibilidade entre modelo e observação. Modelando os CMDs da região central dos aglomerados podemos inferir a metalicidade (Z), o módulo de distância intrínseco ((V - Mv)o) e o avermelhamento (E(B-V)) para cada aglomerado. Também determinamos as idades para os 3 aglomerados mais velhos de nossa amostra. Continuação) Através da comparação em 2D os valores encontrados foram os seguintes: para NGC 1805, Z = 0.007:1+-0.003, (V - Mv)o = 18.50:1+- 0.10, E(B - V) = 0.03:1+-0.01; para NGC 1818,Z = 0.005:1+-0.002, (V - Mv)o = 18.50:1+-0.15, E(B - V)~0.00; para NGC 1831, Z = 0.012:1+-0.002, log(r /yr) = 8.70 :I+-0.05, (V - Mv)o = 18.70:1+- 0.05, E(B - V)~ 0.00; para NGC 1868, Z = 0.008:1+-0.002,log(r/yr) =8.95:1+-0.05, (V - Mv)o = 18.70:1+- 0.05, E(B - V) ~0.00; para Hodge 14, Z = 0.008+-0.004, log(r/yr) = 9.23+-0.10, (V - Mv)o = 18.50+-0.15, E(B - V) = 0.02+- 0.02. Estes valores estão de acordo, dentro das incertezas, com os valores obtidos através da análise de linhas fiduciais (lD), o que agrega confiabilidade a estas determinações. Através da análise de CMDs em regiões concêntricas determinamos a dependência espacial da inclinação da PDMF (a) nos aglomerados mais ricos. Para tanto empregamos duas abordagens distintas para a determinação de a: 1) método tradicional em 1D via conversão direta de V555em massa; 2) método em 2D via modelmento de CMDs. Independente do método de análise, todos os aglomerados analisados demonstram claramente o efeito de segregação em massa. Além disso, NGC 1818, NGC 1831 e NGC 1868 apresentam uma possível perda seletiva de estrelas de baixa massa nas suas regiões mais externas.|http://hdl.handle.net/10183/5314
Desenvolvimento de metodologias para análise direta de óleos vegetais empregando microemulsões de água em óleo e meios não aquosos|2005|Open Access|Tese|Química analítica;Microemulsoes;Óleos vegetais;Voltametria|por|Amostras de óleo vegetal foram preparadas na forma de microemulsão de água em óleo (w/o) utilizando dodecil sulfato de sódio (SDS) como surfactante e um álcool como co-surfactante. As microemulsões foram caracterizadas através de medidas de viscosidade, índice de refração, condutividade elétrica, espalhamento de luz dinâmico e voltametria. Ensaios preliminares para a quantificação de analitos por análise direta foram realizados por voltametria linear e eletroforese capilar. As microemulsões de água em óleo de soja apresentaram gotículas de dimensões nanométricas e estabilidade termodinâmica dependente da temperatura, da concentração dos eletrólitos dissolvidos e da natureza do co-surfactante empregado. Por outro lado, o raio hidrodinâmico das gotículas (Rh) diminui com o aumento da temperatura. Quanto aos valores de condutividade, maiores do que os obtidos para o óleo de soja e para a água deionizada, aumentam com a temperatura, na faixa de 20 a 65 oC, e com o teor de água na microemulsão, entre 5 e 7,5 % de água. A maior estabilidade termodinâmica foi alcançada para uma microemulsão contendo 40,0 % de óleo, 43,2 % de pentanol, 10,8 % de SDS e 6,0 % de água, em massa, na razão 1:4 [SDS]:[álcool]  Medidas voltamétricas com um ultramicroeletrodo de Pt em microemulsões contendo ferroceno ou ácido oléico dissolvidos evidenciaram a dependência linear das correntes limite anódica e catódica com a concentração da espécie eletroativa. Já a oxidação do ferroceno por voltametria cíclica, usando o mesmo ultramicroeletrodo, mostrou que a diminuição dos coeficientes de difusão nestes meios permite realizar medidas em estado transiente, empregando velocidades convencionais de varredura em potencial. Experimentos por cromatografia eletrocinética em microemulsão reversa (RMEEKC) com n-pentanol como fase contínua permitiram a separação de solutos neutros e aniônicos em amostras de óleo vegetal e gordura animal. Finalmente, foi desenvolvido um procedimento rápido para a separação e identificação dos biofenóis presentes em óleos, utilizando como eletrólito de corrida uma mistura de metanol e 1-propanol contendo KOH. Os resultados evidenciam a possibilidade de análise direta de óleos vegetais empregando métodos eletroanalíticos em microemulsões w/o, e métodos por eletroforese capilar, quer em microemulsões, quer em meio não aquoso.|http://hdl.handle.net/10183/5316
APSEE-Global : um modelo de gerência de processos distribuídos de software|2005|Open Access|Dissertação|Desenvolvimento : Software;Engenharia : Software;Processo : Software|por|Desde o início dos anos 90, uma tendência no desenvolvimento de software tem despertado a atenção dos pesquisadores: a distribuição do desenvolvimento. Esse fenômeno é um reflexo de mudanças sociais e econômicas, que têm levado organizações a distribuírem geograficamente seus recursos e investimentos, visando aumento de produtividade, melhorias na qualidade e redução de custos no desenvolvimento de software. Em virtude dessa distribuição, equipes geograficamente dispersas cooperam para a obtenção de um produto final de software. A distribuição física das equipes agrava problemas já inerentes à gerência do processo de software. O desenvolvimento de ambientes, modelos e ferramentas para gerenciar processos conduzidos nesse contexto é um desafio cada vez mais importante nos estudos em Tecnologia de Processos de Software. Os ambientes de suporte a processos devem prover infra-estrutura para processos distribuídos. Este trabalho propõe um modelo de gerência de processos distribuídos, denominado APSEE-Global, que estende o APSEE, um ambiente de engenharia de software centrado no processo desenvolvido no contexto do grupo de pesquisa Prosoft, provendo um conjunto de funcionalidades para suporte a processos distribuídos.  O APSEE-Global viabiliza o aumento da autonomia das equipes que participam do projeto, pela possibilidade de adotarem modelos de processo distintos e pela gerência descentralizada do processo de desenvolvimento; permite a documentação e facilita a análise e a gerência das relações entre as equipes; e provê um canal de comunicação formal para acompanhamento da execução do processo distribuído. Os diferentes componentes do APSEE-Global foram especificados formalmente, o que constitui uma base semântica de alto nível de abstração que deu origem à implementação de um protótipo integrado ao ambiente de desenvolvimento de software Prosoft. A especificação do modelo foi realizada pela combinação dos formalismos Prosoft-Algébrico e Gramática de Grafos.|http://hdl.handle.net/10183/5338
